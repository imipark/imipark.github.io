<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="


Ch9: Instruction Fine-Tuning (IFT/SFT)




  Q1: What are chat templates and why do they matter?
  #

A: Chat templates are formatting systems that structure conversations into a format language models can process. They use special tokens (like &lt;|im_start|&gt;, &lt;|im_end|&gt;) to mark boundaries between different parts of the conversation.
Example:
&lt;|im_start|&gt;system
You are a helpful assistant&lt;|im_end|&gt;
&lt;|im_start|&gt;user
What is 2&#43;2?&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
The answer is 4&lt;|im_end|&gt;


  Q2: What are the three message roles and how do they differ?
  #

A:">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ch9_sft/">
  <meta property="og:site_name" content="AI Reasoning">
  <meta property="og:title" content="Ch9. Instruction Fine-Tuning (IFT/SFT)">
  <meta property="og:description" content="Ch9: Instruction Fine-Tuning (IFT/SFT) Q1: What are chat templates and why do they matter? # A: Chat templates are formatting systems that structure conversations into a format language models can process. They use special tokens (like &lt;|im_start|&gt;, &lt;|im_end|&gt;) to mark boundaries between different parts of the conversation.
Example:
&lt;|im_start|&gt;system You are a helpful assistant&lt;|im_end|&gt; &lt;|im_start|&gt;user What is 2&#43;2?&lt;|im_end|&gt; &lt;|im_start|&gt;assistant The answer is 4&lt;|im_end|&gt; Q2: What are the three message roles and how do they differ? # A:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="ai-workflows">
<title>Ch9. Instruction Fine-Tuning (IFT/SFT) | AI Reasoning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ch9_sft/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.a0efbe41a7b1c6f20d16b2d51ba7430c4d0d9b69849963c0c682ad9812647b67.js" integrity="sha256-oO&#43;&#43;QaexxvINFrLVG6dDDE0Nm2mEmWPAxoKtmBJke2c=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>AI Reasoning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ai-workflows/" class="">AI Reasoning Stack</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-f8e502c34e18a04e1c3aecf192b0355a" class="toggle"  />
    <label for="section-f8e502c34e18a04e1c3aecf192b0355a" class="flex justify-between">
      <a href="/ai-workflows/data-modeling/" class="">Data Modeling</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-906a2243f4790a09188fae70fbc32dbd" class="toggle"  />
    <label for="section-906a2243f4790a09188fae70fbc32dbd" class="flex justify-between">
      <a href="/ai-workflows/data-modeling/data-centric-ai/" class="">Data-Centric AI (DCAI)</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-5bf1c216f4b747bd7c2f992550ff09a1" class="toggle"  />
    <label for="section-5bf1c216f4b747bd7c2f992550ff09a1" class="flex justify-between">
      <a href="/ai-workflows/genai-systems/" class="">GenAI Systems</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8cbfd420d91de13f61964bf971127312" class="toggle"  />
    <label for="section-8cbfd420d91de13f61964bf971127312" class="flex justify-between">
      <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/" class="">5-Day GenAI with Google 2005</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day1_foundational_llm_text_generation/" class="">Day 1 - Foundational LLMs &amp; Text Generation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day1_prompt_engineering/" class="">Day 1 – Prompt Engineering</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day2_embeddings_vectordb/" class="">Day 2 – Embeddings &amp; Vector Databases</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day3_generative_agents/" class="">Day 3 – Generative Agents</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day4_domainspecific_llms/" class="">Day 4 – Domain-Specific LLMs</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day5_mlops/" class="">Day 5 – MLOps for Generative AI</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/multimodel_llms/" class="">Multimodal LLMs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-17ba62e37c896ad50105fedeb71549dd" class="toggle"  />
    <label for="section-17ba62e37c896ad50105fedeb71549dd" class="flex justify-between">
      <a href="/ai-workflows/reasoning/" class="">Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd94e161670d28ecff992edf840d76e8" class="toggle"  />
    <label for="section-cd94e161670d28ecff992edf840d76e8" class="flex justify-between">
      <a href="/ai-workflows/reasoning/causality/" class="">Causality</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-ai/" class="">Causal AI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-inference/" class="">Causal Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7e468aa05ceb7c844f07a2e754606b76" class="toggle"  />
    <label for="section-7e468aa05ceb7c844f07a2e754606b76" class="flex justify-between">
      <a href="/ai-workflows/reasoning/graph-reasoning/" class="">Graph Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/graphrag/" class="">GraphRAG</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/knowledge-graphs/" class="">Knowledge Graphs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="toggle" checked />
    <label for="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="flex justify-between">
      <a href="/ai-workflows/rlhf/" class="">RLHF</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b1afcafdefac57f3420f64e23d73f05d" class="toggle" checked />
    <label for="section-b1afcafdefac57f3420f64e23d73f05d" class="flex justify-between">
      <a href="/ai-workflows/rlhf/rlhf2006/" class="">RLHF 2006</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/" class="">Instruct Gpt Codes Params</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ca53d32fab0e1a54fdf5627349d86bfc" class="toggle"  />
    <label for="section-ca53d32fab0e1a54fdf5627349d86bfc" class="flex justify-between">
      <a href="/ai-workflows/eval/" class="">AI Evaluation</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/healthcare/" class="">Healthcare</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="toggle"  />
    <label for="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/" class="">Domain</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-45ba5974905f86df95925365835eadbb" class="toggle"  />
    <label for="section-45ba5974905f86df95925365835eadbb" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/ai-in-healthcare/" class="">AI in Healthcare</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/" class="">C2 Clinical Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/" class="">C3 ML Healthcare</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/" class="">C4 AI Evaluations</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c5_capstone/" class="">C5 Capstone Projects</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-9722ba71bf098ab02c3220d6e8d9056f" class="toggle"  />
    <label for="section-9722ba71bf098ab02c3220d6e8d9056f" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/hands-on-healthcare-data/" class="">Hands-On Healthcare Data</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/data/" class="">Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/clinical_ai/" class="">AI Applications</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ipark/" class="">Inhee Park, PhD - Resume</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>










  
<ul>
  
  <li>
    <a href="https://www.linkedin.com/in/inheepark/"  target="_blank" rel="noopener">
        ╰──LinkedIn
      </a>
  </li>
  
  <li>
    <a href="https://github.com/imipark/"  target="_blank" rel="noopener">
        ╰──GitHub
      </a>
  </li>
  
  <li>
    <a href="/posts/"  target="_blank" rel="noopener">
        ╰──Blog
      </a>
  </li>
  
  <li>
    <a href="https://iparkirk.github.io"  target="_blank" rel="noopener">
        ╰──Old Web
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Ch9. Instruction Fine-Tuning (IFT/SFT)</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#q1-what-are-chat-templates-and-why-do-they-matter"><strong>Q1: What are chat templates and why do they matter?</strong></a></li>
            <li><a href="#q2-what-are-the-three-message-roles-and-how-do-they-differ"><strong>Q2: What are the three message roles and how do they differ?</strong></a></li>
            <li><a href="#q3-what-is-prompt-masking-in-iftsft"><strong>Q3: What is prompt masking in IFT/SFT?</strong></a></li>
            <li><a href="#q4-how-is-masking-different-in-iftsft-vs-bertgpt-pretraining"><strong>Q4: How is &ldquo;masking&rdquo; different in IFT/SFT vs BERT/GPT pretraining?</strong></a></li>
            <li><a href="#q5-why-use-multi-turn-conversations-instead-of-just-single-turn"><strong>Q5: Why use multi-turn conversations instead of just single-turn?</strong></a></li>
            <li><a href="#q6-what-are-the-supervised-learning-pairs-in-iftsft"><strong>Q6: What are the supervised learning pairs in IFT/SFT?</strong></a></li>
            <li><a href="#q7-what-exactly-gets-masked-in-multi-turn-training"><strong>Q7: What exactly gets &ldquo;masked&rdquo; in multi-turn training?</strong></a></li>
            <li><a href="#q8-what-are-the-key-implementation-differences-from-pretraining"><strong>Q8: What are the key implementation differences from pretraining?</strong></a></li>
            <li><a href="#q9-what-are-the-best-practices-for-instruction-tuning"><strong>Q9: What are the best practices for instruction tuning?</strong></a></li>
          </ul>
        </li>
        <li><a href="#content-summary"><strong>Content Summary</strong></a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><p align="center">
<img src="/images/AIR_logo.png" alt="AI Reasoning Logo" width="200"/>
<strong style="font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;">
Ch9: Instruction Fine-Tuning (IFT/SFT)
</strong>
</p>
<hr>
<h3 id="q1-what-are-chat-templates-and-why-do-they-matter">
  <strong>Q1: What are chat templates and why do they matter?</strong>
  <a class="anchor" href="#q1-what-are-chat-templates-and-why-do-they-matter">#</a>
</h3>
<p><strong>A:</strong> Chat templates are formatting systems that structure conversations into a format language models can process. They use special tokens (like <code>&lt;|im_start|&gt;</code>, <code>&lt;|im_end|&gt;</code>) to mark boundaries between different parts of the conversation.</p>
<p><strong>Example:</strong></p>
<pre tabindex="0"><code>&lt;|im_start|&gt;system
You are a helpful assistant&lt;|im_end|&gt;
&lt;|im_start|&gt;user
What is 2+2?&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
The answer is 4&lt;|im_end|&gt;
</code></pre><hr>
<h3 id="q2-what-are-the-three-message-roles-and-how-do-they-differ">
  <strong>Q2: What are the three message roles and how do they differ?</strong>
  <a class="anchor" href="#q2-what-are-the-three-message-roles-and-how-do-they-differ">#</a>
</h3>
<p><strong>A:</strong></p>
<ul>
<li><strong>System</strong>: Sets persistent context/instructions for the entire conversation. Applied once at the beginning. Think of it as &ldquo;background instructions&rdquo; that influence all responses.</li>
<li><strong>User</strong>: Messages from the person using the AI</li>
<li><strong>Assistant</strong>: Individual responses from the AI model</li>
</ul>
<p><strong>Key point:</strong> System provides context that affects all assistant responses, but each assistant message is a separate turn in the conversation.</p>
<hr>
<h3 id="q3-what-is-prompt-masking-in-iftsft">
  <strong>Q3: What is prompt masking in IFT/SFT?</strong>
  <a class="anchor" href="#q3-what-is-prompt-masking-in-iftsft">#</a>
</h3>
<p><strong>A:</strong> Prompt masking means the model <strong>sees all tokens</strong> (prompt + response) but the loss is only calculated on the assistant&rsquo;s response tokens. The prompt tokens are excluded from loss computation.</p>
<p><strong>Why?</strong> We want the model to learn to generate good responses, not to predict user queries.</p>
<p><strong>Example:</strong></p>
<pre tabindex="0"><code>User: &#34;What is 2+2?&#34; ← SEEN but NO LOSS applied
Assistant: &#34;The answer is 4&#34; ← SEEN and LOSS applied
</code></pre><hr>
<h3 id="q4-how-is-masking-different-in-iftsft-vs-bertgpt-pretraining">
  <strong>Q4: How is &ldquo;masking&rdquo; different in IFT/SFT vs BERT/GPT pretraining?</strong>
  <a class="anchor" href="#q4-how-is-masking-different-in-iftsft-vs-bertgpt-pretraining">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Training Type</th>
          <th>Masked Tokens Visibility</th>
          <th>Loss Applied To</th>
          <th>Purpose</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>BERT</strong></td>
          <td>❌ Hidden/Not seen</td>
          <td>✅ Masked tokens</td>
          <td>Predict missing words</td>
      </tr>
      <tr>
          <td><strong>GPT Pretraining</strong></td>
          <td>❌ Hidden (future tokens)</td>
          <td>✅ All tokens</td>
          <td>Predict next token</td>
      </tr>
      <tr>
          <td><strong>IFT/SFT</strong></td>
          <td>✅ Fully visible</td>
          <td>✅ Only assistant responses</td>
          <td>Generate good responses</td>
      </tr>
  </tbody>
</table>
<p><strong>Critical Difference:</strong></p>
<ul>
<li><strong>BERT/GPT</strong>: Masked tokens <strong>NOT SEEN</strong> during training, <strong>INCLUDED</strong> in loss</li>
<li><strong>IFT/SFT</strong>: Masked tokens <strong>FULLY SEEN</strong> during training, <strong>EXCLUDED</strong> from loss</li>
</ul>
<hr>
<h3 id="q5-why-use-multi-turn-conversations-instead-of-just-single-turn">
  <strong>Q5: Why use multi-turn conversations instead of just single-turn?</strong>
  <a class="anchor" href="#q5-why-use-multi-turn-conversations-instead-of-just-single-turn">#</a>
</h3>
<p><strong>A:</strong> Multi-turn data teaches the model:</p>
<ol>
<li><strong>Context tracking</strong> - understand references to previous turns</li>
<li><strong>Conversation coherence</strong> - maintain consistency across dialogue</li>
<li><strong>Real conversation skills</strong> - handle follow-ups and clarifications</li>
</ol>
<p><strong>Example why this matters:</strong></p>
<pre tabindex="0"><code>Turn 1
User: &#34;I have a dog&#34;
Assistant: &#34;What breed?&#34;

Turn 2
User: &#34;He&#39;s very playful&#34; ← needs to understand &#34;he&#34; = the dog
</code></pre><p>Single-turn only would make models bad at maintaining conversational context.</p>
<hr>
<h3 id="q6-what-are-the-supervised-learning-pairs-in-iftsft">
  <strong>Q6: What are the supervised learning pairs in IFT/SFT?</strong>
  <a class="anchor" href="#q6-what-are-the-supervised-learning-pairs-in-iftsft">#</a>
</h3>
<p><strong>A:</strong> The pairs are: <strong>[Full conversation context] → [Next assistant response]</strong></p>
<p><strong>Single-turn:</strong></p>
<pre tabindex="0"><code>Input: [System + User1] → Target: [Assistant1]
</code></pre><p><strong>Multi-turn (unrolled into multiple training examples):</strong></p>
<pre tabindex="0"><code>Example 1: [System + User1] → Target: [Assistant1]
Example 2: [System + User1 + Assistant1 + User2] → Target: [Assistant2]
Example 3: [System + User1 + Assistant1 + User2 + Assistant2 + User3] → Target: [Assistant3]
</code></pre><p><strong>Key insight:</strong> One N-turn conversation creates N training examples, each predicting a different assistant response.</p>
<hr>
<h3 id="q7-what-exactly-gets-masked-in-multi-turn-training">
  <strong>Q7: What exactly gets &ldquo;masked&rdquo; in multi-turn training?</strong>
  <a class="anchor" href="#q7-what-exactly-gets-masked-in-multi-turn-training">#</a>
</h3>
<p><strong>A:</strong></p>
<p>For Turn 2 example:</p>
<ul>
<li><strong>Input sequence (what model sees):</strong> <code>[System + User1 + Assistant1 + User2 + Assistant2]</code></li>
<li><strong>Masked from loss:</strong> <code>[System + User1 + Assistant1 + User2]</code></li>
<li><strong>Loss applied to:</strong> <code>[Assistant2]</code> only</li>
</ul>
<p>The model <strong>sees everything</strong> for context, but only learns to generate the current assistant response.</p>
<hr>
<h3 id="q8-what-are-the-key-implementation-differences-from-pretraining">
  <strong>Q8: What are the key implementation differences from pretraining?</strong>
  <a class="anchor" href="#q8-what-are-the-key-implementation-differences-from-pretraining">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Aspect</th>
          <th>Pretraining</th>
          <th>IFT/SFT</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Batch size</strong></td>
          <td>Large (1024-2048)</td>
          <td>Smaller (256)</td>
      </tr>
      <tr>
          <td><strong>Loss applied to</strong></td>
          <td>All tokens</td>
          <td>Only assistant responses</td>
      </tr>
      <tr>
          <td><strong>Training data</strong></td>
          <td>Raw text</td>
          <td>Structured conversations</td>
      </tr>
  </tbody>
</table>
<p><strong>Key differences:</strong></p>
<ol>
<li><strong>Smaller batch sizes</strong> - fewer GPUs needed</li>
<li><strong>Prompt masking</strong> - loss only on responses</li>
<li><strong>Multi-turn masking</strong> - only final assistant turn per example</li>
</ol>
<hr>
<h3 id="q9-what-are-the-best-practices-for-instruction-tuning">
  <strong>Q9: What are the best practices for instruction tuning?</strong>
  <a class="anchor" href="#q9-what-are-the-best-practices-for-instruction-tuning">#</a>
</h3>
<p><strong>A:</strong></p>
<ul>
<li><strong>Quality over quantity</strong> - High-quality completions are crucial (model learns from responses)</li>
<li><strong>~1M prompts</strong> sufficient for excellent results (diminishing returns after)</li>
<li><strong>Data distribution matters</strong> - Use prompts similar to target use cases</li>
<li><strong>Overall optimization</strong> - Models can recover from noise; focus on complete pipeline</li>
</ul>
<hr>
<h2 id="content-summary">
  <strong>Content Summary</strong>
  <a class="anchor" href="#content-summary">#</a>
</h2>
<p><strong>Core Concept:</strong> Instruction fine-tuning transforms pretrained language models into conversational assistants by teaching them to generate appropriate responses to user queries.</p>
<p><strong>Key Mechanisms:</strong></p>
<ol>
<li>Chat templates structure conversations with role-based formatting</li>
<li>Prompt masking ensures models learn response generation, not query prediction</li>
<li>Multi-turn training develops conversational coherence and context tracking</li>
<li>Supervised learning pairs full context with target responses</li>
</ol>
<p><strong>Critical Insight:</strong> The &ldquo;masking&rdquo; terminology is overloaded:</p>
<ul>
<li><strong>In IFT/SFT:</strong> &ldquo;masked&rdquo; = excluded from loss (but still visible to model)</li>
<li><strong>In BERT/GPT:</strong> &ldquo;masked&rdquo; = hidden from model (and included in loss for prediction)</li>
</ul>
<p><strong>Most Important Takeaway:</strong> In IFT/SFT, the model sees the entire conversation history for context, but only learns to predict the assistant&rsquo;s responses. This creates models that can follow instructions while maintaining conversational context.</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#q1-what-are-chat-templates-and-why-do-they-matter"><strong>Q1: What are chat templates and why do they matter?</strong></a></li>
            <li><a href="#q2-what-are-the-three-message-roles-and-how-do-they-differ"><strong>Q2: What are the three message roles and how do they differ?</strong></a></li>
            <li><a href="#q3-what-is-prompt-masking-in-iftsft"><strong>Q3: What is prompt masking in IFT/SFT?</strong></a></li>
            <li><a href="#q4-how-is-masking-different-in-iftsft-vs-bertgpt-pretraining"><strong>Q4: How is &ldquo;masking&rdquo; different in IFT/SFT vs BERT/GPT pretraining?</strong></a></li>
            <li><a href="#q5-why-use-multi-turn-conversations-instead-of-just-single-turn"><strong>Q5: Why use multi-turn conversations instead of just single-turn?</strong></a></li>
            <li><a href="#q6-what-are-the-supervised-learning-pairs-in-iftsft"><strong>Q6: What are the supervised learning pairs in IFT/SFT?</strong></a></li>
            <li><a href="#q7-what-exactly-gets-masked-in-multi-turn-training"><strong>Q7: What exactly gets &ldquo;masked&rdquo; in multi-turn training?</strong></a></li>
            <li><a href="#q8-what-are-the-key-implementation-differences-from-pretraining"><strong>Q8: What are the key implementation differences from pretraining?</strong></a></li>
            <li><a href="#q9-what-are-the-best-practices-for-instruction-tuning"><strong>Q9: What are the best practices for instruction tuning?</strong></a></li>
          </ul>
        </li>
        <li><a href="#content-summary"><strong>Content Summary</strong></a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












