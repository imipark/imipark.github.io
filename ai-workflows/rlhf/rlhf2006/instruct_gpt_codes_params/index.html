<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  RLHF Pipeline: Key Non-Default Settings
  #


  Critical Configuration (Non-Defaults Only)
  #


  üéØ Models &amp; Architecture
  #


  
      
          Setting
          Value
          Why Not Default?
      
  
  
      
          Model Class
          Step 1/3: CausalLMStep 2: SequenceClassification
          Step 2 needs scalar reward output, not text generation
      
      
          Quantization
          4-bit QLoRA
          Fits 7B model in 24GB VRAM (vs 28GB for fp16)
      
      
          num_labels
          Step 2: 1
          Reward model outputs single scalar score
      
  


  üìä Dataset Configuration
  #


  
      
          Setting
          Value
          Why Not Default?
      
  
  
      
          Dataset Size
          SFT: 20K, RM: 50K, PPO: 20K
          RM needs more data for robust preference learning
      
      
          Data Format
          SFT: chosen onlyRM: chosen&#43;rejected pairsPPO: prompts only
          Each step requires different supervision signal
      
      
          Train/Eval Split
          Step 2: 5% eval
          Only RM needs validation to prevent reward hacking
      
  


  ‚öôÔ∏è Training Hyperparameters
  #


  
      
          Setting
          Value
          Why Not Default?
      
  
  
      
          Learning Rate
          SFT: 2e-4RM: 5e-5PPO: 1e-6
          Decreasing LR prevents destabilizing previous training
      
      
          Batch Size
          1 (SFT/RM)8 (PPO)
          Memory constraint; PPO needs multiple rollouts per update
      
      
          Gradient Accumulation
          SFT: 16RM: 32
          Simulates larger batch sizes within memory limit
      
      
          Effective Batch Size
          SFT: 16RM: 32PPO: 8
          RM needs larger batches for stable ranking gradients
      
  


  üîß LoRA Configuration
  #


  
      
          Setting
          Value
          Why Not Default?
      
  
  
      
          r (rank)
          16
          Balance between parameter efficiency and model capacity
      
      
          alpha
          32 (2√ór)
          Standard scaling for LoRA updates
      
      
          dropout
          0.05
          Mild regularization to prevent adapter overfitting
      
      
          task_type
          SFT/PPO: CAUSAL_LMRM: SEQ_CLS
          Matches the model head type for each step
      
  


  üéõÔ∏è Quantization Details
  #


  
      
          Setting
          Value
          Why Not Default?
      
  
  
      
          load_in_4bit
          True
          Reduces memory by 75% vs fp16
      
      
          bnb_4bit_use_double_quant
          True
          Quantizes quantization constants (extra memory savings)
      
      
          bnb_4bit_quant_type
          &quot;nf4&quot;
          Normal Float 4-bit optimal for weights (vs uniform)
      
      
          bnb_4bit_compute_dtype
          bfloat16
          Better numerical stability than fp16 for training
      
  


  üìà Optimization Settings
  #


  
      
          Setting
          Value
          Why Not Default?
      
  
  
      
          optim
          paged_adamw_8bit
          Memory-efficient optimizer for 4-bit training
      
      
          bf16
          True
          Better gradient stability than fp16
      
      
          gradient_checkpointing
          True
          Trades compute for memory (enables longer sequences)
      
      
          lr_scheduler_type
          &quot;cosine&quot;
          Smooth LR decay prevents abrupt training disruption
      
      
          warmup_ratio
          0.03
          Stabilizes initial training with 4-bit quantization
      
      
          max_grad_norm
          0.3
          Prevents gradient explosion in LoRA training
      
  


  üîÑ PPO-Specific (Step 3 Only)
  #


  
      
          Setting
          Value
          Why Not Default?
      
  
  
      
          mini_batch_size
          1
          Memory constraint during on-policy generation
      
      
          ppo_epochs
          4
          Multiple passes over collected experience
      
      
          init_kl_coef
          0.1
          Prevents policy from diverging too far from SFT
      
      
          adap_kl_ctrl
          True
          Dynamically adjusts KL penalty based on divergence
      
      
          gamma
          1.0
          No discounting (language has no clear episode structure)
      
      
          lam
          0.95
          GAE parameter balancing bias-variance in advantage
      
      
          cliprange
          0.2
          Limits policy update size (PPO core mechanism)
      
      
          vf_coef
          0.1
          Weight of value function loss vs policy loss
      
  



  Training Flow Summary
  #

Llama-2-7b-hf (4-bit quantized)
       ‚Üì
   [Step 1: SFT]  ‚Üê 20K chosen examples, LR=2e-4, LoRA r=16
       ‚Üì
       ‚îú‚îÄ‚Üí [Step 2: RM]  ‚Üê 50K preference pairs, LR=5e-5, outputs scalar
       ‚îÇ        ‚Üì
       ‚îî‚îÄ‚Üí [Step 3: PPO]  ‚Üê 20K prompts, LR=1e-6, KL=0.1
            ‚Üì
     Final RLHF Model


  Key Metrics to Monitor
  #


  
      
          Step
          Primary Metric
          Danger Sign
      
  
  
      
          SFT
          Training loss ‚Üì
          Eval loss ‚Üë (overfitting)
      
      
          RM
          Ranking accuracy ‚Üë
          Reward always higher for longer text (length bias)
      
      
          PPO
          Mean reward ‚Üë
          KL &gt; 0.5 (policy collapse)
      
  



  Why These Specific Values?
  #


  Learning Rate Decay Pattern
  #


SFT (2e-4): Highest LR for initial adaptation from base model
RM (5e-5): Lower to preserve SFT knowledge while learning preferences
PPO (1e-6): Tiny updates to avoid destroying alignment from RM


  Batch Size Strategy
  #


Small per-device (1): GPU memory constraint with 7B model
Large accumulation (16-32): Stabilizes gradients for contrastive learning (RM)
PPO (8 rollouts): Enough diversity for policy gradient estimation


  Quantization Choices
  #


4-bit: Only option that fits 7B &#43; optimizer states in 24GB
NF4: Specifically designed for neural network weight distributions
Double quant: Squeezes extra ~1GB by quantizing quantization parameters
bfloat16 compute: Prevents underflow in gradients during backprop


  LoRA Design
  #


r=16: Sweet spot for 7B models (too low = capacity loss, too high = overfitting)
alpha=32: Standard 2√ó scaling keeps update magnitudes reasonable
All attention &#43; FFN: Covers both information routing and transformation


  PPO Parameters
  #


KL penalty (0.1): Prevents catastrophic forgetting of SFT behavior
Clip (0.2): Conservative updates reduce instability
Gamma (1.0): No temporal discounting (each token equally important)
">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/">
  <meta property="og:site_name" content="AI Reasoning">
  <meta property="og:title" content="AI Reasoning">
  <meta property="og:description" content="RLHF Pipeline: Key Non-Default Settings # Critical Configuration (Non-Defaults Only) # üéØ Models &amp; Architecture # Setting Value Why Not Default? Model Class Step 1/3: CausalLM
Step 2: SequenceClassification Step 2 needs scalar reward output, not text generation Quantization 4-bit QLoRA Fits 7B model in 24GB VRAM (vs 28GB for fp16) num_labels Step 2: 1 Reward model outputs single scalar score üìä Dataset Configuration # Setting Value Why Not Default? Dataset Size SFT: 20K, RM: 50K, PPO: 20K RM needs more data for robust preference learning Data Format SFT: chosen only
RM: chosen&#43;rejected pairs
PPO: prompts only Each step requires different supervision signal Train/Eval Split Step 2: 5% eval Only RM needs validation to prevent reward hacking ‚öôÔ∏è Training Hyperparameters # Setting Value Why Not Default? Learning Rate SFT: 2e-4
RM: 5e-5
PPO: 1e-6 Decreasing LR prevents destabilizing previous training Batch Size 1 (SFT/RM)
8 (PPO) Memory constraint; PPO needs multiple rollouts per update Gradient Accumulation SFT: 16
RM: 32 Simulates larger batch sizes within memory limit Effective Batch Size SFT: 16
RM: 32
PPO: 8 RM needs larger batches for stable ranking gradients üîß LoRA Configuration # Setting Value Why Not Default? r (rank) 16 Balance between parameter efficiency and model capacity alpha 32 (2√ór) Standard scaling for LoRA updates dropout 0.05 Mild regularization to prevent adapter overfitting task_type SFT/PPO: CAUSAL_LM
RM: SEQ_CLS Matches the model head type for each step üéõÔ∏è Quantization Details # Setting Value Why Not Default? load_in_4bit True Reduces memory by 75% vs fp16 bnb_4bit_use_double_quant True Quantizes quantization constants (extra memory savings) bnb_4bit_quant_type &#34;nf4&#34; Normal Float 4-bit optimal for weights (vs uniform) bnb_4bit_compute_dtype bfloat16 Better numerical stability than fp16 for training üìà Optimization Settings # Setting Value Why Not Default? optim paged_adamw_8bit Memory-efficient optimizer for 4-bit training bf16 True Better gradient stability than fp16 gradient_checkpointing True Trades compute for memory (enables longer sequences) lr_scheduler_type &#34;cosine&#34; Smooth LR decay prevents abrupt training disruption warmup_ratio 0.03 Stabilizes initial training with 4-bit quantization max_grad_norm 0.3 Prevents gradient explosion in LoRA training üîÑ PPO-Specific (Step 3 Only) # Setting Value Why Not Default? mini_batch_size 1 Memory constraint during on-policy generation ppo_epochs 4 Multiple passes over collected experience init_kl_coef 0.1 Prevents policy from diverging too far from SFT adap_kl_ctrl True Dynamically adjusts KL penalty based on divergence gamma 1.0 No discounting (language has no clear episode structure) lam 0.95 GAE parameter balancing bias-variance in advantage cliprange 0.2 Limits policy update size (PPO core mechanism) vf_coef 0.1 Weight of value function loss vs policy loss Training Flow Summary # Llama-2-7b-hf (4-bit quantized) ‚Üì [Step 1: SFT] ‚Üê 20K chosen examples, LR=2e-4, LoRA r=16 ‚Üì ‚îú‚îÄ‚Üí [Step 2: RM] ‚Üê 50K preference pairs, LR=5e-5, outputs scalar ‚îÇ ‚Üì ‚îî‚îÄ‚Üí [Step 3: PPO] ‚Üê 20K prompts, LR=1e-6, KL=0.1 ‚Üì Final RLHF Model Key Metrics to Monitor # Step Primary Metric Danger Sign SFT Training loss ‚Üì Eval loss ‚Üë (overfitting) RM Ranking accuracy ‚Üë Reward always higher for longer text (length bias) PPO Mean reward ‚Üë KL &gt; 0.5 (policy collapse) Why These Specific Values? # Learning Rate Decay Pattern # SFT (2e-4): Highest LR for initial adaptation from base model RM (5e-5): Lower to preserve SFT knowledge while learning preferences PPO (1e-6): Tiny updates to avoid destroying alignment from RM Batch Size Strategy # Small per-device (1): GPU memory constraint with 7B model Large accumulation (16-32): Stabilizes gradients for contrastive learning (RM) PPO (8 rollouts): Enough diversity for policy gradient estimation Quantization Choices # 4-bit: Only option that fits 7B &#43; optimizer states in 24GB NF4: Specifically designed for neural network weight distributions Double quant: Squeezes extra ~1GB by quantizing quantization parameters bfloat16 compute: Prevents underflow in gradients during backprop LoRA Design # r=16: Sweet spot for 7B models (too low = capacity loss, too high = overfitting) alpha=32: Standard 2√ó scaling keeps update magnitudes reasonable All attention &#43; FFN: Covers both information routing and transformation PPO Parameters # KL penalty (0.1): Prevents catastrophic forgetting of SFT behavior Clip (0.2): Conservative updates reduce instability Gamma (1.0): No temporal discounting (each token equally important)">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="ai-workflows">
<title>Instruct Gpt Codes Params | AI Reasoning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.4ddc672eeb84245fa7864c180b2b6afa0c779ad1eedcf805d6cdf4d6d9e1195c.js" integrity="sha256-TdxnLuuEJF&#43;nhkwYCytq&#43;gx3mtHu3PgF1s301tnhGVw=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>AI Reasoning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ai-workflows/" class="">AI Reasoning Stack</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7f3f4cf59430750c2ad109248c8c879b" class="toggle"  />
    <label for="section-7f3f4cf59430750c2ad109248c8c879b" class="flex justify-between">
      <a href="/ai-workflows/data/" class="">Data</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fc566bddf8394fb4d0c5cff688d2febc" class="toggle"  />
    <label for="section-fc566bddf8394fb4d0c5cff688d2febc" class="flex justify-between">
      <a href="/ai-workflows/data/data-centric-ai/" class="">Data-Centric AI (DCAI)</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cff08f084db31c8732ef81d1fe1c4130" class="toggle"  />
    <label for="section-cff08f084db31c8732ef81d1fe1c4130" class="flex justify-between">
      <a href="/ai-workflows/genai/" class="">GenAI</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-63b806a012c3062adb6281022ca8468f" class="toggle"  />
    <label for="section-63b806a012c3062adb6281022ca8468f" class="flex justify-between">
      <a href="/ai-workflows/genai/5-day-genai-google-2025/" class="">5-Day GenAI with Google 2005</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day1_foundational_llm_text_generation/" class="">Day 1 - Foundational LLMs &amp; Text Generation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day1_prompt_engineering/" class="">Day 1 ‚Äì Prompt Engineering</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day2_embeddings_vectordb/" class="">Day 2 ‚Äì Embeddings &amp; Vector Databases</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day3_generative_agents/" class="">Day 3 ‚Äì Generative Agents</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day4_domainspecific_llms/" class="">Day 4 ‚Äì Domain-Specific LLMs</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day5_mlops/" class="">Day 5 ‚Äì MLOps for Generative AI</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/multimodel_llms/" class="">Multimodal LLMs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-17ba62e37c896ad50105fedeb71549dd" class="toggle"  />
    <label for="section-17ba62e37c896ad50105fedeb71549dd" class="flex justify-between">
      <a href="/ai-workflows/reasoning/" class="">Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd94e161670d28ecff992edf840d76e8" class="toggle"  />
    <label for="section-cd94e161670d28ecff992edf840d76e8" class="flex justify-between">
      <a href="/ai-workflows/reasoning/causality/" class="">Causality</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-ai/" class="">Causal AI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-inference/" class="">Causal Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7e468aa05ceb7c844f07a2e754606b76" class="toggle"  />
    <label for="section-7e468aa05ceb7c844f07a2e754606b76" class="flex justify-between">
      <a href="/ai-workflows/reasoning/graph-reasoning/" class="">Graph Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/graphrag/" class="">GraphRAG</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/knowledge-graphs/" class="">Knowledge Graphs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="toggle" checked />
    <label for="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="flex justify-between">
      <a href="/ai-workflows/rlhf/" class="">RLHF</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b1afcafdefac57f3420f64e23d73f05d" class="toggle" checked />
    <label for="section-b1afcafdefac57f3420f64e23d73f05d" class="flex justify-between">
      <a href="/ai-workflows/rlhf/rlhf2006/" class="">RLHF 2006</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/" class="active">Instruct Gpt Codes Params</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ca53d32fab0e1a54fdf5627349d86bfc" class="toggle"  />
    <label for="section-ca53d32fab0e1a54fdf5627349d86bfc" class="flex justify-between">
      <a href="/ai-workflows/eval/" class="">Eval</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/healthcare/" class="">Healthcare</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="toggle"  />
    <label for="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/" class="">Domain</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-45ba5974905f86df95925365835eadbb" class="toggle"  />
    <label for="section-45ba5974905f86df95925365835eadbb" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/ai-in-healthcare/" class="">AI in Healthcare</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/" class="">C2 Clinical Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/" class="">C3 ML Healthcare</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/" class="">C4 AI Evaluations</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c5_capstone/" class="">C5 Capstone Projects</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-9722ba71bf098ab02c3220d6e8d9056f" class="toggle"  />
    <label for="section-9722ba71bf098ab02c3220d6e8d9056f" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/hands-on-healthcare-data/" class="">Hands-On Healthcare Data</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/data/" class="">Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/clinical_ai/" class="">AI Applications</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ipark/" class="">Inhee Park, PhD - Resume</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>










  
<ul>
  
  <li>
    <a href="https://www.linkedin.com/in/inheepark/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄLinkedIn
      </a>
  </li>
  
  <li>
    <a href="https://github.com/imipark/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄGitHub
      </a>
  </li>
  
  <li>
    <a href="/posts/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄBlog
      </a>
  </li>
  
  <li>
    <a href="https://iparkirk.github.io"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄOld Web
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Instruct Gpt Codes Params</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#rlhf-pipeline-key-non-default-settings">RLHF Pipeline: Key Non-Default Settings</a>
      <ul>
        <li><a href="#critical-configuration-non-defaults-only">Critical Configuration (Non-Defaults Only)</a>
          <ul>
            <li><a href="#-models--architecture">üéØ Models &amp; Architecture</a></li>
            <li><a href="#-dataset-configuration">üìä Dataset Configuration</a></li>
            <li><a href="#-training-hyperparameters">‚öôÔ∏è Training Hyperparameters</a></li>
            <li><a href="#-lora-configuration">üîß LoRA Configuration</a></li>
            <li><a href="#-quantization-details">üéõÔ∏è Quantization Details</a></li>
            <li><a href="#-optimization-settings">üìà Optimization Settings</a></li>
            <li><a href="#-ppo-specific-step-3-only">üîÑ PPO-Specific (Step 3 Only)</a></li>
          </ul>
        </li>
        <li><a href="#training-flow-summary">Training Flow Summary</a></li>
        <li><a href="#key-metrics-to-monitor">Key Metrics to Monitor</a></li>
        <li><a href="#why-these-specific-values">Why These Specific Values?</a>
          <ul>
            <li><a href="#learning-rate-decay-pattern">Learning Rate Decay Pattern</a></li>
            <li><a href="#batch-size-strategy">Batch Size Strategy</a></li>
            <li><a href="#quantization-choices">Quantization Choices</a></li>
            <li><a href="#lora-design">LoRA Design</a></li>
            <li><a href="#ppo-parameters">PPO Parameters</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h1 id="rlhf-pipeline-key-non-default-settings">
  RLHF Pipeline: Key Non-Default Settings
  <a class="anchor" href="#rlhf-pipeline-key-non-default-settings">#</a>
</h1>
<h2 id="critical-configuration-non-defaults-only">
  Critical Configuration (Non-Defaults Only)
  <a class="anchor" href="#critical-configuration-non-defaults-only">#</a>
</h2>
<h3 id="-models--architecture">
  üéØ Models &amp; Architecture
  <a class="anchor" href="#-models--architecture">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Value</th>
          <th>Why Not Default?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Model Class</strong></td>
          <td>Step 1/3: <code>CausalLM</code><br>Step 2: <code>SequenceClassification</code></td>
          <td>Step 2 needs scalar reward output, not text generation</td>
      </tr>
      <tr>
          <td><strong>Quantization</strong></td>
          <td>4-bit QLoRA</td>
          <td>Fits 7B model in 24GB VRAM (vs 28GB for fp16)</td>
      </tr>
      <tr>
          <td><strong>num_labels</strong></td>
          <td>Step 2: <code>1</code></td>
          <td>Reward model outputs single scalar score</td>
      </tr>
  </tbody>
</table>
<h3 id="-dataset-configuration">
  üìä Dataset Configuration
  <a class="anchor" href="#-dataset-configuration">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Value</th>
          <th>Why Not Default?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Dataset Size</strong></td>
          <td>SFT: 20K, RM: 50K, PPO: 20K</td>
          <td>RM needs more data for robust preference learning</td>
      </tr>
      <tr>
          <td><strong>Data Format</strong></td>
          <td>SFT: chosen only<br>RM: chosen+rejected pairs<br>PPO: prompts only</td>
          <td>Each step requires different supervision signal</td>
      </tr>
      <tr>
          <td><strong>Train/Eval Split</strong></td>
          <td>Step 2: 5% eval</td>
          <td>Only RM needs validation to prevent reward hacking</td>
      </tr>
  </tbody>
</table>
<h3 id="-training-hyperparameters">
  ‚öôÔ∏è Training Hyperparameters
  <a class="anchor" href="#-training-hyperparameters">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Value</th>
          <th>Why Not Default?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Learning Rate</strong></td>
          <td>SFT: <code>2e-4</code><br>RM: <code>5e-5</code><br>PPO: <code>1e-6</code></td>
          <td>Decreasing LR prevents destabilizing previous training</td>
      </tr>
      <tr>
          <td><strong>Batch Size</strong></td>
          <td><code>1</code> (SFT/RM)<br><code>8</code> (PPO)</td>
          <td>Memory constraint; PPO needs multiple rollouts per update</td>
      </tr>
      <tr>
          <td><strong>Gradient Accumulation</strong></td>
          <td>SFT: 16<br>RM: 32</td>
          <td>Simulates larger batch sizes within memory limit</td>
      </tr>
      <tr>
          <td><strong>Effective Batch Size</strong></td>
          <td>SFT: 16<br>RM: 32<br>PPO: 8</td>
          <td>RM needs larger batches for stable ranking gradients</td>
      </tr>
  </tbody>
</table>
<h3 id="-lora-configuration">
  üîß LoRA Configuration
  <a class="anchor" href="#-lora-configuration">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Value</th>
          <th>Why Not Default?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>r</strong> (rank)</td>
          <td><code>16</code></td>
          <td>Balance between parameter efficiency and model capacity</td>
      </tr>
      <tr>
          <td><strong>alpha</strong></td>
          <td><code>32</code> (2√ór)</td>
          <td>Standard scaling for LoRA updates</td>
      </tr>
      <tr>
          <td><strong>dropout</strong></td>
          <td><code>0.05</code></td>
          <td>Mild regularization to prevent adapter overfitting</td>
      </tr>
      <tr>
          <td><strong>task_type</strong></td>
          <td>SFT/PPO: <code>CAUSAL_LM</code><br>RM: <code>SEQ_CLS</code></td>
          <td>Matches the model head type for each step</td>
      </tr>
  </tbody>
</table>
<h3 id="-quantization-details">
  üéõÔ∏è Quantization Details
  <a class="anchor" href="#-quantization-details">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Value</th>
          <th>Why Not Default?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>load_in_4bit</strong></td>
          <td><code>True</code></td>
          <td>Reduces memory by 75% vs fp16</td>
      </tr>
      <tr>
          <td><strong>bnb_4bit_use_double_quant</strong></td>
          <td><code>True</code></td>
          <td>Quantizes quantization constants (extra memory savings)</td>
      </tr>
      <tr>
          <td><strong>bnb_4bit_quant_type</strong></td>
          <td><code>&quot;nf4&quot;</code></td>
          <td>Normal Float 4-bit optimal for weights (vs uniform)</td>
      </tr>
      <tr>
          <td><strong>bnb_4bit_compute_dtype</strong></td>
          <td><code>bfloat16</code></td>
          <td>Better numerical stability than fp16 for training</td>
      </tr>
  </tbody>
</table>
<h3 id="-optimization-settings">
  üìà Optimization Settings
  <a class="anchor" href="#-optimization-settings">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Value</th>
          <th>Why Not Default?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>optim</strong></td>
          <td><code>paged_adamw_8bit</code></td>
          <td>Memory-efficient optimizer for 4-bit training</td>
      </tr>
      <tr>
          <td><strong>bf16</strong></td>
          <td><code>True</code></td>
          <td>Better gradient stability than fp16</td>
      </tr>
      <tr>
          <td><strong>gradient_checkpointing</strong></td>
          <td><code>True</code></td>
          <td>Trades compute for memory (enables longer sequences)</td>
      </tr>
      <tr>
          <td><strong>lr_scheduler_type</strong></td>
          <td><code>&quot;cosine&quot;</code></td>
          <td>Smooth LR decay prevents abrupt training disruption</td>
      </tr>
      <tr>
          <td><strong>warmup_ratio</strong></td>
          <td><code>0.03</code></td>
          <td>Stabilizes initial training with 4-bit quantization</td>
      </tr>
      <tr>
          <td><strong>max_grad_norm</strong></td>
          <td><code>0.3</code></td>
          <td>Prevents gradient explosion in LoRA training</td>
      </tr>
  </tbody>
</table>
<h3 id="-ppo-specific-step-3-only">
  üîÑ PPO-Specific (Step 3 Only)
  <a class="anchor" href="#-ppo-specific-step-3-only">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Value</th>
          <th>Why Not Default?</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>mini_batch_size</strong></td>
          <td><code>1</code></td>
          <td>Memory constraint during on-policy generation</td>
      </tr>
      <tr>
          <td><strong>ppo_epochs</strong></td>
          <td><code>4</code></td>
          <td>Multiple passes over collected experience</td>
      </tr>
      <tr>
          <td><strong>init_kl_coef</strong></td>
          <td><code>0.1</code></td>
          <td>Prevents policy from diverging too far from SFT</td>
      </tr>
      <tr>
          <td><strong>adap_kl_ctrl</strong></td>
          <td><code>True</code></td>
          <td>Dynamically adjusts KL penalty based on divergence</td>
      </tr>
      <tr>
          <td><strong>gamma</strong></td>
          <td><code>1.0</code></td>
          <td>No discounting (language has no clear episode structure)</td>
      </tr>
      <tr>
          <td><strong>lam</strong></td>
          <td><code>0.95</code></td>
          <td>GAE parameter balancing bias-variance in advantage</td>
      </tr>
      <tr>
          <td><strong>cliprange</strong></td>
          <td><code>0.2</code></td>
          <td>Limits policy update size (PPO core mechanism)</td>
      </tr>
      <tr>
          <td><strong>vf_coef</strong></td>
          <td><code>0.1</code></td>
          <td>Weight of value function loss vs policy loss</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="training-flow-summary">
  Training Flow Summary
  <a class="anchor" href="#training-flow-summary">#</a>
</h2>
<pre tabindex="0"><code>Llama-2-7b-hf (4-bit quantized)
       ‚Üì
   [Step 1: SFT]  ‚Üê 20K chosen examples, LR=2e-4, LoRA r=16
       ‚Üì
       ‚îú‚îÄ‚Üí [Step 2: RM]  ‚Üê 50K preference pairs, LR=5e-5, outputs scalar
       ‚îÇ        ‚Üì
       ‚îî‚îÄ‚Üí [Step 3: PPO]  ‚Üê 20K prompts, LR=1e-6, KL=0.1
            ‚Üì
     Final RLHF Model
</code></pre><hr>
<h2 id="key-metrics-to-monitor">
  Key Metrics to Monitor
  <a class="anchor" href="#key-metrics-to-monitor">#</a>
</h2>
<table>
  <thead>
      <tr>
          <th>Step</th>
          <th>Primary Metric</th>
          <th>Danger Sign</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>SFT</strong></td>
          <td>Training loss ‚Üì</td>
          <td>Eval loss ‚Üë (overfitting)</td>
      </tr>
      <tr>
          <td><strong>RM</strong></td>
          <td>Ranking accuracy ‚Üë</td>
          <td>Reward always higher for longer text (length bias)</td>
      </tr>
      <tr>
          <td><strong>PPO</strong></td>
          <td>Mean reward ‚Üë</td>
          <td>KL &gt; 0.5 (policy collapse)</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="why-these-specific-values">
  Why These Specific Values?
  <a class="anchor" href="#why-these-specific-values">#</a>
</h2>
<h3 id="learning-rate-decay-pattern">
  Learning Rate Decay Pattern
  <a class="anchor" href="#learning-rate-decay-pattern">#</a>
</h3>
<ul>
<li><strong>SFT (2e-4)</strong>: Highest LR for initial adaptation from base model</li>
<li><strong>RM (5e-5)</strong>: Lower to preserve SFT knowledge while learning preferences</li>
<li><strong>PPO (1e-6)</strong>: Tiny updates to avoid destroying alignment from RM</li>
</ul>
<h3 id="batch-size-strategy">
  Batch Size Strategy
  <a class="anchor" href="#batch-size-strategy">#</a>
</h3>
<ul>
<li><strong>Small per-device (1)</strong>: GPU memory constraint with 7B model</li>
<li><strong>Large accumulation (16-32)</strong>: Stabilizes gradients for contrastive learning (RM)</li>
<li><strong>PPO (8 rollouts)</strong>: Enough diversity for policy gradient estimation</li>
</ul>
<h3 id="quantization-choices">
  Quantization Choices
  <a class="anchor" href="#quantization-choices">#</a>
</h3>
<ul>
<li><strong>4-bit</strong>: Only option that fits 7B + optimizer states in 24GB</li>
<li><strong>NF4</strong>: Specifically designed for neural network weight distributions</li>
<li><strong>Double quant</strong>: Squeezes extra ~1GB by quantizing quantization parameters</li>
<li><strong>bfloat16 compute</strong>: Prevents underflow in gradients during backprop</li>
</ul>
<h3 id="lora-design">
  LoRA Design
  <a class="anchor" href="#lora-design">#</a>
</h3>
<ul>
<li><strong>r=16</strong>: Sweet spot for 7B models (too low = capacity loss, too high = overfitting)</li>
<li><strong>alpha=32</strong>: Standard 2√ó scaling keeps update magnitudes reasonable</li>
<li><strong>All attention + FFN</strong>: Covers both information routing and transformation</li>
</ul>
<h3 id="ppo-parameters">
  PPO Parameters
  <a class="anchor" href="#ppo-parameters">#</a>
</h3>
<ul>
<li><strong>KL penalty (0.1)</strong>: Prevents catastrophic forgetting of SFT behavior</li>
<li><strong>Clip (0.2)</strong>: Conservative updates reduce instability</li>
<li><strong>Gamma (1.0)</strong>: No temporal discounting (each token equally important)</li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#rlhf-pipeline-key-non-default-settings">RLHF Pipeline: Key Non-Default Settings</a>
      <ul>
        <li><a href="#critical-configuration-non-defaults-only">Critical Configuration (Non-Defaults Only)</a>
          <ul>
            <li><a href="#-models--architecture">üéØ Models &amp; Architecture</a></li>
            <li><a href="#-dataset-configuration">üìä Dataset Configuration</a></li>
            <li><a href="#-training-hyperparameters">‚öôÔ∏è Training Hyperparameters</a></li>
            <li><a href="#-lora-configuration">üîß LoRA Configuration</a></li>
            <li><a href="#-quantization-details">üéõÔ∏è Quantization Details</a></li>
            <li><a href="#-optimization-settings">üìà Optimization Settings</a></li>
            <li><a href="#-ppo-specific-step-3-only">üîÑ PPO-Specific (Step 3 Only)</a></li>
          </ul>
        </li>
        <li><a href="#training-flow-summary">Training Flow Summary</a></li>
        <li><a href="#key-metrics-to-monitor">Key Metrics to Monitor</a></li>
        <li><a href="#why-these-specific-values">Why These Specific Values?</a>
          <ul>
            <li><a href="#learning-rate-decay-pattern">Learning Rate Decay Pattern</a></li>
            <li><a href="#batch-size-strategy">Batch Size Strategy</a></li>
            <li><a href="#quantization-choices">Quantization Choices</a></li>
            <li><a href="#lora-design">LoRA Design</a></li>
            <li><a href="#ppo-parameters">PPO Parameters</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












