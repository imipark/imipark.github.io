<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RLHF 2006 on AI Reasoning</title>
    <link>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/</link>
    <description>Recent content in RLHF 2006 on AI Reasoning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/</guid>
      <description>&lt;h1 id=&#34;rlhf-pipeline-key-non-default-settings&#34;&gt;&#xA;  RLHF Pipeline: Key Non-Default Settings&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#rlhf-pipeline-key-non-default-settings&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h2 id=&#34;critical-configuration-non-defaults-only&#34;&gt;&#xA;  Critical Configuration (Non-Defaults Only)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#critical-configuration-non-defaults-only&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;-models--architecture&#34;&gt;&#xA;  ğŸ¯ Models &amp;amp; Architecture&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-models--architecture&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Setting&lt;/th&gt;&#xA;          &lt;th&gt;Value&lt;/th&gt;&#xA;          &lt;th&gt;Why Not Default?&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Model Class&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Step 1/3: &lt;code&gt;CausalLM&lt;/code&gt;&lt;br&gt;Step 2: &lt;code&gt;SequenceClassification&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Step 2 needs scalar reward output, not text generation&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Quantization&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;4-bit QLoRA&lt;/td&gt;&#xA;          &lt;td&gt;Fits 7B model in 24GB VRAM (vs 28GB for fp16)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;num_labels&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Step 2: &lt;code&gt;1&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Reward model outputs single scalar score&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;-dataset-configuration&#34;&gt;&#xA;  ğŸ“Š Dataset Configuration&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-dataset-configuration&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Setting&lt;/th&gt;&#xA;          &lt;th&gt;Value&lt;/th&gt;&#xA;          &lt;th&gt;Why Not Default?&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Dataset Size&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;SFT: 20K, RM: 50K, PPO: 20K&lt;/td&gt;&#xA;          &lt;td&gt;RM needs more data for robust preference learning&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Data Format&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;SFT: chosen only&lt;br&gt;RM: chosen+rejected pairs&lt;br&gt;PPO: prompts only&lt;/td&gt;&#xA;          &lt;td&gt;Each step requires different supervision signal&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Train/Eval Split&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Step 2: 5% eval&lt;/td&gt;&#xA;          &lt;td&gt;Only RM needs validation to prevent reward hacking&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;-training-hyperparameters&#34;&gt;&#xA;  âš™ï¸ Training Hyperparameters&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-training-hyperparameters&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Setting&lt;/th&gt;&#xA;          &lt;th&gt;Value&lt;/th&gt;&#xA;          &lt;th&gt;Why Not Default?&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;SFT: &lt;code&gt;2e-4&lt;/code&gt;&lt;br&gt;RM: &lt;code&gt;5e-5&lt;/code&gt;&lt;br&gt;PPO: &lt;code&gt;1e-6&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Decreasing LR prevents destabilizing previous training&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;1&lt;/code&gt; (SFT/RM)&lt;br&gt;&lt;code&gt;8&lt;/code&gt; (PPO)&lt;/td&gt;&#xA;          &lt;td&gt;Memory constraint; PPO needs multiple rollouts per update&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Gradient Accumulation&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;SFT: 16&lt;br&gt;RM: 32&lt;/td&gt;&#xA;          &lt;td&gt;Simulates larger batch sizes within memory limit&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Effective Batch Size&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;SFT: 16&lt;br&gt;RM: 32&lt;br&gt;PPO: 8&lt;/td&gt;&#xA;          &lt;td&gt;RM needs larger batches for stable ranking gradients&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;-lora-configuration&#34;&gt;&#xA;  ğŸ”§ LoRA Configuration&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-lora-configuration&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Setting&lt;/th&gt;&#xA;          &lt;th&gt;Value&lt;/th&gt;&#xA;          &lt;th&gt;Why Not Default?&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;r&lt;/strong&gt; (rank)&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;16&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Balance between parameter efficiency and model capacity&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;alpha&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;32&lt;/code&gt; (2Ã—r)&lt;/td&gt;&#xA;          &lt;td&gt;Standard scaling for LoRA updates&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;dropout&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;0.05&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Mild regularization to prevent adapter overfitting&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;task_type&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;SFT/PPO: &lt;code&gt;CAUSAL_LM&lt;/code&gt;&lt;br&gt;RM: &lt;code&gt;SEQ_CLS&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Matches the model head type for each step&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;-quantization-details&#34;&gt;&#xA;  ğŸ›ï¸ Quantization Details&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-quantization-details&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Setting&lt;/th&gt;&#xA;          &lt;th&gt;Value&lt;/th&gt;&#xA;          &lt;th&gt;Why Not Default?&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;load_in_4bit&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Reduces memory by 75% vs fp16&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;bnb_4bit_use_double_quant&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Quantizes quantization constants (extra memory savings)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;bnb_4bit_quant_type&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;&amp;quot;nf4&amp;quot;&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Normal Float 4-bit optimal for weights (vs uniform)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;bnb_4bit_compute_dtype&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;bfloat16&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Better numerical stability than fp16 for training&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;-optimization-settings&#34;&gt;&#xA;  ğŸ“ˆ Optimization Settings&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-optimization-settings&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Setting&lt;/th&gt;&#xA;          &lt;th&gt;Value&lt;/th&gt;&#xA;          &lt;th&gt;Why Not Default?&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;optim&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;paged_adamw_8bit&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Memory-efficient optimizer for 4-bit training&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;bf16&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Better gradient stability than fp16&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;gradient_checkpointing&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Trades compute for memory (enables longer sequences)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;lr_scheduler_type&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;&amp;quot;cosine&amp;quot;&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Smooth LR decay prevents abrupt training disruption&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;warmup_ratio&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;0.03&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Stabilizes initial training with 4-bit quantization&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;max_grad_norm&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;0.3&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Prevents gradient explosion in LoRA training&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;-ppo-specific-step-3-only&#34;&gt;&#xA;  ğŸ”„ PPO-Specific (Step 3 Only)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-ppo-specific-step-3-only&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Setting&lt;/th&gt;&#xA;          &lt;th&gt;Value&lt;/th&gt;&#xA;          &lt;th&gt;Why Not Default?&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;mini_batch_size&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Memory constraint during on-policy generation&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;ppo_epochs&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;4&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Multiple passes over collected experience&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;init_kl_coef&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;0.1&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Prevents policy from diverging too far from SFT&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;adap_kl_ctrl&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Dynamically adjusts KL penalty based on divergence&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;gamma&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;No discounting (language has no clear episode structure)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;lam&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;0.95&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;GAE parameter balancing bias-variance in advantage&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;cliprange&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;0.2&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Limits policy update size (PPO core mechanism)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;vf_coef&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;0.1&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Weight of value function loss vs policy loss&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;training-flow-summary&#34;&gt;&#xA;  Training Flow Summary&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#training-flow-summary&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Llama-2-7b-hf (4-bit quantized)&#xA;       â†“&#xA;   [Step 1: SFT]  â† 20K chosen examples, LR=2e-4, LoRA r=16&#xA;       â†“&#xA;       â”œâ”€â†’ [Step 2: RM]  â† 50K preference pairs, LR=5e-5, outputs scalar&#xA;       â”‚        â†“&#xA;       â””â”€â†’ [Step 3: PPO]  â† 20K prompts, LR=1e-6, KL=0.1&#xA;            â†“&#xA;     Final RLHF Model&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h2 id=&#34;key-metrics-to-monitor&#34;&gt;&#xA;  Key Metrics to Monitor&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#key-metrics-to-monitor&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Step&lt;/th&gt;&#xA;          &lt;th&gt;Primary Metric&lt;/th&gt;&#xA;          &lt;th&gt;Danger Sign&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;SFT&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Training loss â†“&lt;/td&gt;&#xA;          &lt;td&gt;Eval loss â†‘ (overfitting)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;RM&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Ranking accuracy â†‘&lt;/td&gt;&#xA;          &lt;td&gt;Reward always higher for longer text (length bias)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;PPO&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Mean reward â†‘&lt;/td&gt;&#xA;          &lt;td&gt;KL &amp;gt; 0.5 (policy collapse)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;why-these-specific-values&#34;&gt;&#xA;  Why These Specific Values?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#why-these-specific-values&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 id=&#34;learning-rate-decay-pattern&#34;&gt;&#xA;  Learning Rate Decay Pattern&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#learning-rate-decay-pattern&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;SFT (2e-4)&lt;/strong&gt;: Highest LR for initial adaptation from base model&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;RM (5e-5)&lt;/strong&gt;: Lower to preserve SFT knowledge while learning preferences&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;PPO (1e-6)&lt;/strong&gt;: Tiny updates to avoid destroying alignment from RM&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;batch-size-strategy&#34;&gt;&#xA;  Batch Size Strategy&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#batch-size-strategy&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Small per-device (1)&lt;/strong&gt;: GPU memory constraint with 7B model&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Large accumulation (16-32)&lt;/strong&gt;: Stabilizes gradients for contrastive learning (RM)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;PPO (8 rollouts)&lt;/strong&gt;: Enough diversity for policy gradient estimation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;quantization-choices&#34;&gt;&#xA;  Quantization Choices&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#quantization-choices&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;4-bit&lt;/strong&gt;: Only option that fits 7B + optimizer states in 24GB&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;NF4&lt;/strong&gt;: Specifically designed for neural network weight distributions&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Double quant&lt;/strong&gt;: Squeezes extra ~1GB by quantizing quantization parameters&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;bfloat16 compute&lt;/strong&gt;: Prevents underflow in gradients during backprop&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;lora-design&#34;&gt;&#xA;  LoRA Design&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#lora-design&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;r=16&lt;/strong&gt;: Sweet spot for 7B models (too low = capacity loss, too high = overfitting)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;alpha=32&lt;/strong&gt;: Standard 2Ã— scaling keeps update magnitudes reasonable&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;All attention + FFN&lt;/strong&gt;: Covers both information routing and transformation&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;ppo-parameters&#34;&gt;&#xA;  PPO Parameters&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ppo-parameters&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;KL penalty (0.1)&lt;/strong&gt;: Prevents catastrophic forgetting of SFT behavior&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Clip (0.2)&lt;/strong&gt;: Conservative updates reduce instability&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Gamma (1.0)&lt;/strong&gt;: No temporal discounting (each token equally important)&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Ch9. Instruction Fine-Tuning (IFT/SFT)</title>
      <link>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ch9_sft/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ch9_sft/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Ch9: Instruction Fine-Tuning (IFT/SFT)&#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-are-chat-templates-and-why-do-they-matter&#34;&gt;&#xA;  &lt;strong&gt;Q1: What are chat templates and why do they matter?&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-are-chat-templates-and-why-do-they-matter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Chat templates are formatting systems that structure conversations into a format language models can process. They use special tokens (like &lt;code&gt;&amp;lt;|im_start|&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;|im_end|&amp;gt;&lt;/code&gt;) to mark boundaries between different parts of the conversation.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;|im_start|&amp;gt;system&#xA;You are a helpful assistant&amp;lt;|im_end|&amp;gt;&#xA;&amp;lt;|im_start|&amp;gt;user&#xA;What is 2+2?&amp;lt;|im_end|&amp;gt;&#xA;&amp;lt;|im_start|&amp;gt;assistant&#xA;The answer is 4&amp;lt;|im_end|&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q2-what-are-the-three-message-roles-and-how-do-they-differ&#34;&gt;&#xA;  &lt;strong&gt;Q2: What are the three message roles and how do they differ?&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-what-are-the-three-message-roles-and-how-do-they-differ&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Preparation in RLHF -- Ch6 (Preference Data) vs Ch9 (SFT Data)</title>
      <link>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ch6_vs_ch9_data_comparison/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ch6_vs_ch9_data_comparison/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Data Prep in RLHF - Ch6 (Preference Data) vs Ch9 (SFT Data)&#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;data-preparation-comparison-ch6-vs-ch9&#34;&gt;&#xA;  &lt;strong&gt;Data Preparation Comparison: Ch6 vs Ch9&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#data-preparation-comparison-ch6-vs-ch9&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Aspect&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Ch6: Preference Data&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Ch9: SFT/IFT Data&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Data Structure&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;PAIRWISE comparisons&lt;/strong&gt;: &lt;code&gt;(prompt, chosen_response, rejected_response)&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;SINGLE examples&lt;/strong&gt;: &lt;code&gt;(prompt, good_response)&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Purpose&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Learn to &lt;strong&gt;JUDGE&lt;/strong&gt; which response is better&lt;/td&gt;&#xA;          &lt;td&gt;Learn to &lt;strong&gt;GENERATE&lt;/strong&gt; good responses&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Data Format Example&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;{&amp;quot;prompt&amp;quot;: &amp;quot;What is 2+2?&amp;quot;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;&amp;quot;chosen&amp;quot;: &amp;quot;The answer is 4&amp;quot;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;&amp;quot;rejected&amp;quot;: &amp;quot;5&amp;quot;}&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;{&amp;quot;prompt&amp;quot;: &amp;quot;What is 2+2?&amp;quot;,&lt;/code&gt;&lt;br&gt;&lt;code&gt;&amp;quot;response&amp;quot;: &amp;quot;The answer is 4&amp;quot;}&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Collection Method&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;- Side-by-side comparison UI&lt;br&gt;- Likert scales (5-point, 8-point)&lt;br&gt;- Thumbs up/down&lt;br&gt;- ChatBotArena&lt;/td&gt;&#xA;          &lt;td&gt;- Human-written high-quality examples&lt;br&gt;- Curated Q&amp;amp;A pairs&lt;br&gt;- Single demonstrations&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Data Source&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Human labelers&lt;/strong&gt; comparing responses&lt;br&gt;OR&lt;br&gt;&lt;strong&gt;Structured/Synthetic&lt;/strong&gt;:&lt;br&gt;- Correct vs incorrect (math)&lt;br&gt;- With constraint vs without&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Human-written&lt;/strong&gt; completions&lt;br&gt;OR&lt;br&gt;&lt;strong&gt;Curated&lt;/strong&gt; high-quality examples&lt;br&gt;OR&lt;br&gt;&lt;strong&gt;Synthetic&lt;/strong&gt; (from stronger models)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Signal Type&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Comparative/Relative&lt;/strong&gt;: Which is better?&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Absolute&lt;/strong&gt;: This is a good response&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Typical Dataset Size&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;~100K preference pairs (InstructGPT)&lt;/td&gt;&#xA;          &lt;td&gt;~10K-1M examples (InstructGPT: 10K, modern: ~1M)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Multi-turn Handling&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;- Preference on &lt;strong&gt;final turn&lt;/strong&gt; only&lt;br&gt;- Continue with &amp;ldquo;chosen&amp;rdquo; answer&lt;br&gt;- Mask previous turns from loss&lt;/td&gt;&#xA;          &lt;td&gt;- Each turn = separate training example&lt;br&gt;- Unroll N-turn â†’ N examples&lt;br&gt;- Mask prompts/previous turns&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Used to Train&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Ch7: Reward Model&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Ch9: SFT Model&lt;/strong&gt; (initial policy)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;What Gets Trained&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;L = -log(Ïƒ(r(chosen) - r(rejected)))&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;L = -log P(response|prompt)&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Next Stage Usage&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Ch11: RL training&lt;/strong&gt;&lt;br&gt;- Same/similar prompts can be reused&lt;br&gt;- RM provides scores&lt;/td&gt;&#xA;          &lt;td&gt;&lt;strong&gt;Ch7: RM base model&lt;/strong&gt;&lt;br&gt;&lt;strong&gt;Ch11: RL starting policy&lt;/strong&gt;&lt;br&gt;- Policy generates responses&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;key-insight-the-data-types-are-fundamentally-different&#34;&gt;&#xA;  &lt;strong&gt;Key Insight: The DATA TYPES are FUNDAMENTALLY DIFFERENT&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#key-insight-the-data-types-are-fundamentally-different&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ch9 SFT Data Says: &amp;ldquo;Here&amp;rsquo;s a good response. Learn to generate this.&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#8839ef&#34;&gt;&amp;#34;prompt&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#40a02b&#34;&gt;&amp;#34;Write a poem about goldfish&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#8839ef&#34;&gt;&amp;#34;response&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#40a02b&#34;&gt;&amp;#34;Golden swimmer, circling slow...&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;Ch6 Preference Data Says: &amp;ldquo;Between these two responses, A is better than B. Learn to prefer A.&amp;rdquo;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#8839ef&#34;&gt;&amp;#34;prompt&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#40a02b&#34;&gt;&amp;#34;Write a poem about goldfish&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#8839ef&#34;&gt;&amp;#34;chosen&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#40a02b&#34;&gt;&amp;#34;Golden swimmer, circling slow... (follows constraint)&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#8839ef&#34;&gt;&amp;#34;rejected&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#40a02b&#34;&gt;&amp;#34;In circles bright, the goldfish glides... (violates constraint)&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;&#xA;&lt;h3 id=&#34;the-complete-rlhf-pipeline&#34;&gt;&#xA;  &lt;strong&gt;The Complete RLHF Pipeline&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#the-complete-rlhf-pipeline&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre style=&#34;line-height: 1.0;&#34;&gt;&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚   Pretrained Model  â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;           â”‚&#xA;           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;           â†“                              â†“&#xA;    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;    â”‚  Ch9: SFT   â”‚              â”‚  Ch6: Collect â”‚&#xA;    â”‚             â”‚              â”‚  Preference   â”‚&#xA;    â”‚ Data:       â”‚              â”‚  Data         â”‚&#xA;    â”‚ (prompt,    â”‚              â”‚               â”‚&#xA;    â”‚  response)  â”‚              â”‚ Data:         â”‚&#xA;    â”‚             â”‚              â”‚ (prompt,      â”‚&#xA;    â”‚ Single good â”‚              â”‚  chosen,      â”‚&#xA;    â”‚ examples    â”‚              â”‚  rejected)    â”‚&#xA;    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚               â”‚&#xA;           â”‚                     â”‚ Comparisons   â”‚&#xA;           â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;           â”‚                             â”‚&#xA;           â†“                             â†“&#xA;    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;    â”‚  SFT Model  â”‚â”€â”€base modelâ”€â†’â”‚   Ch7: RM     â”‚&#xA;    â”‚  (Policy    â”‚              â”‚   Training    â”‚&#xA;    â”‚  starting   â”‚              â”‚               â”‚&#xA;    â”‚  point)     â”‚              â”‚ Uses Ch6 data â”‚&#xA;    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;           â”‚                             â”‚&#xA;           â”‚                             â”‚&#xA;           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                      â†“&#xA;              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;              â”‚  Ch11: RL     â”‚&#xA;              â”‚  Optimization â”‚&#xA;              â”‚               â”‚&#xA;              â”‚ Policy: Ch9   â”‚&#xA;              â”‚ Scorer: Ch7   â”‚&#xA;              â”‚ Prompts: Ch6  â”‚&#xA;              â”‚ (or similar)  â”‚&#xA;              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;&lt;/pre&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;summary&#34;&gt;&#xA;  &lt;strong&gt;Summary&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#summary&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;The fundamental difference between Ch6 and Ch9 data lies in their &lt;strong&gt;learning objectives&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>PPO in LLMs vs PPO in Walker2D</title>
      <link>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ppo_comparison/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ppo_comparison/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;ğŸ¤–ğŸ¦¿ Understanding PPO: From Language Generation to Robot Control â€” Code, Concepts, and Comparisons&#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Proximal Policy Optimization (PPO)&lt;/strong&gt; in both large language models (LLMs, e.g., GPT-style) and classical control environments (e.g., Walker2D), focusing on the structure of the PPO update and how actions are selected during inference.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1--ppo-step-call--argument-by-argument-breakdown&#34;&gt;&#xA;  1. ğŸ§¾ PPO &lt;code&gt;step()&lt;/code&gt; Call â€” Argument-by-Argument Breakdown&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1--ppo-step-call--argument-by-argument-breakdown&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ppo_trainer&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;.&lt;/span&gt;step(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    queries&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt;[input_ids[&lt;span style=&#34;color:#fe640b&#34;&gt;0&lt;/span&gt;]],       &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Prompt (tokenized) â€” represents the current state&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    responses&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt;[response_ids[&lt;span style=&#34;color:#fe640b&#34;&gt;0&lt;/span&gt;]],  &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Generated tokens â€” represents the action taken&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rewards&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt;[reward]              &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Scalar from reward model â€” score for that action&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mapping-to-classic-rl-walker2d&#34;&gt;&#xA;  Mapping to Classic RL (Walker2D)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mapping-to-classic-rl-walker2d&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;PPO Argument&lt;/th&gt;&#xA;          &lt;th&gt;ğŸ¤– LLM (RLHF)&lt;/th&gt;&#xA;          &lt;th&gt;ğŸ¦¿ Walker2D (Classic RL)&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;code&gt;queries = [input_ids[0]]&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Prompt as input (discrete tokenized state)&lt;/td&gt;&#xA;          &lt;td&gt;Robot&amp;rsquo;s continuous state (joint angles, velocities)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;code&gt;responses = [response_ids[0]]&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Generated tokens (sequence of actions)&lt;/td&gt;&#xA;          &lt;td&gt;Applied joint torques (vector of real numbers)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;code&gt;rewards = [reward]&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Reward model output (alignment score)&lt;/td&gt;&#xA;          &lt;td&gt;Environment reward (e.g., distance walked)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;2--action-selection-in-ppo&#34;&gt;&#xA;  2. ğŸ¯ Action Selection in PPO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2--action-selection-in-ppo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;How does the agent choose its next action, given a state/prompt?&lt;/p&gt;</description>
    </item>
    <item>
      <title>PPO vs DPO in RLHF</title>
      <link>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ppo_vs_dpo_comparison/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ppo_vs_dpo_comparison/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;PPO vs DPO in RLHF&#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;div style=&#34;display: grid; grid-template-columns: 1fr 1fr; gap: 20px;&#34;&gt;&#xA;&lt;div&gt;&#xA;&lt;h3 id=&#34;ppo-traditional-rlhf&#34;&gt;&#xA;  PPO (Traditional RLHF)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ppo-traditional-rlhf&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;core-concept&#34;&gt;&#xA;  Core Concept&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#core-concept&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&amp;ldquo;Let me &lt;strong&gt;generate new text&lt;/strong&gt;, score it with the reward model, and update my policy based on those scores.&amp;rdquo;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h4 id=&#34;pipeline&#34;&gt;&#xA;  Pipeline&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pipeline&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Step 1: Train Reward Model (Ch7)&#xA;   Preference Data â†’ Reward Model&#xA;   &#xA;Step 2: RL Optimization (Ch11)&#xA;   Policy generates text â†’ RM scores it &#xA;   â†’ PPO updates policy&#xA;   (Repeat with new generations)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;chapter-flow&#34;&gt;&#xA;  Chapter Flow&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#chapter-flow&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Ch6 (Preference Data)&#xA;    â†“&#xA;Ch7 (Reward Model Training)&#xA;    â†“&#xA;Ch11 (PPO RL Optimization)&#xA;    â†“&#xA;Final Model&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;example-code&#34;&gt;&#xA;  Example Code&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#example-code&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# PPO Training Loop&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8839ef&#34;&gt;for&lt;/span&gt; batch &lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;in&lt;/span&gt; prompts:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Generate NEW responses&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    responses &lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt; policy&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;.&lt;/span&gt;generate(batch)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Score with reward model&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rewards &lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt; reward_model(responses)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Compute advantages&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    advantages &lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt; compute_advantages(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        rewards, values&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    )&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Update with PPO&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    policy&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;.&lt;/span&gt;update_with_ppo(advantages)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;the-math&#34;&gt;&#xA;  The Math&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#the-math&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;J(Î¸) = E[min(ratio * A, &#xA;             clip(ratio, 1-Îµ, 1+Îµ) * A)] &#xA;       - Î² * D_KL(Ï€ || Ï€_ref)&#xA;&#xA;Where:&#xA;- ratio = Ï€_Î¸(a|s) / Ï€_old(a|s)&#xA;- A = advantage&#xA;- clip prevents large updates&#xA;- D_KL = distance from reference&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;pros&#34;&gt;&#xA;  Pros&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pros&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;âœ… &lt;strong&gt;Online learning&lt;/strong&gt;: Discover new responses&lt;/li&gt;&#xA;&lt;li&gt;âœ… &lt;strong&gt;Best performance&lt;/strong&gt;: Highest quality&lt;/li&gt;&#xA;&lt;li&gt;âœ… &lt;strong&gt;Adaptive&lt;/strong&gt;: Uses current policy&lt;/li&gt;&#xA;&lt;li&gt;âœ… &lt;strong&gt;Proven&lt;/strong&gt;: ChatGPT, Claude, etc.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;cons&#34;&gt;&#xA;  Cons&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#cons&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;âŒ &lt;strong&gt;Complex&lt;/strong&gt;: RM + value + PPO&lt;/li&gt;&#xA;&lt;li&gt;âŒ &lt;strong&gt;Expensive&lt;/strong&gt;: Multiple models&lt;/li&gt;&#xA;&lt;li&gt;âŒ &lt;strong&gt;High memory&lt;/strong&gt;: RM + policy + ref&lt;/li&gt;&#xA;&lt;li&gt;âŒ &lt;strong&gt;Hard to tune&lt;/strong&gt;: Many hyperparameters&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;best-for&#34;&gt;&#xA;  Best For&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#best-for&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ğŸ¯ Maximum performance&lt;/li&gt;&#xA;&lt;li&gt;ğŸ¯ Large compute budgets&lt;/li&gt;&#xA;&lt;li&gt;ğŸ¯ Frontier models&lt;/li&gt;&#xA;&lt;li&gt;ğŸ¯ Continuous improvement&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;examples&#34;&gt;&#xA;  Examples&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#examples&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ChatGPT (OpenAI)&lt;/li&gt;&#xA;&lt;li&gt;Claude (Anthropic)&lt;/li&gt;&#xA;&lt;li&gt;InstructGPT (OpenAI)&lt;/li&gt;&#xA;&lt;li&gt;DeepSeek R1&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/div&gt;&#xA;&lt;div&gt;&#xA;&lt;h3 id=&#34;dpo-direct-alignment&#34;&gt;&#xA;  DPO (Direct Alignment)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#dpo-direct-alignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h4 id=&#34;core-concept-1&#34;&gt;&#xA;  Core Concept&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#core-concept-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h4&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&amp;ldquo;Let me &lt;strong&gt;directly learn&lt;/strong&gt; from the preference data without generating anything new.&amp;rdquo;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Single GPUT (RTX4090) RLHF Training Pipeline w/ TRL</title>
      <link>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/instruct_gpt_architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/instruct_gpt_architecture/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Single GPUT (RTX4090) RLHF Training Pipeline w/ TRL&#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;pre style=&#34;line-height: 1.0;&#34;&gt;&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚                    Anthropic/hh-rlhf Dataset                    â”‚&#xA;â”‚  160k examples with &#34;chosen&#34; and &#34;rejected&#34; responses           â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;               â”‚                        â”‚&#xA;               â”‚ &#34;chosen&#34; only          â”‚ preference pairs&#xA;               â”‚ (20k subset)           â”‚ (50k subset)&#xA;               â†“                        â”‚&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚&#xA;â”‚   meta-llama/Llama-2-7b-hf       â”‚    â”‚&#xA;â”‚   (4-bit quantized base)         â”‚    â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚&#xA;               â†“                        â”‚&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚              STEP 1: SFT Training                â”‚&#xA;â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚&#xA;â”‚  â”‚ Input:  &#34;chosen&#34; responses (20k)           â”‚  â”‚&#xA;â”‚  â”‚ Loss:   Cross-entropy on completions       â”‚  â”‚&#xA;â”‚  â”‚ Metric: Perplexity â†’ 3.3-4.5               â”‚  â”‚&#xA;â”‚  â”‚ Memory: 18-20 GB                           â”‚  â”‚&#xA;â”‚  â”‚ Time:   2-4 hours                          â”‚  â”‚&#xA;â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;               â†“&#xA;       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;       â”‚  SFT Model    â”‚ (~50 MB LoRA adapters)&#xA;       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;               â”‚&#xA;       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;       â”‚                        â”‚                 â”‚&#xA;       â†“                        â†“                 â†“&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚ Policy (PPO)     â”‚   â”‚ Reference (PPO) â”‚   â”‚ RM Base      â”‚&#xA;â”‚ +value head      â”‚   â”‚ frozen copy     â”‚   â”‚ +reward head â”‚&#xA;â”‚ trainable        â”‚   â”‚ for KL penalty  â”‚   â”‚              â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;       â”‚                        â”‚                   â”‚ + pairs (50k)&#xA;       â”‚                        â”‚                   â†“&#xA;       â”‚                        â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;       â”‚                        â”‚         â”‚  STEP 2: Reward Model       â”‚&#xA;       â”‚                        â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚&#xA;       â”‚                        â”‚         â”‚  â”‚ Input:  Pairs (50k)    â”‚ â”‚&#xA;       â”‚                        â”‚         â”‚  â”‚ Loss:   Ranking loss   â”‚ â”‚&#xA;       â”‚                        â”‚         â”‚  â”‚ Metric: Accuracy &gt;70%  â”‚ â”‚&#xA;       â”‚                        â”‚         â”‚  â”‚ Memory: 20-22 GB       â”‚ â”‚&#xA;       â”‚                        â”‚         â”‚  â”‚ Time:   3-6 hours      â”‚ â”‚&#xA;       â”‚                        â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚&#xA;       â”‚                        â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;       â”‚                        â”‚                    â†“&#xA;       â”‚                        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;       â”‚                        â”‚              â”‚ Reward Model â”‚ (frozen scorer)&#xA;       â”‚                        â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;       â”‚                        â”‚                     â”‚&#xA;       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                                â”‚&#xA;                + prompts (20k) â”‚&#xA;                                â†“&#xA;       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;       â”‚         STEP 3: PPO RLHF Optimization          â”‚&#xA;       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚&#xA;       â”‚  â”‚ Input:  Prompts (20k)                    â”‚  â”‚&#xA;       â”‚  â”‚         Policy + Reference + RM          â”‚  â”‚&#xA;       â”‚  â”‚ Loss:   PPO clipped + KL penalty         â”‚  â”‚&#xA;       â”‚  â”‚ Metric: Mean reward â†‘, KL 0.1-0.3        â”‚  â”‚&#xA;       â”‚  â”‚ Memory: 22-24 GB                         â”‚  â”‚&#xA;       â”‚  â”‚ Time:   6-12 hours                       â”‚  â”‚&#xA;       â”‚  â”‚ Loop:   1000 PPO steps                   â”‚  â”‚&#xA;       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚&#xA;       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                               â†“&#xA;                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;                    â”‚  Final RLHF Model   â”‚&#xA;                    â”‚  (LoRA adapters)    â”‚&#xA;                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;memory-usage-breakdown-rtx-4090---24-gb&#34;&gt;&#xA;  Memory Usage Breakdown (RTX 4090 - 24 GB)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#memory-usage-breakdown-rtx-4090---24-gb&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Component                  Memory      Notes&#xA;â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€&#xA;Base Model (4-bit)         ~3.5 GB    Llama-2-7B in NF4&#xA;LoRA Adapters              ~0.05 GB   r=16, small footprint&#xA;Optimizer States           ~8-10 GB   Paged AdamW 8-bit&#xA;Activations                ~6-8 GB    With gradient checkpointing&#xA;KV Cache (PPO generation)  ~2-4 GB    During generation phase&#xA;â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€&#xA;Total                      18-24 GB   Fits on RTX 4090!&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;training-metrics-timeline&#34;&gt;&#xA;  Training Metrics Timeline&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#training-metrics-timeline&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre style=&#34;line-height: 1.0;&#34;&gt;&#xA;                SFT                    RM                    PPO&#xA;Time:       0hâ”€â”€â”€â”€4h              4hâ”€â”€â”€â”€10h            10hâ”€â”€â”€â”€22h&#xA;            â”‚                     â”‚                    â”‚&#xA;Loss:       2.5 â†’ 1.2            0.69 â†’ 0.35           -&#xA;Perplexity: 12 â†’ 3.3             -                     -&#xA;Accuracy:   -                    50% â†’ 72%             -&#xA;Reward:     -                    -                     +2 â†’ +8&#xA;KL:         -                    -                     0.05 â†’ 0.25&#xA;            â”‚                     â”‚                    â”‚&#xA;Output:     SFT Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Reward Model â”€â”€â”€â”€â”€â†’ RLHF Model&#xA;&lt;/pre&gt;&#xA;&lt;h3 id=&#34;key-relationships&#34;&gt;&#xA;  Key Relationships&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#key-relationships&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;SFT â†’ RM&lt;/strong&gt;: Warm start (better initialization than random)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;SFT â†’ Policy&lt;/strong&gt;: Direct inheritance (then optimized)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;SFT â†’ Reference&lt;/strong&gt;: Frozen copy (anchor for KL penalty)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;RM â†’ PPO&lt;/strong&gt;: Provides reward signal (quality score)&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;loss-functions&#34;&gt;&#xA;  Loss Functions&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#loss-functions&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;SFT Loss:&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Complete InstructGPT Recipe (Ch 4.2.1)</title>
      <link>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/instruct_gpt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/rlhf/rlhf2006/instruct_gpt/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;The Complete Instruct_GPT Recipe (Ch 4.2.1)&#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;h3 id=&#34;sequential-and-two-separate-datasets-for-rlhf&#34;&gt;&#xA;  Sequential and Two Separate Datasets for RLHF&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#sequential-and-two-separate-datasets-for-rlhf&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Attribute&lt;/th&gt;&#xA;          &lt;th&gt;Dataset 1: comes 1st&lt;/th&gt;&#xA;          &lt;th&gt;Dataset 2: comes next&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Size&lt;/td&gt;&#xA;          &lt;td&gt;~10K examples (InstructGPT), ~1M modern&lt;/td&gt;&#xA;          &lt;td&gt;~100K preference pairs (InstructGPT)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Format&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;(prompt, good_response)&lt;/code&gt; - SINGLE examples&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;(prompt, chosen, rejected)&lt;/code&gt; - PAIRWISE comparisons&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Source&lt;/td&gt;&#xA;          &lt;td&gt;Human-written OR synthetic from strong models&lt;/td&gt;&#xA;          &lt;td&gt;Human labelers comparing SFT model outputs&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Purpose&lt;/td&gt;&#xA;          &lt;td&gt;Teach the model HOW TO RESPOND in chat format&lt;/td&gt;&#xA;          &lt;td&gt;Teach what GOOD vs BAD responses look like&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;When Collected&lt;/td&gt;&#xA;          &lt;td&gt;BEFORE preference data collection&lt;/td&gt;&#xA;          &lt;td&gt;AFTER SFT model exists (use SFT to generate responses)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Used to Train&lt;/td&gt;&#xA;          &lt;td&gt;SFT Model (Ch9)&lt;/td&gt;&#xA;          &lt;td&gt;Reward Model (Ch7)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Example&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;{ &amp;quot;prompt&amp;quot;: &amp;quot;What is machine learning?&amp;quot;, &amp;quot;response&amp;quot;: &amp;quot;Machine learning is a branch of AI that...&amp;quot; }&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;&lt;code&gt;{&amp;quot;prompt&amp;quot;: &amp;quot;What is machine learning?&amp;quot;, &amp;quot;chosen&amp;quot;: &amp;quot;Machine learning is a branch of AI that enables...&amp;quot;, &amp;quot;rejected&amp;quot;: &amp;quot;ML is when computers learn stuff.&amp;quot; }&lt;/code&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;when-to-prepare-1st-and-2nd-dataset&#34;&gt;&#xA;  when to prepare 1st and 2nd dataset&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#when-to-prepare-1st-and-2nd-dataset&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre style=&#34;line-height: 1.0;&#34;&gt;&#xA;         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;         â”‚  Ch9 SFT â”‚â† FIRST: Prepare SFT data&#xA;         â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜&#xA;             â”‚&#xA;             â†“&#xA;      [Train SFT Model]&#xA;             â”‚&#xA;             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;             â†“              â†“&#xA;      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     Generate responses&#xA;      â”‚ Use in â”‚     for humans to compare&#xA;      â”‚  Ch7   â”‚            â”‚&#xA;      â”‚  RM    â”‚            â†“&#xA;      â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;          â”‚          â”‚Ch6 Pref Data â”‚â† SECOND: Collect preferences&#xA;          â”‚          â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;          â”‚              â”‚&#xA;          â†“              â†“&#xA;         Ch7 RM         Use RM in Ch11&#xA;          â”‚              â”‚&#xA;          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                 â†“&#xA;               Ch11 RL(PPO)&#xA;&lt;/pre&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;the-complete-instructgpt-recipe-ch-421&#34;&gt;&#xA;  The Complete InstructGPT Recipe (Ch 4.2.1)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#the-complete-instructgpt-recipe-ch-421&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;pre style=&#34;line-height: 1.0;&#34;&gt;&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚                    Pretrained Base Model                â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                       â”‚&#xA;                       â†“&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚  STEP 1: Ch9 SFT                                       â”‚&#xA;â”‚  â€¢ Prepare single good examples                        â”‚&#xA;â”‚  â€¢ Train model on (prompt, response) pairs             â”‚&#xA;â”‚  â€¢ Output: SFT Model (can generate responses)          â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                       â”‚&#xA;                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;                       â†“                  â†“&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚  STEP 2a: Ch6 Data Collectionâ”‚  â”‚ STEP 2b: Ch7 RM     â”‚&#xA;â”‚  â€¢ Use SFT to generate       â”‚  â”‚ â€¢ Start from SFT    â”‚&#xA;â”‚  â€¢ Humans compare outputs    â”‚â”€â†’â”‚ â€¢ Train on Ch6 data â”‚&#xA;â”‚  â€¢ Create preference pairs   â”‚  â”‚ â€¢ Output: RM        â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                                             â”‚&#xA;                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;                       â†“&#xA;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”&#xA;â”‚  STEP 3: Ch11 RL Optimization                          â”‚&#xA;â”‚  â€¢ Policy: SFT Model (from Step 1)                     â”‚&#xA;â”‚  â€¢ Scorer: Reward Model (from Step 2)                  â”‚&#xA;â”‚  â€¢ Optimize policy using RM feedback                   â”‚&#xA;â”‚  â€¢ Output: Final RLHF-trained Model                    â”‚&#xA;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜&#xA;&lt;/pre&gt;</description>
    </item>
  </channel>
</rss>
