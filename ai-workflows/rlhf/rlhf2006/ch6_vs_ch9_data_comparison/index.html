<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="


Data Prep in RLHF - Ch6 (Preference Data) vs Ch9 (SFT Data)




  Data Preparation Comparison: Ch6 vs Ch9
  #


  
      
          Aspect
          Ch6: Preference Data
          Ch9: SFT/IFT Data
      
  
  
      
          Data Structure
          PAIRWISE comparisons: (prompt, chosen_response, rejected_response)
          SINGLE examples: (prompt, good_response)
      
      
          Purpose
          Learn to JUDGE which response is better
          Learn to GENERATE good responses
      
      
          Data Format Example
          {&quot;prompt&quot;: &quot;What is 2&#43;2?&quot;,&quot;chosen&quot;: &quot;The answer is 4&quot;,&quot;rejected&quot;: &quot;5&quot;}
          {&quot;prompt&quot;: &quot;What is 2&#43;2?&quot;,&quot;response&quot;: &quot;The answer is 4&quot;}
      
      
          Collection Method
          - Side-by-side comparison UI- Likert scales (5-point, 8-point)- Thumbs up/down- ChatBotArena
          - Human-written high-quality examples- Curated Q&amp;A pairs- Single demonstrations
      
      
          Data Source
          Human labelers comparing responsesORStructured/Synthetic:- Correct vs incorrect (math)- With constraint vs without
          Human-written completionsORCurated high-quality examplesORSynthetic (from stronger models)
      
      
          Signal Type
          Comparative/Relative: Which is better?
          Absolute: This is a good response
      
      
          Typical Dataset Size
          ~100K preference pairs (InstructGPT)
          ~10K-1M examples (InstructGPT: 10K, modern: ~1M)
      
      
          Multi-turn Handling
          - Preference on final turn only- Continue with &ldquo;chosen&rdquo; answer- Mask previous turns from loss
          - Each turn = separate training example- Unroll N-turn → N examples- Mask prompts/previous turns
      
      
          Used to Train
          Ch7: Reward Model
          Ch9: SFT Model (initial policy)
      
      
          What Gets Trained
          L = -log(σ(r(chosen) - r(rejected)))
          L = -log P(response|prompt)
      
      
          Next Stage Usage
          Ch11: RL training- Same/similar prompts can be reused- RM provides scores
          Ch7: RM base modelCh11: RL starting policy- Policy generates responses
      
  



  Key Insight: The DATA TYPES are FUNDAMENTALLY DIFFERENT
  #


Ch9 SFT Data Says: &ldquo;Here&rsquo;s a good response. Learn to generate this.&rdquo;

{
  &#34;prompt&#34;: &#34;Write a poem about goldfish&#34;,
  &#34;response&#34;: &#34;Golden swimmer, circling slow...&#34;
}

Ch6 Preference Data Says: &ldquo;Between these two responses, A is better than B. Learn to prefer A.&rdquo;

{
  &#34;prompt&#34;: &#34;Write a poem about goldfish&#34;,
  &#34;chosen&#34;: &#34;Golden swimmer, circling slow... (follows constraint)&#34;,
  &#34;rejected&#34;: &#34;In circles bright, the goldfish glides... (violates constraint)&#34;
}


  The Complete RLHF Pipeline
  #


┌─────────────────────┐
│   Pretrained Model  │
└──────────┬──────────┘
           │
           ├──────────────────────────────┐
           ↓                              ↓
    ┌─────────────┐              ┌───────────────┐
    │  Ch9: SFT   │              │  Ch6: Collect │
    │             │              │  Preference   │
    │ Data:       │              │  Data         │
    │ (prompt,    │              │               │
    │  response)  │              │ Data:         │
    │             │              │ (prompt,      │
    │ Single good │              │  chosen,      │
    │ examples    │              │  rejected)    │
    └──────┬──────┘              │               │
           │                     │ Comparisons   │
           │                     └───────┬───────┘
           │                             │
           ↓                             ↓
    ┌─────────────┐              ┌───────────────┐
    │  SFT Model  │──base model─→│   Ch7: RM     │
    │  (Policy    │              │   Training    │
    │  starting   │              │               │
    │  point)     │              │ Uses Ch6 data │
    └──────┬──────┘              └───────┬───────┘
           │                             │
           │                             │
           └──────────┬──────────────────┘
                      ↓
              ┌───────────────┐
              │  Ch11: RL     │
              │  Optimization │
              │               │
              │ Policy: Ch9   │
              │ Scorer: Ch7   │
              │ Prompts: Ch6  │
              │ (or similar)  │
              └───────────────┘



  Summary
  #

The fundamental difference between Ch6 and Ch9 data lies in their learning objectives:">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ch6_vs_ch9_data_comparison/">
  <meta property="og:site_name" content="AI Reasoning">
  <meta property="og:title" content="Data Preparation in RLHF -- Ch6 (Preference Data) vs Ch9 (SFT Data)">
  <meta property="og:description" content="Data Prep in RLHF - Ch6 (Preference Data) vs Ch9 (SFT Data) Data Preparation Comparison: Ch6 vs Ch9 # Aspect Ch6: Preference Data Ch9: SFT/IFT Data Data Structure PAIRWISE comparisons: (prompt, chosen_response, rejected_response) SINGLE examples: (prompt, good_response) Purpose Learn to JUDGE which response is better Learn to GENERATE good responses Data Format Example {&#34;prompt&#34;: &#34;What is 2&#43;2?&#34;,
&#34;chosen&#34;: &#34;The answer is 4&#34;,
&#34;rejected&#34;: &#34;5&#34;} {&#34;prompt&#34;: &#34;What is 2&#43;2?&#34;,
&#34;response&#34;: &#34;The answer is 4&#34;} Collection Method - Side-by-side comparison UI
- Likert scales (5-point, 8-point)
- Thumbs up/down
- ChatBotArena - Human-written high-quality examples
- Curated Q&amp;A pairs
- Single demonstrations Data Source Human labelers comparing responses
OR
Structured/Synthetic:
- Correct vs incorrect (math)
- With constraint vs without Human-written completions
OR
Curated high-quality examples
OR
Synthetic (from stronger models) Signal Type Comparative/Relative: Which is better? Absolute: This is a good response Typical Dataset Size ~100K preference pairs (InstructGPT) ~10K-1M examples (InstructGPT: 10K, modern: ~1M) Multi-turn Handling - Preference on final turn only
- Continue with “chosen” answer
- Mask previous turns from loss - Each turn = separate training example
- Unroll N-turn → N examples
- Mask prompts/previous turns Used to Train Ch7: Reward Model Ch9: SFT Model (initial policy) What Gets Trained L = -log(σ(r(chosen) - r(rejected))) L = -log P(response|prompt) Next Stage Usage Ch11: RL training
- Same/similar prompts can be reused
- RM provides scores Ch7: RM base model
Ch11: RL starting policy
- Policy generates responses Key Insight: The DATA TYPES are FUNDAMENTALLY DIFFERENT # Ch9 SFT Data Says: “Here’s a good response. Learn to generate this.” { &#34;prompt&#34;: &#34;Write a poem about goldfish&#34;, &#34;response&#34;: &#34;Golden swimmer, circling slow...&#34; } Ch6 Preference Data Says: “Between these two responses, A is better than B. Learn to prefer A.” { &#34;prompt&#34;: &#34;Write a poem about goldfish&#34;, &#34;chosen&#34;: &#34;Golden swimmer, circling slow... (follows constraint)&#34;, &#34;rejected&#34;: &#34;In circles bright, the goldfish glides... (violates constraint)&#34; } The Complete RLHF Pipeline # ┌─────────────────────┐ │ Pretrained Model │ └──────────┬──────────┘ │ ├──────────────────────────────┐ ↓ ↓ ┌─────────────┐ ┌───────────────┐ │ Ch9: SFT │ │ Ch6: Collect │ │ │ │ Preference │ │ Data: │ │ Data │ │ (prompt, │ │ │ │ response) │ │ Data: │ │ │ │ (prompt, │ │ Single good │ │ chosen, │ │ examples │ │ rejected) │ └──────┬──────┘ │ │ │ │ Comparisons │ │ └───────┬───────┘ │ │ ↓ ↓ ┌─────────────┐ ┌───────────────┐ │ SFT Model │──base model─→│ Ch7: RM │ │ (Policy │ │ Training │ │ starting │ │ │ │ point) │ │ Uses Ch6 data │ └──────┬──────┘ └───────┬───────┘ │ │ │ │ └──────────┬──────────────────┘ ↓ ┌───────────────┐ │ Ch11: RL │ │ Optimization │ │ │ │ Policy: Ch9 │ │ Scorer: Ch7 │ │ Prompts: Ch6 │ │ (or similar) │ └───────────────┘ Summary # The fundamental difference between Ch6 and Ch9 data lies in their learning objectives:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="ai-workflows">
<title>Data Preparation in RLHF -- Ch6 (Preference Data) vs Ch9 (SFT Data) | AI Reasoning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ch6_vs_ch9_data_comparison/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.0173bbefee938ca18415a40e44b93e080353b5ef582efcbf2ea5007cdddc86e0.js" integrity="sha256-AXO77&#43;6TjKGEFaQORLk&#43;CANTte9YLvy/LqUAfN3chuA=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>AI Reasoning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ai-workflows/" class="">AI Reasoning Stack</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7f3f4cf59430750c2ad109248c8c879b" class="toggle"  />
    <label for="section-7f3f4cf59430750c2ad109248c8c879b" class="flex justify-between">
      <a href="/ai-workflows/data/" class="">Data</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fc566bddf8394fb4d0c5cff688d2febc" class="toggle"  />
    <label for="section-fc566bddf8394fb4d0c5cff688d2febc" class="flex justify-between">
      <a href="/ai-workflows/data/data-centric-ai/" class="">Data-Centric AI (DCAI)</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cff08f084db31c8732ef81d1fe1c4130" class="toggle"  />
    <label for="section-cff08f084db31c8732ef81d1fe1c4130" class="flex justify-between">
      <a href="/ai-workflows/genai/" class="">GenAI</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-63b806a012c3062adb6281022ca8468f" class="toggle"  />
    <label for="section-63b806a012c3062adb6281022ca8468f" class="flex justify-between">
      <a href="/ai-workflows/genai/5-day-genai-google-2025/" class="">5-Day GenAI with Google 2005</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day1_foundational_llm_text_generation/" class="">Day 1 - Foundational LLMs &amp; Text Generation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day1_prompt_engineering/" class="">Day 1 – Prompt Engineering</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day2_embeddings_vectordb/" class="">Day 2 – Embeddings &amp; Vector Databases</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day3_generative_agents/" class="">Day 3 – Generative Agents</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day4_domainspecific_llms/" class="">Day 4 – Domain-Specific LLMs</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day5_mlops/" class="">Day 5 – MLOps for Generative AI</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/multimodel_llms/" class="">Multimodal LLMs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-17ba62e37c896ad50105fedeb71549dd" class="toggle"  />
    <label for="section-17ba62e37c896ad50105fedeb71549dd" class="flex justify-between">
      <a href="/ai-workflows/reasoning/" class="">Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd94e161670d28ecff992edf840d76e8" class="toggle"  />
    <label for="section-cd94e161670d28ecff992edf840d76e8" class="flex justify-between">
      <a href="/ai-workflows/reasoning/causality/" class="">Causality</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-ai/" class="">Causal AI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-inference/" class="">Causal Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7e468aa05ceb7c844f07a2e754606b76" class="toggle"  />
    <label for="section-7e468aa05ceb7c844f07a2e754606b76" class="flex justify-between">
      <a href="/ai-workflows/reasoning/graph-reasoning/" class="">Graph Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/graphrag/" class="">GraphRAG</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/knowledge-graphs/" class="">Knowledge Graphs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="toggle" checked />
    <label for="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="flex justify-between">
      <a href="/ai-workflows/rlhf/" class="">RLHF</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b1afcafdefac57f3420f64e23d73f05d" class="toggle" checked />
    <label for="section-b1afcafdefac57f3420f64e23d73f05d" class="flex justify-between">
      <a href="/ai-workflows/rlhf/rlhf2006/" class="">RLHF 2006</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/" class="">Instruct Gpt Codes Params</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ca53d32fab0e1a54fdf5627349d86bfc" class="toggle"  />
    <label for="section-ca53d32fab0e1a54fdf5627349d86bfc" class="flex justify-between">
      <a href="/ai-workflows/eval/" class="">Eval</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/healthcare/" class="">Healthcare</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="toggle"  />
    <label for="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/" class="">Domain</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-45ba5974905f86df95925365835eadbb" class="toggle"  />
    <label for="section-45ba5974905f86df95925365835eadbb" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/ai-in-healthcare/" class="">AI in Healthcare</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/" class="">C2 Clinical Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/" class="">C3 ML Healthcare</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/" class="">C4 AI Evaluations</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c5_capstone/" class="">C5 Capstone Projects</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-9722ba71bf098ab02c3220d6e8d9056f" class="toggle"  />
    <label for="section-9722ba71bf098ab02c3220d6e8d9056f" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/hands-on-healthcare-data/" class="">Hands-On Healthcare Data</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/data/" class="">Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/clinical_ai/" class="">AI Applications</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ipark/" class="">Inhee Park, PhD - Resume</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>










  
<ul>
  
  <li>
    <a href="https://www.linkedin.com/in/inheepark/"  target="_blank" rel="noopener">
        ╰──LinkedIn
      </a>
  </li>
  
  <li>
    <a href="https://github.com/imipark/"  target="_blank" rel="noopener">
        ╰──GitHub
      </a>
  </li>
  
  <li>
    <a href="/posts/"  target="_blank" rel="noopener">
        ╰──Blog
      </a>
  </li>
  
  <li>
    <a href="https://iparkirk.github.io"  target="_blank" rel="noopener">
        ╰──Old Web
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Data Preparation in RLHF -- Ch6 (Preference Data) vs Ch9 (SFT Data)</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#data-preparation-comparison-ch6-vs-ch9"><strong>Data Preparation Comparison: Ch6 vs Ch9</strong></a></li>
            <li><a href="#key-insight-the-data-types-are-fundamentally-different"><strong>Key Insight: The DATA TYPES are FUNDAMENTALLY DIFFERENT</strong></a></li>
            <li><a href="#the-complete-rlhf-pipeline"><strong>The Complete RLHF Pipeline</strong></a></li>
            <li><a href="#summary"><strong>Summary</strong></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><p align="center">
<img src="/images/AIR_logo.png" alt="AI Reasoning Logo" width="200"/>
<strong style="font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;">
Data Prep in RLHF - Ch6 (Preference Data) vs Ch9 (SFT Data)
</strong>
</p>
<hr>
<h3 id="data-preparation-comparison-ch6-vs-ch9">
  <strong>Data Preparation Comparison: Ch6 vs Ch9</strong>
  <a class="anchor" href="#data-preparation-comparison-ch6-vs-ch9">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Aspect</strong></th>
          <th><strong>Ch6: Preference Data</strong></th>
          <th><strong>Ch9: SFT/IFT Data</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Data Structure</strong></td>
          <td><strong>PAIRWISE comparisons</strong>: <code>(prompt, chosen_response, rejected_response)</code></td>
          <td><strong>SINGLE examples</strong>: <code>(prompt, good_response)</code></td>
      </tr>
      <tr>
          <td><strong>Purpose</strong></td>
          <td>Learn to <strong>JUDGE</strong> which response is better</td>
          <td>Learn to <strong>GENERATE</strong> good responses</td>
      </tr>
      <tr>
          <td><strong>Data Format Example</strong></td>
          <td><code>{&quot;prompt&quot;: &quot;What is 2+2?&quot;,</code><br><code>&quot;chosen&quot;: &quot;The answer is 4&quot;,</code><br><code>&quot;rejected&quot;: &quot;5&quot;}</code></td>
          <td><code>{&quot;prompt&quot;: &quot;What is 2+2?&quot;,</code><br><code>&quot;response&quot;: &quot;The answer is 4&quot;}</code></td>
      </tr>
      <tr>
          <td><strong>Collection Method</strong></td>
          <td>- Side-by-side comparison UI<br>- Likert scales (5-point, 8-point)<br>- Thumbs up/down<br>- ChatBotArena</td>
          <td>- Human-written high-quality examples<br>- Curated Q&amp;A pairs<br>- Single demonstrations</td>
      </tr>
      <tr>
          <td><strong>Data Source</strong></td>
          <td><strong>Human labelers</strong> comparing responses<br>OR<br><strong>Structured/Synthetic</strong>:<br>- Correct vs incorrect (math)<br>- With constraint vs without</td>
          <td><strong>Human-written</strong> completions<br>OR<br><strong>Curated</strong> high-quality examples<br>OR<br><strong>Synthetic</strong> (from stronger models)</td>
      </tr>
      <tr>
          <td><strong>Signal Type</strong></td>
          <td><strong>Comparative/Relative</strong>: Which is better?</td>
          <td><strong>Absolute</strong>: This is a good response</td>
      </tr>
      <tr>
          <td><strong>Typical Dataset Size</strong></td>
          <td>~100K preference pairs (InstructGPT)</td>
          <td>~10K-1M examples (InstructGPT: 10K, modern: ~1M)</td>
      </tr>
      <tr>
          <td><strong>Multi-turn Handling</strong></td>
          <td>- Preference on <strong>final turn</strong> only<br>- Continue with &ldquo;chosen&rdquo; answer<br>- Mask previous turns from loss</td>
          <td>- Each turn = separate training example<br>- Unroll N-turn → N examples<br>- Mask prompts/previous turns</td>
      </tr>
      <tr>
          <td><strong>Used to Train</strong></td>
          <td><strong>Ch7: Reward Model</strong></td>
          <td><strong>Ch9: SFT Model</strong> (initial policy)</td>
      </tr>
      <tr>
          <td><strong>What Gets Trained</strong></td>
          <td><code>L = -log(σ(r(chosen) - r(rejected)))</code></td>
          <td><code>L = -log P(response|prompt)</code></td>
      </tr>
      <tr>
          <td><strong>Next Stage Usage</strong></td>
          <td><strong>Ch11: RL training</strong><br>- Same/similar prompts can be reused<br>- RM provides scores</td>
          <td><strong>Ch7: RM base model</strong><br><strong>Ch11: RL starting policy</strong><br>- Policy generates responses</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="key-insight-the-data-types-are-fundamentally-different">
  <strong>Key Insight: The DATA TYPES are FUNDAMENTALLY DIFFERENT</strong>
  <a class="anchor" href="#key-insight-the-data-types-are-fundamentally-different">#</a>
</h3>
<ul>
<li>Ch9 SFT Data Says: &ldquo;Here&rsquo;s a good response. Learn to generate this.&rdquo;</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#8839ef">&#34;prompt&#34;</span>: <span style="color:#40a02b">&#34;Write a poem about goldfish&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#8839ef">&#34;response&#34;</span>: <span style="color:#40a02b">&#34;Golden swimmer, circling slow...&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><ul>
<li>Ch6 Preference Data Says: &ldquo;Between these two responses, A is better than B. Learn to prefer A.&rdquo;</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#8839ef">&#34;prompt&#34;</span>: <span style="color:#40a02b">&#34;Write a poem about goldfish&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#8839ef">&#34;chosen&#34;</span>: <span style="color:#40a02b">&#34;Golden swimmer, circling slow... (follows constraint)&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#8839ef">&#34;rejected&#34;</span>: <span style="color:#40a02b">&#34;In circles bright, the goldfish glides... (violates constraint)&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h3 id="the-complete-rlhf-pipeline">
  <strong>The Complete RLHF Pipeline</strong>
  <a class="anchor" href="#the-complete-rlhf-pipeline">#</a>
</h3>
<pre style="line-height: 1.0;">
┌─────────────────────┐
│   Pretrained Model  │
└──────────┬──────────┘
           │
           ├──────────────────────────────┐
           ↓                              ↓
    ┌─────────────┐              ┌───────────────┐
    │  Ch9: SFT   │              │  Ch6: Collect │
    │             │              │  Preference   │
    │ Data:       │              │  Data         │
    │ (prompt,    │              │               │
    │  response)  │              │ Data:         │
    │             │              │ (prompt,      │
    │ Single good │              │  chosen,      │
    │ examples    │              │  rejected)    │
    └──────┬──────┘              │               │
           │                     │ Comparisons   │
           │                     └───────┬───────┘
           │                             │
           ↓                             ↓
    ┌─────────────┐              ┌───────────────┐
    │  SFT Model  │──base model─→│   Ch7: RM     │
    │  (Policy    │              │   Training    │
    │  starting   │              │               │
    │  point)     │              │ Uses Ch6 data │
    └──────┬──────┘              └───────┬───────┘
           │                             │
           │                             │
           └──────────┬──────────────────┘
                      ↓
              ┌───────────────┐
              │  Ch11: RL     │
              │  Optimization │
              │               │
              │ Policy: Ch9   │
              │ Scorer: Ch7   │
              │ Prompts: Ch6  │
              │ (or similar)  │
              └───────────────┘
</pre>
<hr>
<h3 id="summary">
  <strong>Summary</strong>
  <a class="anchor" href="#summary">#</a>
</h3>
<p>The fundamental difference between Ch6 and Ch9 data lies in their <strong>learning objectives</strong>:</p>
<ul>
<li><strong>Ch6 (Preference Data)</strong>: Teaches models to <strong>discriminate</strong> between better and worse responses through pairwise comparisons, ultimately training a Reward Model</li>
<li><strong>Ch9 (SFT Data)</strong>: Teaches models to <strong>generate</strong> appropriate responses through demonstration, creating the initial policy for RL optimization</li>
</ul>
<p>Both are essential but serve distinct roles in the RLHF pipeline, with Ch9 establishing generation capabilities and Ch6 enabling quality assessment.</p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#data-preparation-comparison-ch6-vs-ch9"><strong>Data Preparation Comparison: Ch6 vs Ch9</strong></a></li>
            <li><a href="#key-insight-the-data-types-are-fundamentally-different"><strong>Key Insight: The DATA TYPES are FUNDAMENTALLY DIFFERENT</strong></a></li>
            <li><a href="#the-complete-rlhf-pipeline"><strong>The Complete RLHF Pipeline</strong></a></li>
            <li><a href="#summary"><strong>Summary</strong></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












