<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="


ü§ñü¶ø Understanding PPO: From Language Generation to Robot Control ‚Äî Code, Concepts, and Comparisons


Proximal Policy Optimization (PPO) in both large language models (LLMs, e.g., GPT-style) and classical control environments (e.g., Walker2D), focusing on the structure of the PPO update and how actions are selected during inference.


  1. üßæ PPO step() Call ‚Äî Argument-by-Argument Breakdown
  #

ppo_trainer.step(
    queries=[input_ids[0]],       # Prompt (tokenized) ‚Äî represents the current state
    responses=[response_ids[0]],  # Generated tokens ‚Äî represents the action taken
    rewards=[reward]              # Scalar from reward model ‚Äî score for that action
)

  Mapping to Classic RL (Walker2D)
  #


  
      
          PPO Argument
          ü§ñ LLM (RLHF)
          ü¶ø Walker2D (Classic RL)
      
  
  
      
          queries = [input_ids[0]]
          Prompt as input (discrete tokenized state)
          Robot&rsquo;s continuous state (joint angles, velocities)
      
      
          responses = [response_ids[0]]
          Generated tokens (sequence of actions)
          Applied joint torques (vector of real numbers)
      
      
          rewards = [reward]
          Reward model output (alignment score)
          Environment reward (e.g., distance walked)
      
  



  2. üéØ Action Selection in PPO
  #

How does the agent choose its next action, given a state/prompt?">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ppo_comparison/">
  <meta property="og:site_name" content="AI Reasoning">
  <meta property="og:title" content="PPO in LLMs vs PPO in Walker2D">
  <meta property="og:description" content="ü§ñü¶ø Understanding PPO: From Language Generation to Robot Control ‚Äî Code, Concepts, and Comparisons Proximal Policy Optimization (PPO) in both large language models (LLMs, e.g., GPT-style) and classical control environments (e.g., Walker2D), focusing on the structure of the PPO update and how actions are selected during inference.
1. üßæ PPO step() Call ‚Äî Argument-by-Argument Breakdown # ppo_trainer.step( queries=[input_ids[0]], # Prompt (tokenized) ‚Äî represents the current state responses=[response_ids[0]], # Generated tokens ‚Äî represents the action taken rewards=[reward] # Scalar from reward model ‚Äî score for that action ) Mapping to Classic RL (Walker2D) # PPO Argument ü§ñ LLM (RLHF) ü¶ø Walker2D (Classic RL) queries = [input_ids[0]] Prompt as input (discrete tokenized state) Robot‚Äôs continuous state (joint angles, velocities) responses = [response_ids[0]] Generated tokens (sequence of actions) Applied joint torques (vector of real numbers) rewards = [reward] Reward model output (alignment score) Environment reward (e.g., distance walked) 2. üéØ Action Selection in PPO # How does the agent choose its next action, given a state/prompt?">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="ai-workflows">
<title>PPO in LLMs vs PPO in Walker2D | AI Reasoning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ppo_comparison/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.c4e10013ea577c4e1908c5eaa1f1303fe07b484ac90714514291bb2f12aaae31.js" integrity="sha256-xOEAE&#43;pXfE4ZCMXqofEwP&#43;B7SErJBxRRQpG7LxKqrjE=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>AI Reasoning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ai-workflows/" class="">AI Reasoning Stack</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7f3f4cf59430750c2ad109248c8c879b" class="toggle"  />
    <label for="section-7f3f4cf59430750c2ad109248c8c879b" class="flex justify-between">
      <a href="/ai-workflows/data/" class="">Data</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fc566bddf8394fb4d0c5cff688d2febc" class="toggle"  />
    <label for="section-fc566bddf8394fb4d0c5cff688d2febc" class="flex justify-between">
      <a href="/ai-workflows/data/data-centric-ai/" class="">Data-Centric AI (DCAI)</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cff08f084db31c8732ef81d1fe1c4130" class="toggle"  />
    <label for="section-cff08f084db31c8732ef81d1fe1c4130" class="flex justify-between">
      <a href="/ai-workflows/genai/" class="">GenAI</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-63b806a012c3062adb6281022ca8468f" class="toggle"  />
    <label for="section-63b806a012c3062adb6281022ca8468f" class="flex justify-between">
      <a href="/ai-workflows/genai/5-day-genai-google-2025/" class="">5-Day GenAI with Google 2005</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day1_foundational_llm_text_generation/" class="">Day 1 - Foundational LLMs &amp; Text Generation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day1_prompt_engineering/" class="">Day 1 ‚Äì Prompt Engineering</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day2_embeddings_vectordb/" class="">Day 2 ‚Äì Embeddings &amp; Vector Databases</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day3_generative_agents/" class="">Day 3 ‚Äì Generative Agents</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day4_domainspecific_llms/" class="">Day 4 ‚Äì Domain-Specific LLMs</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/5-day-genai-google-2025/day5_mlops/" class="">Day 5 ‚Äì MLOps for Generative AI</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai/multimodel_llms/" class="">Multimodal LLMs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-17ba62e37c896ad50105fedeb71549dd" class="toggle"  />
    <label for="section-17ba62e37c896ad50105fedeb71549dd" class="flex justify-between">
      <a href="/ai-workflows/reasoning/" class="">Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd94e161670d28ecff992edf840d76e8" class="toggle"  />
    <label for="section-cd94e161670d28ecff992edf840d76e8" class="flex justify-between">
      <a href="/ai-workflows/reasoning/causality/" class="">Causality</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-ai/" class="">Causal AI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-inference/" class="">Causal Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7e468aa05ceb7c844f07a2e754606b76" class="toggle"  />
    <label for="section-7e468aa05ceb7c844f07a2e754606b76" class="flex justify-between">
      <a href="/ai-workflows/reasoning/graph-reasoning/" class="">Graph Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/graphrag/" class="">GraphRAG</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/knowledge-graphs/" class="">Knowledge Graphs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="toggle" checked />
    <label for="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="flex justify-between">
      <a href="/ai-workflows/rlhf/" class="">RLHF</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b1afcafdefac57f3420f64e23d73f05d" class="toggle" checked />
    <label for="section-b1afcafdefac57f3420f64e23d73f05d" class="flex justify-between">
      <a href="/ai-workflows/rlhf/rlhf2006/" class="">RLHF 2006</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/" class="">Instruct Gpt Codes Params</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ca53d32fab0e1a54fdf5627349d86bfc" class="toggle"  />
    <label for="section-ca53d32fab0e1a54fdf5627349d86bfc" class="flex justify-between">
      <a href="/ai-workflows/eval/" class="">Eval</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/healthcare/" class="">Healthcare</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="toggle"  />
    <label for="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/" class="">Domain</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-45ba5974905f86df95925365835eadbb" class="toggle"  />
    <label for="section-45ba5974905f86df95925365835eadbb" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/ai-in-healthcare/" class="">AI in Healthcare</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/" class="">C2 Clinical Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/" class="">C3 ML Healthcare</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/" class="">C4 AI Evaluations</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c5_capstone/" class="">C5 Capstone Projects</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-9722ba71bf098ab02c3220d6e8d9056f" class="toggle"  />
    <label for="section-9722ba71bf098ab02c3220d6e8d9056f" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/hands-on-healthcare-data/" class="">Hands-On Healthcare Data</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/data/" class="">Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/clinical_ai/" class="">AI Applications</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ipark/" class="">Inhee Park, PhD - Resume</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>










  
<ul>
  
  <li>
    <a href="https://www.linkedin.com/in/inheepark/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄLinkedIn
      </a>
  </li>
  
  <li>
    <a href="https://github.com/imipark/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄGitHub
      </a>
  </li>
  
  <li>
    <a href="/posts/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄBlog
      </a>
  </li>
  
  <li>
    <a href="https://iparkirk.github.io"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄOld Web
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>PPO in LLMs vs PPO in Walker2D</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1--ppo-step-call--argument-by-argument-breakdown">1. üßæ PPO <code>step()</code> Call ‚Äî Argument-by-Argument Breakdown</a>
          <ul>
            <li><a href="#mapping-to-classic-rl-walker2d">Mapping to Classic RL (Walker2D)</a></li>
          </ul>
        </li>
        <li><a href="#2--action-selection-in-ppo">2. üéØ Action Selection in PPO</a>
          <ul>
            <li><a href="#-llms-text-generation">ü§ñ LLMs (Text Generation)</a></li>
            <li><a href="#-walker2d-physical-control">ü¶ø Walker2D (Physical Control)</a></li>
            <li><a href="#-comparison-of-action-logic">üîÅ Comparison of Action Logic</a></li>
          </ul>
        </li>
        <li><a href="#-ppo-mapping-in-llms-rlhf-vs-classical-rl">üîÅ PPO Mapping in LLMs (RLHF) vs Classical RL</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><p align="center">
<img src="/images/AIR_logo.png" alt="AI Reasoning Logo" width="200"/>
<strong style="font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;">
ü§ñü¶ø Understanding PPO: From Language Generation to Robot Control ‚Äî Code, Concepts, and Comparisons
</strong>
</p>
<p><strong>Proximal Policy Optimization (PPO)</strong> in both large language models (LLMs, e.g., GPT-style) and classical control environments (e.g., Walker2D), focusing on the structure of the PPO update and how actions are selected during inference.</p>
<hr>
<h2 id="1--ppo-step-call--argument-by-argument-breakdown">
  1. üßæ PPO <code>step()</code> Call ‚Äî Argument-by-Argument Breakdown
  <a class="anchor" href="#1--ppo-step-call--argument-by-argument-breakdown">#</a>
</h2>
<div class="highlight"><pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ppo_trainer<span style="color:#04a5e5;font-weight:bold">.</span>step(
</span></span><span style="display:flex;"><span>    queries<span style="color:#04a5e5;font-weight:bold">=</span>[input_ids[<span style="color:#fe640b">0</span>]],       <span style="color:#9ca0b0;font-style:italic"># Prompt (tokenized) ‚Äî represents the current state</span>
</span></span><span style="display:flex;"><span>    responses<span style="color:#04a5e5;font-weight:bold">=</span>[response_ids[<span style="color:#fe640b">0</span>]],  <span style="color:#9ca0b0;font-style:italic"># Generated tokens ‚Äî represents the action taken</span>
</span></span><span style="display:flex;"><span>    rewards<span style="color:#04a5e5;font-weight:bold">=</span>[reward]              <span style="color:#9ca0b0;font-style:italic"># Scalar from reward model ‚Äî score for that action</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="mapping-to-classic-rl-walker2d">
  Mapping to Classic RL (Walker2D)
  <a class="anchor" href="#mapping-to-classic-rl-walker2d">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>PPO Argument</th>
          <th>ü§ñ LLM (RLHF)</th>
          <th>ü¶ø Walker2D (Classic RL)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>queries = [input_ids[0]]</code></td>
          <td>Prompt as input (discrete tokenized state)</td>
          <td>Robot&rsquo;s continuous state (joint angles, velocities)</td>
      </tr>
      <tr>
          <td><code>responses = [response_ids[0]]</code></td>
          <td>Generated tokens (sequence of actions)</td>
          <td>Applied joint torques (vector of real numbers)</td>
      </tr>
      <tr>
          <td><code>rewards = [reward]</code></td>
          <td>Reward model output (alignment score)</td>
          <td>Environment reward (e.g., distance walked)</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="2--action-selection-in-ppo">
  2. üéØ Action Selection in PPO
  <a class="anchor" href="#2--action-selection-in-ppo">#</a>
</h2>
<p>How does the agent choose its next action, given a state/prompt?</p>
<h3 id="-llms-text-generation">
  ü§ñ LLMs (Text Generation)
  <a class="anchor" href="#-llms-text-generation">#</a>
</h3>
<div class="highlight"><pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># Given a prompt (state)</span>
</span></span><span style="display:flex;"><span>input_ids <span style="color:#04a5e5;font-weight:bold">=</span> tokenizer(<span style="color:#40a02b">&#34;What causes rain?&#34;</span>, return_tensors<span style="color:#04a5e5;font-weight:bold">=</span><span style="color:#40a02b">&#34;pt&#34;</span>)<span style="color:#04a5e5;font-weight:bold">.</span>input_ids
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># Model outputs token logits for next action</span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#04a5e5;font-weight:bold">=</span> model(input_ids<span style="color:#04a5e5;font-weight:bold">=</span>input_ids)
</span></span><span style="display:flex;"><span>logits <span style="color:#04a5e5;font-weight:bold">=</span> outputs<span style="color:#04a5e5;font-weight:bold">.</span>logits[:, <span style="color:#04a5e5;font-weight:bold">-</span><span style="color:#fe640b">1</span>, :]
</span></span><span style="display:flex;"><span>probs <span style="color:#04a5e5;font-weight:bold">=</span> torch<span style="color:#04a5e5;font-weight:bold">.</span>softmax(logits, dim<span style="color:#04a5e5;font-weight:bold">=-</span><span style="color:#fe640b">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># Sample the next token (action) from distribution</span>
</span></span><span style="display:flex;"><span>next_token <span style="color:#04a5e5;font-weight:bold">=</span> torch<span style="color:#04a5e5;font-weight:bold">.</span>multinomial(probs, num_samples<span style="color:#04a5e5;font-weight:bold">=</span><span style="color:#fe640b">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># Repeat to generate full response</span>
</span></span></code></pre></div><h3 id="-walker2d-physical-control">
  ü¶ø Walker2D (Physical Control)
  <a class="anchor" href="#-walker2d-physical-control">#</a>
</h3>
<div class="highlight"><pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># Get current robot state</span>
</span></span><span style="display:flex;"><span>state <span style="color:#04a5e5;font-weight:bold">=</span> get_env_state()  <span style="color:#9ca0b0;font-style:italic"># vector like [Œ∏1, Œ∏2, v1, v2...]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># Policy network outputs action distribution parameters</span>
</span></span><span style="display:flex;"><span>mean, std <span style="color:#04a5e5;font-weight:bold">=</span> policy_net(state)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># Sample a continuous action (e.g., torque values)</span>
</span></span><span style="display:flex;"><span>action <span style="color:#04a5e5;font-weight:bold">=</span> torch<span style="color:#04a5e5;font-weight:bold">.</span>normal(mean, std)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># Apply action to environment</span>
</span></span><span style="display:flex;"><span>next_state, reward, done, info <span style="color:#04a5e5;font-weight:bold">=</span> env<span style="color:#04a5e5;font-weight:bold">.</span>step(action)
</span></span></code></pre></div><h3 id="-comparison-of-action-logic">
  üîÅ Comparison of Action Logic
  <a class="anchor" href="#-comparison-of-action-logic">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>ü§ñ LLM (RLHF)</th>
          <th>ü¶ø Walker2D (Classic RL)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>State</td>
          <td>Prompt text</td>
          <td>Robot‚Äôs physical state</td>
      </tr>
      <tr>
          <td>Action</td>
          <td>Next token (discrete)</td>
          <td>Joint torques (continuous)</td>
      </tr>
      <tr>
          <td>Policy Output</td>
          <td>Token logits (softmaxed)</td>
          <td>Mean &amp; std dev of Gaussian per action dim</td>
      </tr>
      <tr>
          <td>Sampling Method</td>
          <td>Multinomial over vocab</td>
          <td>Sample from Gaussian</td>
      </tr>
      <tr>
          <td>Result</td>
          <td>Extend response with chosen token</td>
          <td>Step to new physical state</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="-ppo-mapping-in-llms-rlhf-vs-classical-rl">
  üîÅ PPO Mapping in LLMs (RLHF) vs Classical RL
  <a class="anchor" href="#-ppo-mapping-in-llms-rlhf-vs-classical-rl">#</a>
</h2>
<table>
  <thead>
      <tr>
          <th>Category</th>
          <th>PPO in LLMs (RLHF)</th>
          <th>ü¶ø PPO in Walker2D (Classic RL)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Agent</strong></td>
          <td>Language Model (e.g., GPT-2, o1)</td>
          <td>Control Policy Network</td>
      </tr>
      <tr>
          <td><strong>Environment</strong></td>
          <td>Static or semi-static prompt context</td>
          <td>Physics-based simulator (e.g., MuJoCo)</td>
      </tr>
      <tr>
          <td><strong>State</strong></td>
          <td>Prompt (or full token context so far)</td>
          <td>Robot‚Äôs current physical state (joint angles, velocity, etc.)</td>
      </tr>
      <tr>
          <td><strong>Action</strong></td>
          <td>Next token in the sequence (discrete, vocabulary-sized)</td>
          <td>Torque values for each joint (continuous, multi-dimensional)</td>
      </tr>
      <tr>
          <td><strong>Trajectory</strong></td>
          <td>Sequence of tokens (prompt + response)</td>
          <td>Sequence of joint states and actions over time</td>
      </tr>
      <tr>
          <td><strong>Reward Signal</strong></td>
          <td>Given after full response (from reward model trained via human preferences)</td>
          <td>Immediate reward at each time step (distance walked, balance maintained, etc.)</td>
      </tr>
      <tr>
          <td><strong>Reward Nature</strong></td>
          <td>Sparse, episodic, scalar (usually one reward per episode)</td>
          <td>Dense, frequent, multi-dimensional (continuous feedback per step)</td>
      </tr>
      <tr>
          <td><strong>Goal</strong></td>
          <td>Generate text aligned with human values/preferences</td>
          <td>Learn movement to walk forward efficiently without falling</td>
      </tr>
      <tr>
          <td><strong>Policy Network</strong></td>
          <td>Transformer LM (large, ~billions of params)</td>
          <td>Feedforward or RNN-based controller (small, e.g., MLP)</td>
      </tr>
      <tr>
          <td><strong>Reference Model</strong></td>
          <td>Frozen copy of base LM (used for KL-penalty regularization)</td>
          <td>Usually none (KL not common in Walker2D PPO)</td>
      </tr>
      <tr>
          <td><strong>Training Stability</strong></td>
          <td>Needs KL penalty to prevent mode collapse / nonsense generations</td>
          <td>PPO alone is usually enough due to continuous feedback</td>
      </tr>
      <tr>
          <td><strong>Evaluation</strong></td>
          <td>Human evals, reward model scores (e.g., helpfulness, safety)</td>
          <td>Distance walked, steps survived, control energy used</td>
      </tr>
      <tr>
          <td></td>
          <td>üó£Ô∏è ‚ÄúSay the right thing, the way a human likes‚Äù</td>
          <td>ü¶ø ‚ÄúMove the right way, so you don‚Äôt fall‚Äù</td>
      </tr>
      <tr>
          <td></td>
          <td>Actions are <strong>words</strong>, optimizing a sequence to <strong>match human preference</strong></td>
          <td>Actions are <strong>forces</strong>, optimizing to <strong>physically walk and stay balanced</strong></td>
      </tr>
      <tr>
          <td><strong>Reference</strong></td>
          <td><a href="https://rlhfbook.com/">https://rlhfbook.com/</a></td>
          <td><a href="https://gymnasium.farama.org/">https://gymnasium.farama.org/</a></td>
      </tr>
  </tbody>
</table>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1--ppo-step-call--argument-by-argument-breakdown">1. üßæ PPO <code>step()</code> Call ‚Äî Argument-by-Argument Breakdown</a>
          <ul>
            <li><a href="#mapping-to-classic-rl-walker2d">Mapping to Classic RL (Walker2D)</a></li>
          </ul>
        </li>
        <li><a href="#2--action-selection-in-ppo">2. üéØ Action Selection in PPO</a>
          <ul>
            <li><a href="#-llms-text-generation">ü§ñ LLMs (Text Generation)</a></li>
            <li><a href="#-walker2d-physical-control">ü¶ø Walker2D (Physical Control)</a></li>
            <li><a href="#-comparison-of-action-logic">üîÅ Comparison of Action Logic</a></li>
          </ul>
        </li>
        <li><a href="#-ppo-mapping-in-llms-rlhf-vs-classical-rl">üîÅ PPO Mapping in LLMs (RLHF) vs Classical RL</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












