<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="


PPO vs DPO in RLHF






  PPO (Traditional RLHF)
  #


  Core Concept
  #


&ldquo;Let me generate new text, score it with the reward model, and update my policy based on those scores.&rdquo;


  Pipeline
  #

Step 1: Train Reward Model (Ch7)
   Preference Data ‚Üí Reward Model
   
Step 2: RL Optimization (Ch11)
   Policy generates text ‚Üí RM scores it 
   ‚Üí PPO updates policy
   (Repeat with new generations)

  Chapter Flow
  #

Ch6 (Preference Data)
    ‚Üì
Ch7 (Reward Model Training)
    ‚Üì
Ch11 (PPO RL Optimization)
    ‚Üì
Final Model

  Example Code
  #

# PPO Training Loop
for batch in prompts:
    # Generate NEW responses
    responses = policy.generate(batch)
    
    # Score with reward model
    rewards = reward_model(responses)
    
    # Compute advantages
    advantages = compute_advantages(
        rewards, values
    )
    
    # Update with PPO
    policy.update_with_ppo(advantages)

  The Math
  #

J(Œ∏) = E[min(ratio * A, 
             clip(ratio, 1-Œµ, 1&#43;Œµ) * A)] 
       - Œ≤ * D_KL(œÄ || œÄ_ref)

Where:
- ratio = œÄ_Œ∏(a|s) / œÄ_old(a|s)
- A = advantage
- clip prevents large updates
- D_KL = distance from reference

  Pros
  #


‚úÖ Online learning: Discover new responses
‚úÖ Best performance: Highest quality
‚úÖ Adaptive: Uses current policy
‚úÖ Proven: ChatGPT, Claude, etc.


  Cons
  #


‚ùå Complex: RM &#43; value &#43; PPO
‚ùå Expensive: Multiple models
‚ùå High memory: RM &#43; policy &#43; ref
‚ùå Hard to tune: Many hyperparameters


  Best For
  #


üéØ Maximum performance
üéØ Large compute budgets
üéØ Frontier models
üéØ Continuous improvement


  Examples
  #


ChatGPT (OpenAI)
Claude (Anthropic)
InstructGPT (OpenAI)
DeepSeek R1




  DPO (Direct Alignment)
  #


  Core Concept
  #


&ldquo;Let me directly learn from the preference data without generating anything new.&rdquo;">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ppo_vs_dpo_comparison/">
  <meta property="og:site_name" content="AI Reasoning">
  <meta property="og:title" content="PPO vs DPO in RLHF">
  <meta property="og:description" content="PPO vs DPO in RLHF PPO (Traditional RLHF) # Core Concept # ‚ÄúLet me generate new text, score it with the reward model, and update my policy based on those scores.‚Äù
Pipeline # Step 1: Train Reward Model (Ch7) Preference Data ‚Üí Reward Model Step 2: RL Optimization (Ch11) Policy generates text ‚Üí RM scores it ‚Üí PPO updates policy (Repeat with new generations) Chapter Flow # Ch6 (Preference Data) ‚Üì Ch7 (Reward Model Training) ‚Üì Ch11 (PPO RL Optimization) ‚Üì Final Model Example Code # # PPO Training Loop for batch in prompts: # Generate NEW responses responses = policy.generate(batch) # Score with reward model rewards = reward_model(responses) # Compute advantages advantages = compute_advantages( rewards, values ) # Update with PPO policy.update_with_ppo(advantages) The Math # J(Œ∏) = E[min(ratio * A, clip(ratio, 1-Œµ, 1&#43;Œµ) * A)] - Œ≤ * D_KL(œÄ || œÄ_ref) Where: - ratio = œÄ_Œ∏(a|s) / œÄ_old(a|s) - A = advantage - clip prevents large updates - D_KL = distance from reference Pros # ‚úÖ Online learning: Discover new responses ‚úÖ Best performance: Highest quality ‚úÖ Adaptive: Uses current policy ‚úÖ Proven: ChatGPT, Claude, etc. Cons # ‚ùå Complex: RM &#43; value &#43; PPO ‚ùå Expensive: Multiple models ‚ùå High memory: RM &#43; policy &#43; ref ‚ùå Hard to tune: Many hyperparameters Best For # üéØ Maximum performance üéØ Large compute budgets üéØ Frontier models üéØ Continuous improvement Examples # ChatGPT (OpenAI) Claude (Anthropic) InstructGPT (OpenAI) DeepSeek R1 DPO (Direct Alignment) # Core Concept # ‚ÄúLet me directly learn from the preference data without generating anything new.‚Äù">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="ai-workflows">
<title>PPO vs DPO in RLHF | AI Reasoning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://imipark.github.io/ai-workflows/rlhf/rlhf2006/ppo_vs_dpo_comparison/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.a0efbe41a7b1c6f20d16b2d51ba7430c4d0d9b69849963c0c682ad9812647b67.js" integrity="sha256-oO&#43;&#43;QaexxvINFrLVG6dDDE0Nm2mEmWPAxoKtmBJke2c=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>AI Reasoning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ai-workflows/" class="">AI Reasoning Stack</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-f8e502c34e18a04e1c3aecf192b0355a" class="toggle"  />
    <label for="section-f8e502c34e18a04e1c3aecf192b0355a" class="flex justify-between">
      <a href="/ai-workflows/data-modeling/" class="">Data Modeling</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-906a2243f4790a09188fae70fbc32dbd" class="toggle"  />
    <label for="section-906a2243f4790a09188fae70fbc32dbd" class="flex justify-between">
      <a href="/ai-workflows/data-modeling/data-centric-ai/" class="">Data-Centric AI (DCAI)</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-5bf1c216f4b747bd7c2f992550ff09a1" class="toggle"  />
    <label for="section-5bf1c216f4b747bd7c2f992550ff09a1" class="flex justify-between">
      <a href="/ai-workflows/genai-systems/" class="">GenAI Systems</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8cbfd420d91de13f61964bf971127312" class="toggle"  />
    <label for="section-8cbfd420d91de13f61964bf971127312" class="flex justify-between">
      <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/" class="">5-Day GenAI with Google 2005</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day1_foundational_llm_text_generation/" class="">Day 1 - Foundational LLMs &amp; Text Generation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day1_prompt_engineering/" class="">Day 1 ‚Äì Prompt Engineering</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day2_embeddings_vectordb/" class="">Day 2 ‚Äì Embeddings &amp; Vector Databases</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day3_generative_agents/" class="">Day 3 ‚Äì Generative Agents</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day4_domainspecific_llms/" class="">Day 4 ‚Äì Domain-Specific LLMs</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day5_mlops/" class="">Day 5 ‚Äì MLOps for Generative AI</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/multimodel_llms/" class="">Multimodal LLMs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-17ba62e37c896ad50105fedeb71549dd" class="toggle"  />
    <label for="section-17ba62e37c896ad50105fedeb71549dd" class="flex justify-between">
      <a href="/ai-workflows/reasoning/" class="">Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd94e161670d28ecff992edf840d76e8" class="toggle"  />
    <label for="section-cd94e161670d28ecff992edf840d76e8" class="flex justify-between">
      <a href="/ai-workflows/reasoning/causality/" class="">Causality</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-ai/" class="">Causal AI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-inference/" class="">Causal Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7e468aa05ceb7c844f07a2e754606b76" class="toggle"  />
    <label for="section-7e468aa05ceb7c844f07a2e754606b76" class="flex justify-between">
      <a href="/ai-workflows/reasoning/graph-reasoning/" class="">Graph Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/graphrag/" class="">GraphRAG</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/knowledge-graphs/" class="">Knowledge Graphs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="toggle" checked />
    <label for="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="flex justify-between">
      <a href="/ai-workflows/rlhf/" class="">RLHF</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b1afcafdefac57f3420f64e23d73f05d" class="toggle" checked />
    <label for="section-b1afcafdefac57f3420f64e23d73f05d" class="flex justify-between">
      <a href="/ai-workflows/rlhf/rlhf2006/" class="">RLHF 2006</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/" class="">Instruct Gpt Codes Params</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ca53d32fab0e1a54fdf5627349d86bfc" class="toggle"  />
    <label for="section-ca53d32fab0e1a54fdf5627349d86bfc" class="flex justify-between">
      <a href="/ai-workflows/eval/" class="">AI Evaluation</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/healthcare/" class="">Healthcare</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="toggle"  />
    <label for="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/" class="">Domain</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-45ba5974905f86df95925365835eadbb" class="toggle"  />
    <label for="section-45ba5974905f86df95925365835eadbb" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/ai-in-healthcare/" class="">AI in Healthcare</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/" class="">C2 Clinical Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/" class="">C3 ML Healthcare</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/" class="">C4 AI Evaluations</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c5_capstone/" class="">C5 Capstone Projects</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-9722ba71bf098ab02c3220d6e8d9056f" class="toggle"  />
    <label for="section-9722ba71bf098ab02c3220d6e8d9056f" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/hands-on-healthcare-data/" class="">Hands-On Healthcare Data</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/data/" class="">Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/clinical_ai/" class="">AI Applications</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ipark/" class="">Inhee Park, PhD - Resume</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>










  
<ul>
  
  <li>
    <a href="https://www.linkedin.com/in/inheepark/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄLinkedIn
      </a>
  </li>
  
  <li>
    <a href="https://github.com/imipark/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄGitHub
      </a>
  </li>
  
  <li>
    <a href="/posts/"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄBlog
      </a>
  </li>
  
  <li>
    <a href="https://iparkirk.github.io"  target="_blank" rel="noopener">
        ‚ï∞‚îÄ‚îÄOld Web
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>PPO vs DPO in RLHF</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#ppo-traditional-rlhf">PPO (Traditional RLHF)</a></li>
            <li><a href="#dpo-direct-alignment">DPO (Direct Alignment)</a></li>
            <li><a href="#heading"></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><p align="center">
<img src="/images/AIR_logo.png" alt="AI Reasoning Logo" width="200"/>
<strong style="font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;">
PPO vs DPO in RLHF
</strong>
</p>
<hr>
<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
<div>
<h3 id="ppo-traditional-rlhf">
  PPO (Traditional RLHF)
  <a class="anchor" href="#ppo-traditional-rlhf">#</a>
</h3>
<h4 id="core-concept">
  Core Concept
  <a class="anchor" href="#core-concept">#</a>
</h4>
<blockquote>
<p>&ldquo;Let me <strong>generate new text</strong>, score it with the reward model, and update my policy based on those scores.&rdquo;</p>
</blockquote>
<h4 id="pipeline">
  Pipeline
  <a class="anchor" href="#pipeline">#</a>
</h4>
<pre tabindex="0"><code>Step 1: Train Reward Model (Ch7)
   Preference Data ‚Üí Reward Model
   
Step 2: RL Optimization (Ch11)
   Policy generates text ‚Üí RM scores it 
   ‚Üí PPO updates policy
   (Repeat with new generations)
</code></pre><h4 id="chapter-flow">
  Chapter Flow
  <a class="anchor" href="#chapter-flow">#</a>
</h4>
<pre tabindex="0"><code>Ch6 (Preference Data)
    ‚Üì
Ch7 (Reward Model Training)
    ‚Üì
Ch11 (PPO RL Optimization)
    ‚Üì
Final Model
</code></pre><h4 id="example-code">
  Example Code
  <a class="anchor" href="#example-code">#</a>
</h4>
<div class="highlight"><pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># PPO Training Loop</span>
</span></span><span style="display:flex;"><span><span style="color:#8839ef">for</span> batch <span style="color:#04a5e5;font-weight:bold">in</span> prompts:
</span></span><span style="display:flex;"><span>    <span style="color:#9ca0b0;font-style:italic"># Generate NEW responses</span>
</span></span><span style="display:flex;"><span>    responses <span style="color:#04a5e5;font-weight:bold">=</span> policy<span style="color:#04a5e5;font-weight:bold">.</span>generate(batch)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#9ca0b0;font-style:italic"># Score with reward model</span>
</span></span><span style="display:flex;"><span>    rewards <span style="color:#04a5e5;font-weight:bold">=</span> reward_model(responses)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#9ca0b0;font-style:italic"># Compute advantages</span>
</span></span><span style="display:flex;"><span>    advantages <span style="color:#04a5e5;font-weight:bold">=</span> compute_advantages(
</span></span><span style="display:flex;"><span>        rewards, values
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#9ca0b0;font-style:italic"># Update with PPO</span>
</span></span><span style="display:flex;"><span>    policy<span style="color:#04a5e5;font-weight:bold">.</span>update_with_ppo(advantages)
</span></span></code></pre></div><h4 id="the-math">
  The Math
  <a class="anchor" href="#the-math">#</a>
</h4>
<pre tabindex="0"><code>J(Œ∏) = E[min(ratio * A, 
             clip(ratio, 1-Œµ, 1+Œµ) * A)] 
       - Œ≤ * D_KL(œÄ || œÄ_ref)

Where:
- ratio = œÄ_Œ∏(a|s) / œÄ_old(a|s)
- A = advantage
- clip prevents large updates
- D_KL = distance from reference
</code></pre><h4 id="pros">
  Pros
  <a class="anchor" href="#pros">#</a>
</h4>
<ul>
<li>‚úÖ <strong>Online learning</strong>: Discover new responses</li>
<li>‚úÖ <strong>Best performance</strong>: Highest quality</li>
<li>‚úÖ <strong>Adaptive</strong>: Uses current policy</li>
<li>‚úÖ <strong>Proven</strong>: ChatGPT, Claude, etc.</li>
</ul>
<h4 id="cons">
  Cons
  <a class="anchor" href="#cons">#</a>
</h4>
<ul>
<li>‚ùå <strong>Complex</strong>: RM + value + PPO</li>
<li>‚ùå <strong>Expensive</strong>: Multiple models</li>
<li>‚ùå <strong>High memory</strong>: RM + policy + ref</li>
<li>‚ùå <strong>Hard to tune</strong>: Many hyperparameters</li>
</ul>
<h4 id="best-for">
  Best For
  <a class="anchor" href="#best-for">#</a>
</h4>
<ul>
<li>üéØ Maximum performance</li>
<li>üéØ Large compute budgets</li>
<li>üéØ Frontier models</li>
<li>üéØ Continuous improvement</li>
</ul>
<h4 id="examples">
  Examples
  <a class="anchor" href="#examples">#</a>
</h4>
<ul>
<li>ChatGPT (OpenAI)</li>
<li>Claude (Anthropic)</li>
<li>InstructGPT (OpenAI)</li>
<li>DeepSeek R1</li>
</ul>
</div>
<div>
<h3 id="dpo-direct-alignment">
  DPO (Direct Alignment)
  <a class="anchor" href="#dpo-direct-alignment">#</a>
</h3>
<h4 id="core-concept-1">
  Core Concept
  <a class="anchor" href="#core-concept-1">#</a>
</h4>
<blockquote>
<p>&ldquo;Let me <strong>directly learn</strong> from the preference data without generating anything new.&rdquo;</p>
</blockquote>
<h4 id="pipeline-1">
  Pipeline
  <a class="anchor" href="#pipeline-1">#</a>
</h4>
<pre tabindex="0"><code>Single Step: Direct Optimization
   Preference Data ‚Üí DPO Loss 
   ‚Üí Update Policy
   (No RM, no generation, 
    just gradient updates)
</code></pre><h4 id="chapter-flow-1">
  Chapter Flow
  <a class="anchor" href="#chapter-flow-1">#</a>
</h4>
<pre tabindex="0"><code>Ch6 (Preference Data)
    ‚Üì
Ch12 (DPO Direct Optimization)
    ‚Üì
Final Model

(Skip Ch7 and Ch11!)
</code></pre><h4 id="example-code-1">
  Example Code
  <a class="anchor" href="#example-code-1">#</a>
</h4>
<div class="highlight"><pre tabindex="0" style="color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#9ca0b0;font-style:italic"># DPO Training Loop</span>
</span></span><span style="display:flex;"><span><span style="color:#8839ef">for</span> (prompt, chosen, rejected) \
</span></span><span style="display:flex;"><span>    <span style="color:#04a5e5;font-weight:bold">in</span> preference_data:
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#9ca0b0;font-style:italic"># No generation! Just compute loss</span>
</span></span><span style="display:flex;"><span>    loss <span style="color:#04a5e5;font-weight:bold">=</span> <span style="color:#04a5e5;font-weight:bold">-</span>log(œÉ(
</span></span><span style="display:flex;"><span>        Œ≤ <span style="color:#04a5e5;font-weight:bold">*</span> log(P(chosen)<span style="color:#04a5e5;font-weight:bold">/</span>P_ref(chosen)) 
</span></span><span style="display:flex;"><span>        <span style="color:#04a5e5;font-weight:bold">-</span> Œ≤ <span style="color:#04a5e5;font-weight:bold">*</span> log(P(rejected)<span style="color:#04a5e5;font-weight:bold">/</span>P_ref(rejected))
</span></span><span style="display:flex;"><span>    ))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#9ca0b0;font-style:italic"># Simple gradient descent</span>
</span></span><span style="display:flex;"><span>    policy<span style="color:#04a5e5;font-weight:bold">.</span>update(loss)
</span></span></code></pre></div><h4 id="the-math-1">
  The Math
  <a class="anchor" href="#the-math-1">#</a>
</h4>
<pre tabindex="0"><code>L = -E[log œÉ(
      Œ≤ * log(œÄ(y_c)/œÄ_ref(y_c)) 
      - Œ≤ * log(œÄ(y_r)/œÄ_ref(y_r))
    )]

Where:
- œÉ = sigmoid function
- Œ≤ = controls KL penalty
- œÄ_ref = reference (frozen)
</code></pre><p><strong>Secret:</strong> Implicit reward</p>
<pre tabindex="0"><code>r(x,y) = Œ≤ * log(œÄ(y|x) / œÄ_ref(y|x))
</code></pre><h4 id="pros-1">
  Pros
  <a class="anchor" href="#pros-1">#</a>
</h4>
<ul>
<li>‚úÖ <strong>Simple</strong>: One loss function</li>
<li>‚úÖ <strong>Memory efficient</strong>: No separate RM</li>
<li>‚úÖ <strong>Cheaper</strong>: No RM inference</li>
<li>‚úÖ <strong>Easy to implement</strong>: Standard training</li>
<li>‚úÖ <strong>Accessible</strong>: For academic labs</li>
</ul>
<h4 id="cons-1">
  Cons
  <a class="anchor" href="#cons-1">#</a>
</h4>
<ul>
<li>‚ùå <strong>Offline only</strong>: Can&rsquo;t generate new data</li>
<li>‚ùå <strong>Data dependent</strong>: Limited by dataset</li>
<li>‚ùå <strong>Lower ceiling</strong>: May not match PPO</li>
<li>‚ùå <strong>Preference displacement</strong>: Both ‚Üì</li>
</ul>
<h4 id="best-for-1">
  Best For
  <a class="anchor" href="#best-for-1">#</a>
</h4>
<ul>
<li>üéØ Research &amp; experimentation</li>
<li>üéØ Limited compute</li>
<li>üéØ Academic settings</li>
<li>üéØ Quick iterations</li>
<li>üéØ Good fixed dataset</li>
</ul>
<h4 id="examples-1">
  Examples
  <a class="anchor" href="#examples-1">#</a>
</h4>
<ul>
<li>Zephyr-7B (HuggingFace)</li>
<li>Llama 3 Instruct (Meta)</li>
<li>T√ºlu 2 &amp; 3 (AI2)</li>
<li>Many open-source models</li>
</ul>
</div>
</div>
<h3 id="heading">
  
  <a class="anchor" href="#heading">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Aspect</strong></th>
          <th><strong>PPO (Proximal Policy Optimization)</strong></th>
          <th><strong>DPO (Direct Preference Optimization)</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Approach</strong></td>
          <td><strong>Two-stage</strong>: Train RM ‚Üí Use RL</td>
          <td><strong>One-stage</strong>: Direct optimization from preferences</td>
      </tr>
      <tr>
          <td><strong>Reward Model</strong></td>
          <td>‚úÖ <strong>Needs separate RM</strong> (Ch7)</td>
          <td>‚ùå <strong>No separate RM needed</strong> (implicit)</td>
      </tr>
      <tr>
          <td><strong>Training Data</strong></td>
          <td>Generates <strong>new</strong> responses during training</td>
          <td>Uses <strong>fixed</strong> preference dataset</td>
      </tr>
      <tr>
          <td><strong>Algorithm Type</strong></td>
          <td><strong>Online RL</strong> (policy-gradient)</td>
          <td><strong>Offline optimization</strong> (gradient ascent)</td>
      </tr>
      <tr>
          <td><strong>Complexity</strong></td>
          <td><strong>More complex</strong>: RM + value function + PPO</td>
          <td><strong>Simpler</strong>: Just one loss function</td>
      </tr>
      <tr>
          <td><strong>Memory</strong></td>
          <td><strong>High</strong>: Need RM + policy + reference model</td>
          <td><strong>Lower</strong>: Just policy + reference model</td>
      </tr>
      <tr>
          <td><strong>Data Usage</strong></td>
          <td><strong>On-policy</strong>: Generates fresh data each step</td>
          <td><strong>Off-policy</strong>: Uses pre-collected data</td>
      </tr>
      <tr>
          <td><strong>Cost</strong></td>
          <td><strong>More expensive</strong>: Forward passes through RM</td>
          <td><strong>Cheaper</strong>: No RM inference needed</td>
      </tr>
      <tr>
          <td><strong>When Popular</strong></td>
          <td>2017-2023 (ChatGPT, InstructGPT)</td>
          <td>2023-present (Llama 3, Zephyr, T√ºlu)</td>
      </tr>
  </tbody>
</table>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#ppo-traditional-rlhf">PPO (Traditional RLHF)</a></li>
            <li><a href="#dpo-direct-alignment">DPO (Direct Alignment)</a></li>
            <li><a href="#heading"></a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












