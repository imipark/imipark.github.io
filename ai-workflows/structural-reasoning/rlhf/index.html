<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  PPO in LLMs (Reinforcement Learning with Human Feedbak) vs PPO in Walker2D
  #



  🔁 Comparison Table
  #


  
      
          Category
          PPO in LLMs (RLHF)
          🦿 PPO in Walker2D (Classic RL)
      
  
  
      
          Agent
          Language Model (e.g., GPT-2, o1)
          Control Policy Network
      
      
          Environment
          Static or semi-static prompt context
          Physics-based simulator (e.g., MuJoCo)
      
      
          State
          Prompt (or full token context so far)
          Robot’s current physical state (joint angles, velocity, etc.)
      
      
          Action
          Next token in the sequence (discrete, vocabulary-sized)
          Torque values for each joint (continuous, multi-dimensional)
      
      
          Trajectory
          Sequence of tokens (prompt &#43; response)
          Sequence of joint states and actions over time
      
      
          Reward Signal
          Given after full response (from reward model trained via human preferences)
          Immediate reward at each time step (distance walked, balance maintained, etc.)
      
      
          Reward Nature
          Sparse, episodic, scalar (usually one reward per episode)
          Dense, frequent, multi-dimensional (continuous feedback per step)
      
      
          Goal
          Generate text aligned with human values/preferences
          Learn movement to walk forward efficiently without falling
      
      
          Policy Network
          Transformer LM (large, ~billions of params)
          Feedforward or RNN-based controller (small, e.g., MLP)
      
      
          Reference Model
          Frozen copy of base LM (used for KL-penalty regularization)
          Usually none (KL not common in Walker2D PPO)
      
      
          Training Stability
          Needs KL penalty to prevent mode collapse / nonsense generations
          PPO alone is usually enough due to continuous feedback
      
      
          Evaluation
          Human evals, reward model scores (e.g., helpfulness, safety)
          Distance walked, steps survived, control energy used
      
  



  🧩 Summary
  #


  
      
          LLM (RLHF with PPO)
          Walker2D (PPO)
      
  
  
      
          🗣️ “Say the right thing, the way a human likes”
          🦿 “Move the right way, so you don’t fall”
      
      
          Actions are words, optimizing a sequence to match human preference
          Actions are forces, optimizing to physically walk and stay balanced
      
      
          Reference: https://rlhfbook.com/
          Reference: https://gymnasium.farama.org/environments/mujoco/walker2d/
      
  
">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/ai-workflows/structural-reasoning/rlhf/">
  <meta property="og:site_name" content="AI in Healthcare">
  <meta property="og:title" content="RLHF">
  <meta property="og:description" content="PPO in LLMs (Reinforcement Learning with Human Feedbak) vs PPO in Walker2D # 🔁 Comparison Table # Category PPO in LLMs (RLHF) 🦿 PPO in Walker2D (Classic RL) Agent Language Model (e.g., GPT-2, o1) Control Policy Network Environment Static or semi-static prompt context Physics-based simulator (e.g., MuJoCo) State Prompt (or full token context so far) Robot’s current physical state (joint angles, velocity, etc.) Action Next token in the sequence (discrete, vocabulary-sized) Torque values for each joint (continuous, multi-dimensional) Trajectory Sequence of tokens (prompt &#43; response) Sequence of joint states and actions over time Reward Signal Given after full response (from reward model trained via human preferences) Immediate reward at each time step (distance walked, balance maintained, etc.) Reward Nature Sparse, episodic, scalar (usually one reward per episode) Dense, frequent, multi-dimensional (continuous feedback per step) Goal Generate text aligned with human values/preferences Learn movement to walk forward efficiently without falling Policy Network Transformer LM (large, ~billions of params) Feedforward or RNN-based controller (small, e.g., MLP) Reference Model Frozen copy of base LM (used for KL-penalty regularization) Usually none (KL not common in Walker2D PPO) Training Stability Needs KL penalty to prevent mode collapse / nonsense generations PPO alone is usually enough due to continuous feedback Evaluation Human evals, reward model scores (e.g., helpfulness, safety) Distance walked, steps survived, control energy used 🧩 Summary # LLM (RLHF with PPO) Walker2D (PPO) 🗣️ “Say the right thing, the way a human likes” 🦿 “Move the right way, so you don’t fall” Actions are words, optimizing a sequence to match human preference Actions are forces, optimizing to physically walk and stay balanced Reference: https://rlhfbook.com/ Reference: https://gymnasium.farama.org/environments/mujoco/walker2d/">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">
<title>RLHF | AI in Healthcare</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/ai-workflows/structural-reasoning/rlhf/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.8f0eb91659535145a371668b1dff39aa210d963cc04f048926256384145f962d.js" integrity="sha256-jw65FllTUUWjcWaLHf85qiENljzATwSJJiVjhBRfli0=" crossorigin="anonymous"></script>
<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/ai-workflows/structural-reasoning/rlhf/index.xml" title="AI in Healthcare" />
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>AI in Healthcare</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/healthcare-domain/" class="">Healthcare Domain</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-006e92286777b45b0a28d3a2365a3a67" class="toggle"  />
    <label for="section-006e92286777b45b0a28d3a2365a3a67" class="flex justify-between">
      <a href="/healthcare-domain/learning/" class="">Learning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-85db45cdb58d083b8b67335f89ad3916" class="toggle"  />
    <label for="section-85db45cdb58d083b8b67335f89ad3916" class="flex justify-between">
      <a href="/healthcare-domain/learning/ai-in-healthcare/" class="">AI in Healthcare</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/learning/ai-in-healthcare/c2_clinical_data/" class="">C2 Clinical Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/" class="">C3 ML Healthcare</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/learning/ai-in-healthcare/c4_ai_evaluation/" class="">C4 AI Evaluations</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/learning/ai-in-healthcare/c5_capstone/" class="">C5 Capstone Projects</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-317e9b0f08275c48e5b214edfaed8be3" class="toggle"  />
    <label for="section-317e9b0f08275c48e5b214edfaed8be3" class="flex justify-between">
      <a href="/healthcare-domain/learning/causal-inference-rwd/" class="">Causal Inference RWD</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1ec69014a5624ba0393a04a30874fb12" class="toggle"  />
    <label for="section-1ec69014a5624ba0393a04a30874fb12" class="flex justify-between">
      <a href="/healthcare-domain/learning/clinical-data-science/" class="">Clinical Data Science</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-70fa49cb3f8f56f52a5e1b787c860d19" class="toggle"  />
    <label for="section-70fa49cb3f8f56f52a5e1b787c860d19" class="flex justify-between">
      <a href="/healthcare-domain/learning/hands-on-healthcare-data/" class="">Hands-On Healthcare Data</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/learning/hands-on-healthcare-data/ch4_ehr/" class="">Ch4 EHR</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/learning/hands-on-healthcare-data/ch6_graph_ml/" class="">Ch6 ML and Graph Analytics</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-432f1263c64ec7f8147f13ab5b1f0abf" class="toggle"  />
    <label for="section-432f1263c64ec7f8147f13ab5b1f0abf" class="flex justify-between">
      <a href="/healthcare-domain/data/" class="">Healthcare Data</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/data/healthcare_layers/" class="">Healthcare Data Layers</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/data/healthcare_sources/" class="">Healthcare Data Sources</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/terminology/" class="">Healthcare Glossary</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare-domain/tools/" class="">Infromatics Tools</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ai-workflows/" class="">AI Workflows</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-34244c046af3dd0acc0bbb74c663a8d3" class="toggle"  />
    <label for="section-34244c046af3dd0acc0bbb74c663a8d3" class="flex justify-between">
      <a href="/ai-workflows/nlp-llm-genai/" class="">NLP→LLMs→GenAI→Agents</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-84a2fcdd2f4a95b774882e612724819f" class="toggle"  />
    <label for="section-84a2fcdd2f4a95b774882e612724819f" class="flex justify-between">
      <a href="/ai-workflows/nlp-llm-genai/5-day-genai-google/" class="">5-Day GenAI with Google</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/nlp-llm-genai/5-day-genai-google/day1_foundational_llm_text_generation/" class="">Day 1 - Foundational LLMs &amp; Text Generation – CoT Summary</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/nlp-llm-genai/5-day-genai-google/day1_prompt_engineering/" class="">Day 1 – Prompt Engineering – CoT Summary</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/nlp-llm-genai/5-day-genai-google/day2_embeddings_vectordb/" class="">Day 2 – Embeddings &amp; Vector Databases – CoT Summary</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/nlp-llm-genai/5-day-genai-google/day3_generative_agents/" class="">Day 3 – Generative Agents – CoT Summary</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/nlp-llm-genai/5-day-genai-google/day4_domainspecific_llms/" class="">Day 4 – Domain-Specific LLMs – CoT Summary</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/nlp-llm-genai/5-day-genai-google/day5_mlops/" class="">Day 5 – MLOps for Generative AI – CoT Summary</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-a994622a1338c85ecf70f69adcc39b34" class="toggle"  />
    <label for="section-a994622a1338c85ecf70f69adcc39b34" class="flex justify-between">
      <a href="/ai-workflows/nlp-llm-genai/ai_agents/" class="">AI Agents</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/nlp-llm-genai/prompt_engineering/" class="">Prompt Engineering</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-c3b518d59c6ca41d32658ed3b7cde75b" class="toggle" checked />
    <label for="section-c3b518d59c6ca41d32658ed3b7cde75b" class="flex justify-between">
      <a href="/ai-workflows/structural-reasoning/" class="">Structural Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-2421eaaa685219c6f46672d27e449bd9" class="toggle"  />
    <label for="section-2421eaaa685219c6f46672d27e449bd9" class="flex justify-between">
      <a href="/ai-workflows/structural-reasoning/causality/" class="">Causality</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/structural-reasoning/causality/causal-ai/" class="">Causal AI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/structural-reasoning/causality/causal-inference/" class="">Causal Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-d6d02c58ad32163fbbfe0ca920604379" class="toggle"  />
    <label for="section-d6d02c58ad32163fbbfe0ca920604379" class="flex justify-between">
      <a role="button" class="">Graphs</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/structural-reasoning/graphs/graphrag/" class="">GraphRAG</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/structural-reasoning/graphs/knowledge-graphs/" class="">Knowledge Graphs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-847a080b7898cae70e5723bf792837e6" class="toggle" checked />
    <label for="section-847a080b7898cae70e5723bf792837e6" class="flex justify-between">
      <a href="/ai-workflows/structural-reasoning/rlhf/" class="active">RLHF</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-f57a08bac84df3f46191c3cbf417807e" class="toggle"  />
    <label for="section-f57a08bac84df3f46191c3cbf417807e" class="flex justify-between">
      <a href="/ai-workflows/mlops/" class="">MLOps</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/mlops/ai_cloud_comparision/" class="">Ai Cloud Comparision</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/mlops/clinical_nlp_genai/" class="">Clinical Nlp Gen Ai</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <input type="checkbox" id="section-9320ef7c915cdbbdf424ec3265b5d32b" class="toggle"  />
    <label for="section-9320ef7c915cdbbdf424ec3265b5d32b" class="flex justify-between">
      <a href="/projects/" class="">Projects</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ipark/" class="">Inhee Park, PhD - Resume</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>










  
<ul>
  
  <li>
    <a href="https://www.linkedin.com/in/inheepark/"  target="_blank" rel="noopener">
        ╰──LinkedIn
      </a>
  </li>
  
  <li>
    <a href="https://github.com/imipark/"  target="_blank" rel="noopener">
        ╰──GitHub
      </a>
  </li>
  
  <li>
    <a href="/posts/"  target="_blank" rel="noopener">
        ╰──Blog
      </a>
  </li>
  
  <li>
    <a href="https://iparkirk.github.io"  target="_blank" rel="noopener">
        ╰──Old Web
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>RLHF</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#ppo-in-llms-reinforcement-learning-with-human-feedbak-vs-ppo-in-walker2d">PPO in LLMs (Reinforcement Learning with Human Feedbak) vs PPO in Walker2D</a></li>
        <li><a href="#-comparison-table">🔁 Comparison Table</a></li>
        <li><a href="#-summary">🧩 Summary</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><h2 id="ppo-in-llms-reinforcement-learning-with-human-feedbak-vs-ppo-in-walker2d">
  PPO in LLMs (Reinforcement Learning with Human Feedbak) vs PPO in Walker2D
  <a class="anchor" href="#ppo-in-llms-reinforcement-learning-with-human-feedbak-vs-ppo-in-walker2d">#</a>
</h2>
<hr>
<h2 id="-comparison-table">
  🔁 Comparison Table
  <a class="anchor" href="#-comparison-table">#</a>
</h2>
<table>
  <thead>
      <tr>
          <th>Category</th>
          <th>PPO in LLMs (RLHF)</th>
          <th>🦿 PPO in Walker2D (Classic RL)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Agent</strong></td>
          <td>Language Model (e.g., GPT-2, o1)</td>
          <td>Control Policy Network</td>
      </tr>
      <tr>
          <td><strong>Environment</strong></td>
          <td>Static or semi-static prompt context</td>
          <td>Physics-based simulator (e.g., MuJoCo)</td>
      </tr>
      <tr>
          <td><strong>State</strong></td>
          <td>Prompt (or full token context so far)</td>
          <td>Robot’s current physical state (joint angles, velocity, etc.)</td>
      </tr>
      <tr>
          <td><strong>Action</strong></td>
          <td>Next token in the sequence (discrete, vocabulary-sized)</td>
          <td>Torque values for each joint (continuous, multi-dimensional)</td>
      </tr>
      <tr>
          <td><strong>Trajectory</strong></td>
          <td>Sequence of tokens (prompt + response)</td>
          <td>Sequence of joint states and actions over time</td>
      </tr>
      <tr>
          <td><strong>Reward Signal</strong></td>
          <td>Given after full response (from reward model trained via human preferences)</td>
          <td>Immediate reward at each time step (distance walked, balance maintained, etc.)</td>
      </tr>
      <tr>
          <td><strong>Reward Nature</strong></td>
          <td>Sparse, episodic, scalar (usually one reward per episode)</td>
          <td>Dense, frequent, multi-dimensional (continuous feedback per step)</td>
      </tr>
      <tr>
          <td><strong>Goal</strong></td>
          <td>Generate text aligned with human values/preferences</td>
          <td>Learn movement to walk forward efficiently without falling</td>
      </tr>
      <tr>
          <td><strong>Policy Network</strong></td>
          <td>Transformer LM (large, ~billions of params)</td>
          <td>Feedforward or RNN-based controller (small, e.g., MLP)</td>
      </tr>
      <tr>
          <td><strong>Reference Model</strong></td>
          <td>Frozen copy of base LM (used for KL-penalty regularization)</td>
          <td>Usually none (KL not common in Walker2D PPO)</td>
      </tr>
      <tr>
          <td><strong>Training Stability</strong></td>
          <td>Needs KL penalty to prevent mode collapse / nonsense generations</td>
          <td>PPO alone is usually enough due to continuous feedback</td>
      </tr>
      <tr>
          <td><strong>Evaluation</strong></td>
          <td>Human evals, reward model scores (e.g., helpfulness, safety)</td>
          <td>Distance walked, steps survived, control energy used</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="-summary">
  🧩 Summary
  <a class="anchor" href="#-summary">#</a>
</h2>
<table>
  <thead>
      <tr>
          <th>LLM (RLHF with PPO)</th>
          <th>Walker2D (PPO)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>🗣️ “Say the right thing, the way a human likes”</td>
          <td>🦿 “Move the right way, so you don’t fall”</td>
      </tr>
      <tr>
          <td>Actions are <strong>words</strong>, optimizing a sequence to <strong>match human preference</strong></td>
          <td>Actions are <strong>forces</strong>, optimizing to <strong>physically walk and stay balanced</strong></td>
      </tr>
      <tr>
          <td>Reference: <a href="https://rlhfbook.com/">https://rlhfbook.com/</a></td>
          <td>Reference: <a href="https://gymnasium.farama.org/environments/mujoco/walker2d/">https://gymnasium.farama.org/environments/mujoco/walker2d/</a></td>
      </tr>
  </tbody>
</table>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#ppo-in-llms-reinforcement-learning-with-human-feedbak-vs-ppo-in-walker2d">PPO in LLMs (Reinforcement Learning with Human Feedbak) vs PPO in Walker2D</a></li>
        <li><a href="#-comparison-table">🔁 Comparison Table</a></li>
        <li><a href="#-summary">🧩 Summary</a></li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












