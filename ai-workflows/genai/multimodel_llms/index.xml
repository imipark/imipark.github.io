<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multimodal LLMs on AI Reasoning</title>
    <link>https://imipark.github.io/ai-workflows/genai/multimodel_llms/</link>
    <description>Recent content in Multimodal LLMs on AI Reasoning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://imipark.github.io/ai-workflows/genai/multimodel_llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Core Components and Fusion Strategies in Multimodal LLMs</title>
      <link>https://imipark.github.io/ai-workflows/genai/multimodel_llms/components_fusions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/genai/multimodel_llms/components_fusions/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Core Components of a Multimodal LLM&#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Visual Encoder&lt;/strong&gt;&lt;br&gt;&#xA;Converts input images into feature embeddings. Common choices include CLIP, ViT, and EVA.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Modality Adapter (Aligner)&lt;/strong&gt;&lt;br&gt;&#xA;Projects or transforms visual features to be compatible with the language model‚Äôs embedding space (e.g., via MLP or cross-attention).&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Language Model (LLM)&lt;/strong&gt;&lt;br&gt;&#xA;A large pretrained language model (e.g., LLaMA, GPT) that consumes both text and aligned visual inputs to generate or classify responses.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inputs and Data Preparation for Multimodal LLMs</title>
      <link>https://imipark.github.io/ai-workflows/genai/multimodel_llms/data_prep/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/genai/multimodel_llms/data_prep/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Inputs and Data Preparation for Multimodal LLMs&#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;p&gt;Multimodal LLMs are language models that can process and reason over &lt;strong&gt;multiple data types&lt;/strong&gt;, especially:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Text&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Images&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;(Optionally: audio, video, or other modalities)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;They are designed to understand &lt;strong&gt;both visual and linguistic context&lt;/strong&gt;, enabling tasks like visual question answering, image captioning, grounding, and perception-based reasoning.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;---input-format&#34;&gt;&#xA;  üñºÔ∏è + üí¨ Input Format&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#---input-format&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Inputs typically include:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Image(s)&lt;/strong&gt;: RGB images, optionally annotated (e.g., bounding boxes, circles)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Text Prompt&lt;/strong&gt;: Task instruction or question (e.g., &amp;ldquo;Which object is closer?&amp;rdquo;)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Answer Choices&lt;/strong&gt; (optional): For classification-style tasks like BLINK&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;inputs &lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#40a02b&#34;&gt;&amp;#34;images&amp;#34;&lt;/span&gt;: [&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;...&lt;/span&gt;],   &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# preprocessed (resized, normalized) tensors or raw image paths&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#40a02b&#34;&gt;&amp;#34;text&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#40a02b&#34;&gt;&amp;#34;Which point is closer to the camera? (A) A (B) B&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Some APIs accept JSON-style mixed prompts with interleaved text and image tokens.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
