<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RLHF on AI Reasoning</title>
    <link>https://imipark.github.io/ai-workflows/alignment-reasoning/rlhf/</link>
    <description>Recent content in RLHF on AI Reasoning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://imipark.github.io/ai-workflows/alignment-reasoning/rlhf/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PPO in LLMs vs PPO in Walker2D</title>
      <link>https://imipark.github.io/ai-workflows/alignment-reasoning/rlhf/comparison_ppo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/alignment-reasoning/rlhf/comparison_ppo/</guid>
      <description>&lt;h2 id=&#34;-understanding-ppo-from-language-generation-to-robot-control--code-concepts-and-comparisons&#34;&gt;&#xA;  ðŸ¤–ðŸ¦¿ Understanding PPO: From Language Generation to Robot Control â€” Code, Concepts, and Comparisons&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-understanding-ppo-from-language-generation-to-robot-control--code-concepts-and-comparisons&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;Proximal Policy Optimization (PPO)&lt;/strong&gt; in both large language models (LLMs, e.g., GPT-style) and classical control environments (e.g., Walker2D), focusing on the structure of the PPO update and how actions are selected during inference.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;1--ppo-step-call--argument-by-argument-breakdown&#34;&gt;&#xA;  1. ðŸ§¾ PPO &lt;code&gt;step()&lt;/code&gt; Call â€” Argument-by-Argument Breakdown&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1--ppo-step-call--argument-by-argument-breakdown&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#4c4f69;background-color:#eff1f5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ppo_trainer&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;.&lt;/span&gt;step(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    queries&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt;[input_ids[&lt;span style=&#34;color:#fe640b&#34;&gt;0&lt;/span&gt;]],       &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Prompt (tokenized) â€” represents the current state&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    responses&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt;[response_ids[&lt;span style=&#34;color:#fe640b&#34;&gt;0&lt;/span&gt;]],  &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Generated tokens â€” represents the action taken&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    rewards&lt;span style=&#34;color:#04a5e5;font-weight:bold&#34;&gt;=&lt;/span&gt;[reward]              &lt;span style=&#34;color:#9ca0b0;font-style:italic&#34;&gt;# Scalar from reward model â€” score for that action&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;mapping-to-classic-rl-walker2d&#34;&gt;&#xA;  Mapping to Classic RL (Walker2D)&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#mapping-to-classic-rl-walker2d&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;PPO Argument&lt;/th&gt;&#xA;          &lt;th&gt;ðŸ¤– LLM (RLHF)&lt;/th&gt;&#xA;          &lt;th&gt;ðŸ¦¿ Walker2D (Classic RL)&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;code&gt;queries = [input_ids[0]]&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Prompt as input (discrete tokenized state)&lt;/td&gt;&#xA;          &lt;td&gt;Robot&amp;rsquo;s continuous state (joint angles, velocities)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;code&gt;responses = [response_ids[0]]&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Generated tokens (sequence of actions)&lt;/td&gt;&#xA;          &lt;td&gt;Applied joint torques (vector of real numbers)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;&lt;code&gt;rewards = [reward]&lt;/code&gt;&lt;/td&gt;&#xA;          &lt;td&gt;Reward model output (alignment score)&lt;/td&gt;&#xA;          &lt;td&gt;Environment reward (e.g., distance walked)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;2--action-selection-in-ppo&#34;&gt;&#xA;  2. ðŸŽ¯ Action Selection in PPO&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2--action-selection-in-ppo&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;How does the agent choose its next action, given a state/prompt?&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
