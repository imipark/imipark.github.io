<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data-Centric AI (DCAI) on AI Reasoning</title>
    <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/</link>
    <description>Recent content in Data-Centric AI (DCAI) on AI Reasoning</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Data-Centric AI vs Model-Centric AI</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/data-centric-vs-model-centric/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/data-centric-vs-model-centric/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Data-Centric AI vs Model-Centric AI &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;MIT Lecture : &lt;a href=&#34;https://dcai.csail.mit.edu/&#34;&gt;https://dcai.csail.mit.edu/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;GitHub for Labs: &lt;a href=&#34;https://github.com/dcai-course/dcai-lab&#34;&gt;https://github.com/dcai-course/dcai-lab&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-is-the-traditional-model-centric-approach-to-machine-learning&#34;&gt;&#xA;  Q1: What is the traditional model-centric approach to machine learning?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-is-the-traditional-model-centric-approach-to-machine-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Traditionally, machine learning has focused on &lt;em&gt;model-centric AI&lt;/em&gt;, where the dataset is fixed and the goal is to tune the model:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Learn different model architectures&lt;/li&gt;&#xA;&lt;li&gt;Modify hyperparameters and training losses&lt;/li&gt;&#xA;&lt;li&gt;Focus on improving model performance given clean data&lt;/li&gt;&#xA;&lt;li&gt;This is how ML is often taught in courses (e.g., MIT 6.036)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q2-what-challenges-arise-in-real-world-ml-settings&#34;&gt;&#xA;  Q2: What challenges arise in real-world ML settings?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-what-challenges-arise-in-real-world-ml-settings&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Real-world data is:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Label Errors</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/label-errors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/label-errors/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Label Errors &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-are-label-errors-and-why-do-they-matter&#34;&gt;&#xA;  Q1: What are label errors and why do they matter?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-are-label-errors-and-why-do-they-matter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Label errors&lt;/strong&gt;: Incorrect labels in training/testing datasets.&lt;/li&gt;&#xA;&lt;li&gt;They cause &lt;strong&gt;worse model performance&lt;/strong&gt;, &lt;strong&gt;benchmark misinterpretation&lt;/strong&gt;, and &lt;strong&gt;deployment risks&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-what-are-the-types-of-label-noise&#34;&gt;&#xA;  Q2: What are the types of label noise?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-what-are-the-types-of-label-noise&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Uniform/Symmetric noise&lt;/strong&gt;: Random label flipping.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Systematic/Asymmetric noise&lt;/strong&gt;: Certain labels more likely flipped.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Instance-dependent noise&lt;/strong&gt;: Noise depends on input features (out of scope here).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-what-is-confident-learning-cl&#34;&gt;&#xA;  Q3: What is Confident Learning (CL)?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-what-is-confident-learning-cl&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A framework to:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Find label errors&lt;/li&gt;&#xA;&lt;li&gt;Rank examples by label issue likelihood&lt;/li&gt;&#xA;&lt;li&gt;Learn with noisy labels&lt;/li&gt;&#xA;&lt;li&gt;Characterize noise structure&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Model-agnostic&lt;/strong&gt;: uses model-predicted probabilities.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-how-does-cl-detect-label-errors&#34;&gt;&#xA;  Q4: How does CL detect label errors?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-how-does-cl-detect-label-errors&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use predicted probabilities + noisy labels.&lt;/li&gt;&#xA;&lt;li&gt;Estimate joint distribution of noisy vs. true labels.&lt;/li&gt;&#xA;&lt;li&gt;Detect off-diagonal entries = label errors.&lt;/li&gt;&#xA;&lt;li&gt;Key techniques: &lt;strong&gt;Prune&lt;/strong&gt;, &lt;strong&gt;Count&lt;/strong&gt;, &lt;strong&gt;Rank&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;q5-why-is-a-noise-process-assumption-needed&#34;&gt;&#xA;  Q5: Why is a noise process assumption needed?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-why-is-a-noise-process-assumption-needed&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;To &lt;strong&gt;separate&lt;/strong&gt; model uncertainty (epistemic) and label noise (aleatoric).&lt;/li&gt;&#xA;&lt;li&gt;CL assumes &lt;strong&gt;class-conditional noise&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-why-not-just-sort-by-loss&#34;&gt;&#xA;  Q6: Why not just sort by loss?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-why-not-just-sort-by-loss&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Sorting by loss doesn&amp;rsquo;t tell you:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Where to cut off&lt;/li&gt;&#xA;&lt;li&gt;How many label errors exist&lt;/li&gt;&#xA;&lt;li&gt;How to automate error finding without human review&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-how-does-cl-achieve-robustness-to-imperfect-predictions&#34;&gt;&#xA;  Q7: How does CL achieve robustness to imperfect predictions?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-how-does-cl-achieve-robustness-to-imperfect-predictions&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Prune&lt;/strong&gt; low-confidence examples&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Count&lt;/strong&gt; robustly across examples&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Rank&lt;/strong&gt; by predicted probabilities&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-how-does-label-noise-affect-real-world-ml&#34;&gt;&#xA;  Q8: How does label noise affect real-world ML?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-how-does-label-noise-affect-real-world-ml&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Real-world datasets are &lt;strong&gt;not random noise&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Deep learning claims about noise robustness often assume unrealistic random noise.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-what-happens-when-test-sets-have-label-errors&#34;&gt;&#xA;  Q9: What happens when test sets have label errors?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-what-happens-when-test-sets-have-label-errors&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Benchmark &lt;strong&gt;model rankings change&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;A &amp;ldquo;better&amp;rdquo; model might actually underperform in deployment.&lt;/li&gt;&#xA;&lt;li&gt;Quantifying label errors in &lt;strong&gt;test sets is critical&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q10-how-can-practitioners-fix-this&#34;&gt;&#xA;  Q10: How can practitioners fix this?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-how-can-practitioners-fix-this&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use &lt;strong&gt;corrected test sets&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Benchmark using cleaned labels.&lt;/li&gt;&#xA;&lt;li&gt;Tools like &lt;strong&gt;cleanlab&lt;/strong&gt; can automate finding label issues.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q11-key-takeaways&#34;&gt;&#xA;  Q11: Key Takeaways&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q11-key-takeaways&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Confident learning enables data-centric model improvements.&lt;/li&gt;&#xA;&lt;li&gt;Even small label error rates (~3-6%) can &lt;strong&gt;destabilize ML benchmarks&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;ML needs to &lt;strong&gt;quantify label noise&lt;/strong&gt; to ensure real-world reliability.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;references&#34;&gt;&#xA;  References&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#references&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.00068&#34;&gt;Confident Learning Paper (JAIR 2021)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.14749&#34;&gt;Pervasive Label Errors Paper (NeurIPS 2021)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://labelerrors.com/&#34;&gt;Label Errors Website&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cleanlab/cleanlab&#34;&gt;Cleanlab GitHub&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Advanced Confident Learning and Applications for GenAI</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/advanced-confident-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/advanced-confident-learning/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Advanced Confident Learning and Applications for GenAI&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-is-the-main-focus-of-this-lecture&#34;&gt;&#xA;  Q1: What is the main focus of this lecture?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-is-the-main-focus-of-this-lecture&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Advanced Confident Learning (CL)&lt;/strong&gt;: Theory, methods, and applications, especially for &lt;strong&gt;Generative AI&lt;/strong&gt; (images, text).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-how-does-confident-learning-cl-work-at-its-core&#34;&gt;&#xA;  Q2: How does Confident Learning (CL) work at its core?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-how-does-confident-learning-cl-work-at-its-core&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Inputs: &lt;strong&gt;Noisy labels&lt;/strong&gt; and &lt;strong&gt;predicted probabilities&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Core idea: Find &lt;strong&gt;self-confidence thresholds&lt;/strong&gt; per class to detect label errors.&lt;/li&gt;&#xA;&lt;li&gt;Estimate if an example is an error, correct label, or outlier.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-what-is-the-quick-intuition-behind-cl&#34;&gt;&#xA;  Q3: What is the quick intuition behind CL?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-what-is-the-quick-intuition-behind-cl&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Off-diagonal entries in the predicted-vs-true label matrix reveal &lt;strong&gt;label errors&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-what-makes-cl-robust-to-noise&#34;&gt;&#xA;  Q4: What makes CL robust to noise?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-what-makes-cl-robust-to-noise&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Prune principle&lt;/strong&gt;: Remove low-confidence errors before training.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Count principle&lt;/strong&gt;: Use counts rather than raw outputs.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Rank principle&lt;/strong&gt;: Rank by model confidence, not rely on probabilities.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-how-is-cl-better-than-just-loss-adjustment-techniques&#34;&gt;&#xA;  Q5: How is CL better than just loss adjustment techniques?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-how-is-cl-better-than-just-loss-adjustment-techniques&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CL &lt;strong&gt;avoids error propagation&lt;/strong&gt; common in reweighting methods.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Robust&lt;/strong&gt; to stochastic/noisy outputs from real-world models.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-what-is-the-theoretical-guarantee-of-cl&#34;&gt;&#xA;  Q6: What is the theoretical guarantee of CL?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-what-is-the-theoretical-guarantee-of-cl&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;As long as correct labels dominate wrong ones in a class, CL can exactly find errors ‚Äî even if model probabilities are imperfect (up to ~33% wrong).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-why-does-label-noise-in-test-sets-matter&#34;&gt;&#xA;  Q7: Why does label noise in test sets matter?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-why-does-label-noise-in-test-sets-matter&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;3.4% of labels&lt;/strong&gt; in popular ML test sets are wrong.&lt;/li&gt;&#xA;&lt;li&gt;Small label error rates (~6%) can &lt;strong&gt;change model rankings&lt;/strong&gt; drastically.&lt;/li&gt;&#xA;&lt;li&gt;Benchmark results can be misleading without corrected test sets.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-how-to-fix-label-errors-in-test-sets&#34;&gt;&#xA;  Q8: How to fix label errors in test sets?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-how-to-fix-label-errors-in-test-sets&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use &lt;strong&gt;majority consensus&lt;/strong&gt; among reviewers to correct labels.&lt;/li&gt;&#xA;&lt;li&gt;Prune uncertain/multi-label examples.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-how-is-cl-applied-to-generative-ai-models&#34;&gt;&#xA;  Q9: How is CL applied to Generative AI models?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-how-is-cl-applied-to-generative-ai-models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Before training&lt;/strong&gt;: Clean training data to avoid issues in model generation.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;After generation&lt;/strong&gt;: Run CL on generated data (e.g., images/text) to remove/fix errors.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q10-example-use-cases-for-cl-in-generative-ai&#34;&gt;&#xA;  Q10: Example use cases for CL in Generative AI?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-example-use-cases-for-cl-in-generative-ai&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Scenario&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Application&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Image generation (e.g., DALL-E)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Improve datasets pre/post generation&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;LLM outputs (e.g., GPT-4)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Post-process outputs for better quality&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;RAG (Retrieval-Augmented Generation)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Clean retrieved answers&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Trustworthy Language Models (TLM)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Attach confidence scores to outputs&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;q11-final-takeaways&#34;&gt;&#xA;  Q11: Final Takeaways&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q11-final-takeaways&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CL is &lt;strong&gt;model-agnostic&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Improves reliability&lt;/strong&gt; of both traditional ML models and Generative AI.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;One line of code&lt;/strong&gt; to apply using &lt;a href=&#34;https://github.com/cleanlab/cleanlab&#34;&gt;cleanlab&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;references&#34;&gt;&#xA;  References&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#references&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cleanlab/cleanlab&#34;&gt;Confident Learning: GitHub Repository&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://labelerrors.com/&#34;&gt;Label Errors Website&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://help.cleanlab.ai/tutorials/tlm/&#34;&gt;Trustworthy Language Models (TLM) Tutorial&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;Related Papers:&lt;/a&gt; (GPT-3), (Northcutt et al., Pervasive Label Errors, 2021)&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Class Imbalance, Outliers, and Distribution Shift</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/class-imbalance-outliers-distribution-shift/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/class-imbalance-outliers-distribution-shift/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Class Imbalance, Outliers, and Distribution Shift &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-are-the-main-problems-discussed-in-this-lecture&#34;&gt;&#xA;  Q1: What are the main problems discussed in this lecture?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-are-the-main-problems-discussed-in-this-lecture&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Class imbalance&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Outliers&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Distribution shift&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-what-is-class-imbalance-and-why-is-it-a-problem&#34;&gt;&#xA;  Q2: What is class imbalance and why is it a problem?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-what-is-class-imbalance-and-why-is-it-a-problem&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: Some classes occur much less frequently than others.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Examples&lt;/strong&gt;: COVID detection, fraud detection, manufacturing defects, self-driving cars.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Naive models can have misleadingly high accuracy while failing on rare classes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-how-do-we-address-class-imbalance&#34;&gt;&#xA;  Q3: How do we address class imbalance?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-how-do-we-address-class-imbalance&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sampling Techniques&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Sample weights&lt;/strong&gt; (less stable for mini-batch training)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Over-sampling&lt;/strong&gt; (replicating minority class examples)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Under-sampling&lt;/strong&gt; (dropping majority class examples)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;SMOTE&lt;/strong&gt; (synthetic minority over-sampling)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Balanced mini-batch training&lt;/strong&gt; (better distribution in each batch)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Choose appropriate evaluation metrics&lt;/strong&gt;: precision, recall, F-beta score.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-what-are-outliers-and-why-are-they-problematic&#34;&gt;&#xA;  Q4: What are outliers and why are they problematic?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-what-are-outliers-and-why-are-they-problematic&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: Datapoints that differ significantly from the norm.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Causes&lt;/strong&gt;: Measurement error, bad data collection, adversarial inputs, rare events.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Impact&lt;/strong&gt;: Outliers can harm training and inference stability.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-how-do-we-detect-outliers&#34;&gt;&#xA;  Q5: How do we detect outliers?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-how-do-we-detect-outliers&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Simple methods&lt;/strong&gt;: Tukey&amp;rsquo;s fences, Z-score analysis.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;More advanced&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Isolation forest&lt;/strong&gt; (tree-based)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;KNN distance&lt;/strong&gt; (neighbor proximity)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Autoencoders&lt;/strong&gt; (reconstruction loss)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: ROC curve and AUROC score.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-what-is-distribution-shift&#34;&gt;&#xA;  Q6: What is distribution shift?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-what-is-distribution-shift&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Definition&lt;/strong&gt;: Training and test distributions differ.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Almost all real-world ML deployments experience it.&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-what-are-the-types-of-distribution-shift&#34;&gt;&#xA;  Q7: What are the types of distribution shift?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-what-are-the-types-of-distribution-shift&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Type&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Meaning&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Example&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Covariate shift&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;(p(x)) changes, (p(y&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;x)) stays the same&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Concept shift&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;(p(y&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;x)) changes, (p(x)) stays the same&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Prior probability shift&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;(p(y)) changes, (p(x&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;y)) stays the same&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;q8-how-do-we-detect-and-handle-distribution-shift&#34;&gt;&#xA;  Q8: How do we detect and handle distribution shift?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-how-do-we-detect-and-handle-distribution-shift&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Detection&lt;/strong&gt;: Monitor metrics and statistical properties of data.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Handling&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Retrain with better data.&lt;/li&gt;&#xA;&lt;li&gt;Use sample reweighting if unlabeled test data is available.&lt;/li&gt;&#xA;&lt;li&gt;Concept shift remains hardest to fix without labeled test data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-final-takeaways&#34;&gt;&#xA;  Q9: Final Takeaways&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-final-takeaways&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Handling &lt;strong&gt;class imbalance, outliers, and distribution shift&lt;/strong&gt; is critical for building robust, real-world ML systems.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Evaluation metric choice&lt;/strong&gt;, &lt;strong&gt;proper data preprocessing&lt;/strong&gt;, and &lt;strong&gt;continuous monitoring&lt;/strong&gt; are key strategies.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;references&#34;&gt;&#xA;  References&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#references&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://imbalanced-learn.org&#34;&gt;imbalanced-learn package&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1106.1813&#34;&gt;SMOTE Paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pyod.readthedocs.io/&#34;&gt;PyOD library for outlier detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://direct.mit.edu/books/book/3841/Dataset-Shift-in-Machine-Learning&#34;&gt;Dataset Shift Book&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/outlier_detection.html&#34;&gt;Outlier detection in scikit-learn&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/dcai-course/dcai-lab/blob/master/outliers/Lab%20-%20Outliers.ipynb&#34;&gt;Lab assignment for Outliers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Dataset Creation and Curation</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/dataset-creation-curation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/dataset-creation-curation/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Dataset Creation and Curation &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-are-the-main-themes-of-dataset-creation-and-curation&#34;&gt;&#xA;  Q1: What are the main themes of dataset creation and curation?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-are-the-main-themes-of-dataset-creation-and-curation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Framing the ML task correctly.&lt;/li&gt;&#xA;&lt;li&gt;Addressing data sourcing concerns like selection bias.&lt;/li&gt;&#xA;&lt;li&gt;Handling label sourcing and quality control.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-why-is-careful-sourcing-of-data-important&#34;&gt;&#xA;  Q2: Why is careful sourcing of data important?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-why-is-careful-sourcing-of-data-important&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ML models exploit spurious correlations.&lt;/li&gt;&#xA;&lt;li&gt;If training data does not match real-world deployment conditions, models can fail badly.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-what-is-selection-bias-and-what-are-its-common-causes&#34;&gt;&#xA;  Q3: What is selection bias and what are its common causes?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-what-is-selection-bias-and-what-are-its-common-causes&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Selection bias&lt;/strong&gt;: Systematic mismatch between training data and deployment data.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Causes&lt;/strong&gt;: Time/location bias, demographic bias, response bias, availability bias, long tail bias.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-how-can-we-deal-with-selection-bias-during-data-collection&#34;&gt;&#xA;  Q4: How can we deal with selection bias during data collection?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-how-can-we-deal-with-selection-bias-during-data-collection&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hold out validation sets that mimic deployment conditions, such as latest data, new locations, or oversampled rare events.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-how-can-we-estimate-how-much-data-we-need&#34;&gt;&#xA;  Q5: How can we estimate how much data we need?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-how-can-we-estimate-how-much-data-we-need&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use a method to measure learning curves by sub-sampling data and fitting a simple log-log model to predict performance scaling.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-what-is-the-formula-used-to-predict-model-error-with-more-data&#34;&gt;&#xA;  Q6: What is the formula used to predict model error with more data?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-what-is-the-formula-used-to-predict-model-error-with-more-data&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;( \log(&#x9;ext{error}) = -a \cdot \log(n) + b )&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-what-are-concerns-when-labeling-data-with-crowdsourced-workers&#34;&gt;&#xA;  Q7: What are concerns when labeling data with crowdsourced workers?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-what-are-concerns-when-labeling-data-with-crowdsourced-workers&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Variability in annotator accuracy.&lt;/li&gt;&#xA;&lt;li&gt;Possibility of annotator collusion.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-how-can-we-maintain-label-quality-during-crowdsourcing&#34;&gt;&#xA;  Q8: How can we maintain label quality during crowdsourcing?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-how-can-we-maintain-label-quality-during-crowdsourcing&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Insert &amp;ldquo;quality control&amp;rdquo; examples with known ground-truth to monitor annotator performance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-what-methods-are-used-to-curate-labels-from-multiple-annotators&#34;&gt;&#xA;  Q9: What methods are used to curate labels from multiple annotators?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-what-methods-are-used-to-curate-labels-from-multiple-annotators&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Majority Vote and Inter-Annotator Agreement&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Dawid-Skene model&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;CROWDLAB&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q10-how-does-majority-vote-work&#34;&gt;&#xA;  Q10: How does Majority Vote work?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-how-does-majority-vote-work&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Assigns the label chosen by the majority of annotators.&lt;/li&gt;&#xA;&lt;li&gt;Confidence is based on inter-annotator agreement.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q11-what-are-downsides-of-majority-vote&#34;&gt;&#xA;  Q11: What are downsides of Majority Vote?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q11-what-are-downsides-of-majority-vote&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Ties are ambiguous.&lt;/li&gt;&#xA;&lt;li&gt;Bad annotators have equal influence as good ones.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q12-what-is-the-dawid-skene-model&#34;&gt;&#xA;  Q12: What is the Dawid-Skene model?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q12-what-is-the-dawid-skene-model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Models each annotator with a confusion matrix.&lt;/li&gt;&#xA;&lt;li&gt;Uses Bayesian inference (often approximated with EM) to estimate consensus labels and annotator quality.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q13-what-are-limitations-of-dawid-skene&#34;&gt;&#xA;  Q13: What are limitations of Dawid-Skene?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q13-what-are-limitations-of-dawid-skene&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Requires strong assumptions.&lt;/li&gt;&#xA;&lt;li&gt;Performs poorly if examples are labeled by few annotators.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q14-what-is-crowdlab&#34;&gt;&#xA;  Q14: What is CROWDLAB?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q14-what-is-crowdlab&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Combines classifier predictions and annotator labels for better consensus.&lt;/li&gt;&#xA;&lt;li&gt;Weights depend on model confidence and inter-annotator agreement.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q15-how-does-crowdlab-work&#34;&gt;&#xA;  Q15: How does CROWDLAB work?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q15-how-does-crowdlab-work&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For examples labeled by few annotators, rely more on the classifier.&lt;/li&gt;&#xA;&lt;li&gt;For examples labeled by many annotators, rely more on label agreement.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q16-how-are-weights-estimated-in-crowdlab&#34;&gt;&#xA;  Q16: How are weights estimated in CROWDLAB?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q16-how-are-weights-estimated-in-crowdlab&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Based on annotator agreement rates and classifier accuracy normalized against a majority-class baseline.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q17-what-is-the-hands-on-lab-assignment-for-this-lecture&#34;&gt;&#xA;  Q17: What is the hands-on lab assignment for this lecture?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q17-what-is-the-hands-on-lab-assignment-for-this-lecture&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Analyze a multi-annotator dataset and implement methods for estimating consensus labels and annotator quality.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;references&#34;&gt;&#xA;  References&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#references&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://books.google.com/books/about/Human_in_the_Loop_Machine_Learning.html?id=LCh0zQEACAAJ&#34;&gt;Human-in-the-Loop ML textbook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1807.04975&#34;&gt;Recognition in terra incognita (Beery et al., 2018)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.12673&#34;&gt;Constructive prediction of generalization error (Rosenfeld et al., 2020)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.06812&#34;&gt;CROWDLAB paper (Goh et al., 2022)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://catalogofbias.org/biases/selection-bias/&#34;&gt;Catalogue of Bias (Selection Bias)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/dcai-course/dcai-lab/blob/master/dataset_curation/Lab%20-%20Dataset%20Curation.ipynb&#34;&gt;Dataset curation lab notebook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Data-Centric Evaluation of ML Models</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/data-centric-evaluation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/data-centric-evaluation/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Data-Centric Evaluation of ML Models &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Instead of only asking &lt;strong&gt;how accurate&lt;/strong&gt;, we ask &lt;strong&gt;why and where does the model fail&lt;/strong&gt; ‚Äî and whether the data itself causes it.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Aspect&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Model-Centric Evaluation&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Data-Centric Evaluation&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Focus&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Overall model score&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Specific weaknesses tied to data&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Metrics&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Accuracy, ROC, etc.&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Slice-specific accuracy, Error analysis&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Blind spots&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Hide rare data failures&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Detect hidden failures in data&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Error tracing&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Hard&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Directly trace errors to dirty/outlier/bias data&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Example&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&amp;ldquo;95% accurate model!&amp;rdquo;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&amp;ldquo;Fails badly on young users with rare diseases&amp;rdquo;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Mindset&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Improve model tuning&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Fix the dataset (quality, balance, coverage)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;q1-what-is-the-typical-ml-workflow-before-deployment&#34;&gt;&#xA;  Q1: What is the typical ML workflow before deployment?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-is-the-typical-ml-workflow-before-deployment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Collect data and define the ML task.&lt;/li&gt;&#xA;&lt;li&gt;Explore and preprocess the data.&lt;/li&gt;&#xA;&lt;li&gt;Train a straightforward model.&lt;/li&gt;&#xA;&lt;li&gt;Investigate shortcomings in the model and dataset.&lt;/li&gt;&#xA;&lt;li&gt;Improve dataset and model iteratively.&lt;/li&gt;&#xA;&lt;li&gt;Deploy the model and monitor for new issues.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-why-is-model-evaluation-critical&#34;&gt;&#xA;  Q2: Why is model evaluation critical?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-why-is-model-evaluation-critical&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Evaluation affects practical outcomes in real-world applications.&lt;/li&gt;&#xA;&lt;li&gt;Poor evaluation choices can lead to misleading or harmful models.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-what-are-examples-of-evaluation-metrics-for-classification&#34;&gt;&#xA;  Q3: What are examples of evaluation metrics for classification?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-what-are-examples-of-evaluation-metrics-for-classification&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Accuracy, balanced accuracy, precision, recall, log loss, AUROC, calibration error.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-what-are-some-pitfalls-in-model-evaluation&#34;&gt;&#xA;  Q4: What are some pitfalls in model evaluation?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-what-are-some-pitfalls-in-model-evaluation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Data leakage by using non-held-out data.&lt;/li&gt;&#xA;&lt;li&gt;Misspecified metrics hiding failures in subpopulations.&lt;/li&gt;&#xA;&lt;li&gt;Validation data not representing deployment settings.&lt;/li&gt;&#xA;&lt;li&gt;Label errors.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-how-is-text-generation-model-evaluation-different&#34;&gt;&#xA;  Q5: How is text generation model evaluation different?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-how-is-text-generation-model-evaluation-different&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Human evaluations (üëçüëé or Likert scales).&lt;/li&gt;&#xA;&lt;li&gt;LLM evaluations with multiple criteria.&lt;/li&gt;&#xA;&lt;li&gt;Automated metrics like ROUGE, BLEU, and Perplexity.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-what-is-a-data-slice&#34;&gt;&#xA;  Q6: What is a data slice?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-what-is-a-data-slice&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A subset of the dataset sharing a common characteristic, e.g., different sensor types, demographics.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-why-is-it-insufficient-to-delete-sensitive-features-to-address-slice-fairness&#34;&gt;&#xA;  Q7: Why is it insufficient to delete sensitive features to address slice fairness?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-why-is-it-insufficient-to-delete-sensitive-features-to-address-slice-fairness&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Slice membership information may be correlated with other features.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-how-can-we-improve-model-performance-for-underperforming-slices&#34;&gt;&#xA;  Q8: How can we improve model performance for underperforming slices?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-how-can-we-improve-model-performance-for-underperforming-slices&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use a more flexible model.&lt;/li&gt;&#xA;&lt;li&gt;Over-sample the minority subgroup.&lt;/li&gt;&#xA;&lt;li&gt;Collect more data from the subgroup.&lt;/li&gt;&#xA;&lt;li&gt;Engineer new features that better capture subgroup specifics.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-how-to-discover-underperforming-subpopulations&#34;&gt;&#xA;  Q9: How to discover underperforming subpopulations?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-how-to-discover-underperforming-subpopulations&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Sort validation examples by loss.&lt;/li&gt;&#xA;&lt;li&gt;Cluster high-loss examples to find commonalities.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q10-what-are-typical-causes-of-wrong-predictions&#34;&gt;&#xA;  Q10: What are typical causes of wrong predictions?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-what-are-typical-causes-of-wrong-predictions&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Incorrect labels.&lt;/li&gt;&#xA;&lt;li&gt;Examples that do not belong to any class.&lt;/li&gt;&#xA;&lt;li&gt;Outlier examples.&lt;/li&gt;&#xA;&lt;li&gt;Model type limitations.&lt;/li&gt;&#xA;&lt;li&gt;Conflicting or noisy dataset labels.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q11-what-actions-can-address-wrong-predictions&#34;&gt;&#xA;  Q11: What actions can address wrong predictions?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q11-what-actions-can-address-wrong-predictions&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Correct labels.&lt;/li&gt;&#xA;&lt;li&gt;Remove fundamentally unpredictable examples.&lt;/li&gt;&#xA;&lt;li&gt;Augment or normalize outlier examples.&lt;/li&gt;&#xA;&lt;li&gt;Fit better model architectures or do feature engineering.&lt;/li&gt;&#xA;&lt;li&gt;Enrich the dataset to distinguish overlapping classes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q12-what-is-the-concept-of-leave-one-out-influence&#34;&gt;&#xA;  Q12: What is the concept of leave-one-out influence?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q12-what-is-the-concept-of-leave-one-out-influence&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Measure the impact of omitting a datapoint on the model‚Äôs validation performance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q13-what-is-data-shapley&#34;&gt;&#xA;  Q13: What is Data Shapley?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q13-what-is-data-shapley&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A method that averages the influence of a datapoint over all subsets containing it, providing a fairer measure of its importance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q14-how-can-we-approximate-influence&#34;&gt;&#xA;  Q14: How can we approximate influence?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q14-how-can-we-approximate-influence&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Monte Carlo sampling methods.&lt;/li&gt;&#xA;&lt;li&gt;Closed-form approximations for simple models like linear regression and k-NN.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q15-why-review-influential-samples&#34;&gt;&#xA;  Q15: Why review influential samples?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q15-why-review-influential-samples&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Correcting highly influential mislabeled examples can lead to significant accuracy improvements.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;references&#34;&gt;&#xA;  References&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#references&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cook%27s_distance&#34;&gt;Cook‚Äôs Distance (Linear Regression Influence)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.08734&#34;&gt;Similarity Search Scaling for Big Data (Johnson et al., 2019)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/20591&#34;&gt;Trustworthy Data Influence Estimation&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cleanlab/cleanlab&#34;&gt;Confident Learning and Cleanlab Project&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Data Curation and LLMs</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/data-curation-llms/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/data-curation-llms/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Data Curation and LLMs &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-is-the-background-of-llms-discussed&#34;&gt;&#xA;  Q1: What is the background of LLMs discussed?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-is-the-background-of-llms-discussed&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLMs like ChatGPT, GPT-4, and Llama are sequence models trained to predict the next token.&lt;/li&gt;&#xA;&lt;li&gt;They use unsupervised pre-training on massive internet-scale corpora and can solve various NLP tasks.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-what-are-the-applications-of-llms-discussed&#34;&gt;&#xA;  Q2: What are the applications of LLMs discussed?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-what-are-the-applications-of-llms-discussed&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Zero-shot prompting&lt;/li&gt;&#xA;&lt;li&gt;Few-shot prompting&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-what-is-the-focus-of-this-lecture&#34;&gt;&#xA;  Q3: What is the focus of this lecture?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-what-is-the-focus-of-this-lecture&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Using LLMs for data curation&lt;/li&gt;&#xA;&lt;li&gt;Evaluating LLM output data&lt;/li&gt;&#xA;&lt;li&gt;Curation for LLM pre-training and application fine-tuning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-how-are-llms-used-for-data-curation&#34;&gt;&#xA;  Q4: How are LLMs used for data curation?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-how-are-llms-used-for-data-curation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;They act as powerful, flexible, and computationally inexpensive reasoning engines for text data curation.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-how-was-pii-detection-handled-traditionally-vs-with-llms&#34;&gt;&#xA;  Q5: How was PII detection handled traditionally vs with LLMs?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-how-was-pii-detection-handled-traditionally-vs-with-llms&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Traditional: Custom regex rules.&lt;/li&gt;&#xA;&lt;li&gt;With LLMs: Zero-shot prompting to detect a wider range of PII without needing extensive rule-writing.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-how-can-grammar-checking-be-improved-with-llms&#34;&gt;&#xA;  Q6: How can grammar checking be improved with LLMs?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-how-can-grammar-checking-be-improved-with-llms&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Traditional: Rule-based systems like LanguageTool.&lt;/li&gt;&#xA;&lt;li&gt;LLM-based: Fine-tuning LLMs on curated grammatical acceptability datasets like CoLA.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-what-is-a-major-challenge-in-working-with-llm-outputs&#34;&gt;&#xA;  Q7: What is a major challenge in working with LLM outputs?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-what-is-a-major-challenge-in-working-with-llm-outputs&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hallucinations, where LLMs produce confidently incorrect information.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-how-can-we-evaluate-llm-outputs-more-reliably&#34;&gt;&#xA;  Q8: How can we evaluate LLM outputs more reliably?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-how-can-we-evaluate-llm-outputs-more-reliably&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use a stronger LLM (e.g., GPT-4) to judge the outputs of weaker LLMs.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-what-evidence-supports-llm-based-evaluation&#34;&gt;&#xA;  Q9: What evidence supports LLM-based evaluation?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-what-evidence-supports-llm-based-evaluation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;AlpaGasus fine-tuning project showed better results by curating higher-quality data points evaluated by GPT-3.5 and GPT-4.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q10-what-is-the-challenge-of-using-llms-to-evaluate-other-llms&#34;&gt;&#xA;  Q10: What is the challenge of using LLMs to evaluate other LLMs?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-what-is-the-challenge-of-using-llms-to-evaluate-other-llms&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It risks circular evaluation (turtles all the way down) if the same LLMs are involved in both generation and evaluation.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q11-what-method-helps-with-llm-uncertainty-quantification&#34;&gt;&#xA;  Q11: What method helps with LLM uncertainty quantification?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q11-what-method-helps-with-llm-uncertainty-quantification&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Natural Language Inference (NLI)-based techniques that check for answer contradictions.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q12-what-are-the-key-stages-of-data-curation-for-llm-pre-training&#34;&gt;&#xA;  Q12: What are the key stages of data curation for LLM pre-training?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q12-what-are-the-key-stages-of-data-curation-for-llm-pre-training&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Focus on corpus quality because errors are difficult to &amp;ldquo;un-learn.&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;Use supervised fine-tuning and reinforcement learning from human feedback.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q13-how-does-data-curation-differ-for-llm-applications&#34;&gt;&#xA;  Q13: How does data curation differ for LLM applications?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q13-how-does-data-curation-differ-for-llm-applications&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Zero-shot prompting&lt;/li&gt;&#xA;&lt;li&gt;Few-shot prompting&lt;/li&gt;&#xA;&lt;li&gt;Retrieval-augmented generation&lt;/li&gt;&#xA;&lt;li&gt;Supervised fine-tuning&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q14-why-is-fine-tuning-important-for-llm-applications&#34;&gt;&#xA;  Q14: Why is fine-tuning important for LLM applications?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q14-why-is-fine-tuning-important-for-llm-applications&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Fine-tuning provides the best task-specific performance.&lt;/li&gt;&#xA;&lt;li&gt;It enables training smaller models to match large model performance through synthetic data generation.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q15-how-is-synthetic-data-curated-for-llm-fine-tuning&#34;&gt;&#xA;  Q15: How is synthetic data curated for LLM fine-tuning?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q15-how-is-synthetic-data-curated-for-llm-fine-tuning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Generate synthetic data using a powerful LLM.&lt;/li&gt;&#xA;&lt;li&gt;Use uncertainty quantification to retain only high-confidence examples.&lt;/li&gt;&#xA;&lt;li&gt;Train classifiers to filter out unrealistic synthetic data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q16-what-is-the-future-trend-in-data-curation-for-llms&#34;&gt;&#xA;  Q16: What is the future trend in data curation for LLMs?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q16-what-is-the-future-trend-in-data-curation-for-llms&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Growth of powerful multi-modal LLMs and new tools like CleanVision and GPT-4 to automate and improve data quality.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;references&#34;&gt;&#xA;  References&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#references&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://twitter.com/karpathy/status/1654892810590650376&#34;&gt;Karpathy on LLM prompting&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/LeapBeyond/scrubadub&#34;&gt;Scrubadub for regex-based PII detection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/languagetool-org/languagetool&#34;&gt;LanguageTool grammar checker&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://nyu-mll.github.io/CoLA/&#34;&gt;CoLA dataset&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.08701.pdf&#34;&gt;AlpaGasus paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2308.16175.pdf&#34;&gt;Trustworthy Language Models (TLM)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.05463&#34;&gt;Textbooks Are All You Need (Li et al., 2023)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca project&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/cleanlab/cleanvision&#34;&gt;CleanVision project&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Growing or Compressing Datasets</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/growing-or-compressing-datasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/growing-or-compressing-datasets/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Growing or Compressing Datasets &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-is-the-main-focus-of-this-lecture&#34;&gt;&#xA;  Q1: What is the main focus of this lecture?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-is-the-main-focus-of-this-lecture&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Techniques to carefully select examples for labeling to reduce the burden in ML systems.&lt;/li&gt;&#xA;&lt;li&gt;Growing datasets via active learning and compressing datasets via core-set selection.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-what-is-active-learning&#34;&gt;&#xA;  Q2: What is active learning?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-what-is-active-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A method to intelligently select the most informative examples to label next, maximizing model improvement with fewer labeled samples.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-how-does-pool-based-active-learning-work&#34;&gt;&#xA;  Q3: How does pool-based active learning work?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-how-does-pool-based-active-learning-work&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Start with a pool of unlabeled examples.&lt;/li&gt;&#xA;&lt;li&gt;At each round, score examples using an acquisition function (e.g., entropy of predicted probabilities).&lt;/li&gt;&#xA;&lt;li&gt;Select and label top examples.&lt;/li&gt;&#xA;&lt;li&gt;Retrain the model with newly labeled data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-what-is-a-common-acquisition-function-used-in-active-learning&#34;&gt;&#xA;  Q4: What is a common acquisition function used in active learning?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-what-is-a-common-acquisition-function-used-in-active-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Entropy of the predicted class probabilities, encouraging labeling of uncertain examples.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-how-does-active-learning-compare-to-passive-learning&#34;&gt;&#xA;  Q5: How does active learning compare to passive learning?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-how-does-active-learning-compare-to-passive-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Active learning exponentially improves data efficiency compared to random/passive sampling.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-what-practical-challenges-does-active-learning-face&#34;&gt;&#xA;  Q6: What practical challenges does active learning face?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-what-practical-challenges-does-active-learning-face&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;High computational costs with large models and datasets.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-how-can-active-learning-be-made-more-practical&#34;&gt;&#xA;  Q7: How can active learning be made more practical?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-how-can-active-learning-be-made-more-practical&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Batch active learning with diversity selection.&lt;/li&gt;&#xA;&lt;li&gt;Efficient candidate selection using methods like SEALS to reduce search space.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-what-is-seals&#34;&gt;&#xA;  Q8: What is SEALS?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-what-is-seals&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Similarity Search for Efficient Active Learning and Search of Rare Concepts.&lt;/li&gt;&#xA;&lt;li&gt;Uses nearest neighbor search in embedding space to limit active learning candidate pool.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-what-is-core-set-selection&#34;&gt;&#xA;  Q9: What is core-set selection?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-what-is-core-set-selection&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Choosing a small representative subset of a large labeled dataset that preserves model performance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q10-why-is-core-set-selection-important&#34;&gt;&#xA;  Q10: Why is core-set selection important?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-why-is-core-set-selection-important&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;When we have massive datasets, it reduces computational, time, and energy costs without sacrificing accuracy.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q11-what-methods-help-with-core-set-selection&#34;&gt;&#xA;  Q11: What methods help with core-set selection?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q11-what-methods-help-with-core-set-selection&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Greedy k-centers approach.&lt;/li&gt;&#xA;&lt;li&gt;Selection via Proxy: using smaller proxy models to guide subset selection.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q12-how-does-selection-via-proxy-work&#34;&gt;&#xA;  Q12: How does Selection via Proxy work?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q12-how-does-selection-via-proxy-work&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Train a lightweight model (proxy) on the full data.&lt;/li&gt;&#xA;&lt;li&gt;Use it to select a subset for training a larger model, speeding up training significantly.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q13-what-are-key-takeaways-about-dataset-growth-and-compression&#34;&gt;&#xA;  Q13: What are key takeaways about dataset growth and compression?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q13-what-are-key-takeaways-about-dataset-growth-and-compression&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Active learning enables data-efficient labeling for growing datasets.&lt;/li&gt;&#xA;&lt;li&gt;Core-set selection enables training efficiency for already large datasets.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;active-learning-vs-confident-learning&#34;&gt;&#xA;  Active Learning vs. Confident Learning&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#active-learning-vs-confident-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Category&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Active Learning&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Confident Learning&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Main Goal&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Select the most informative examples to label next&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Find mislabeled examples in existing labeled data&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;When Used&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;During dataset growth (annotation phase)&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;After labels exist (cleaning phase)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Data State&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Partially labeled data pool&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Fully labeled (but noisy) dataset&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Model Uncertainty Usage&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Samples highest-uncertainty examples for human labeling&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Detects label inconsistency via confidence estimation&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Human Role&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Label new examples&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Review and correct suspicious labels&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;New labels added to dataset&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;List of potential label errors to fix&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Typical Workflow&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Train model ‚Üí Select uncertain points ‚Üí Human annotates ‚Üí Expand dataset&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Train model ‚Üí Identify inconsistent labels ‚Üí Human verifies/corrects ‚Üí Clean dataset&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Common Technique&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Uncertainty sampling&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Confidence-based error detection&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Libraries/Tools&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://modal-python.readthedocs.io/en/latest/&#34;&gt;modAL&lt;/a&gt;, &lt;a href=&#34;https://alipy.readthedocs.io/en/latest/&#34;&gt;ALiPy&lt;/a&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;a href=&#34;https://github.com/cleanlab/cleanlab&#34;&gt;cleanlab&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Philosophy&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Proactively grow data wisely&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Reactively audit and clean existing data&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Typical Question&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;em&gt;&amp;ldquo;What should I label next?&amp;rdquo;&lt;/em&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;em&gt;&amp;ldquo;Which labels are probably wrong?&amp;rdquo;&lt;/em&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;End Goal&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Smarter, faster data acquisition&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Higher quality existing labels&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Example Scenario&lt;/strong&gt;&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Medical image AI needing efficient expert labeling&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Noisy crowd-sourced labeled text needing cleaning&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Key Distillation&lt;/p&gt;</description>
    </item>
    <item>
      <title>Interpretability in Data-Centric ML</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/interpretability-data-centric-ml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/interpretability-data-centric-ml/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Interpretability in Data-Centric ML &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-why-do-we-need-interpretable-machine-learning&#34;&gt;&#xA;  Q1: Why do we need interpretable machine learning?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-why-do-we-need-interpretable-machine-learning&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;For debugging and validation of models.&lt;/li&gt;&#xA;&lt;li&gt;To allow human review and oversight of decisions.&lt;/li&gt;&#xA;&lt;li&gt;To improve usability by aligning models with human intuition, past experience, and values.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-when-is-interpretability-particularly-important&#34;&gt;&#xA;  Q2: When is interpretability particularly important?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-when-is-interpretability-particularly-important&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;When the problem formulation is incomplete.&lt;/li&gt;&#xA;&lt;li&gt;When the model&amp;rsquo;s predictions have associated risks.&lt;/li&gt;&#xA;&lt;li&gt;When humans are involved in the decision-making loop.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-what-are-interpretable-features&#34;&gt;&#xA;  Q3: What are interpretable features?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-what-are-interpretable-features&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Features that are most useful, understandable, and meaningful to the user.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-how-can-interpretable-features-help-performance&#34;&gt;&#xA;  Q4: How can interpretable features help performance?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-how-can-interpretable-features-help-performance&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Lead to more efficient training.&lt;/li&gt;&#xA;&lt;li&gt;Improve model generalization.&lt;/li&gt;&#xA;&lt;li&gt;Reduce vulnerability to adversarial examples.&lt;/li&gt;&#xA;&lt;li&gt;The perceived interpretability-performance tradeoff is mostly a myth.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-what-qualities-make-features-interpretable&#34;&gt;&#xA;  Q5: What qualities make features interpretable?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-what-qualities-make-features-interpretable&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Readability&lt;/li&gt;&#xA;&lt;li&gt;Understandability&lt;/li&gt;&#xA;&lt;li&gt;Relevance&lt;/li&gt;&#xA;&lt;li&gt;Abstraction when necessary&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-how-do-we-get-interpretable-features&#34;&gt;&#xA;  Q6: How do we get interpretable features?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-how-do-we-get-interpretable-features&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Involving users directly in the feature design process.&lt;/li&gt;&#xA;&lt;li&gt;Using interpretable feature transformations.&lt;/li&gt;&#xA;&lt;li&gt;Generating new interpretable features through crowd-sourcing and algorithms.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-what-are-examples-of-methods-for-interpretable-feature-creation&#34;&gt;&#xA;  Q7: What are examples of methods for interpretable feature creation?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-what-are-examples-of-methods-for-interpretable-feature-creation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Collaborative feature engineering with domain experts.&lt;/li&gt;&#xA;&lt;li&gt;Flock: clustering crowd-generated feature descriptions.&lt;/li&gt;&#xA;&lt;li&gt;Ballet: allowing feature engineering with simple feedback loops.&lt;/li&gt;&#xA;&lt;li&gt;Pyreal: structured feature transformations for explanations.&lt;/li&gt;&#xA;&lt;li&gt;Mind the Gap Model (MGM): groups features using AND/OR logical structures.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-what-was-observed-in-the-child-welfare-case-study&#34;&gt;&#xA;  Q8: What was observed in the Child Welfare case study?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-what-was-observed-in-the-child-welfare-case-study&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Confusing or irrelevant features can hinder usability and trust.&lt;/li&gt;&#xA;&lt;li&gt;Clear, meaningful features helped screeners better interpret model recommendations.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-what-is-the-role-of-explanation-algorithms-in-interpretability&#34;&gt;&#xA;  Q9: What is the role of explanation algorithms in interpretability?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-what-is-the-role-of-explanation-algorithms-in-interpretability&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;They help diagnose flawed features or data by revealing what the model actually uses.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q10-what-are-the-final-conclusions-about-interpretable-features&#34;&gt;&#xA;  Q10: What are the final conclusions about interpretable features?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-what-are-the-final-conclusions-about-interpretable-features&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ML models are only as interpretable as their features.&lt;/li&gt;&#xA;&lt;li&gt;Interpretable features are central for transparent, human-centered ML.&lt;/li&gt;&#xA;&lt;li&gt;Effective feature engineering must involve human collaboration, thoughtful transformations, and systematic generation methods.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;references&#34;&gt;&#xA;  References&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#references&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://doi.org/10.1109/TVCG.2021.3114864&#34;&gt;Sibyl: Challenges of ML in Child Welfare (Zytek et al., 2021)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/s42256-019-0048-x&#34;&gt;Stop Explaining Black-Box Models (Rudin, 2019)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.02175&#34;&gt;Adversarial Examples Are Not Bugs (Ilyas et al., 2019)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/2675133.2675242&#34;&gt;Flock: Crowd-Machine Learning Classifiers (Cheng &amp;amp; Bernstein, 2015)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/10.1145/3479503&#34;&gt;Ballet: Collaborative Feature Engineering (Smith et al., 2021)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://dtail.gitbook.io/pyreal/&#34;&gt;Pyreal Project&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper_files/paper/2015/hash/89d217d180f6b3b732459e4fe6b63c99-Abstract.html&#34;&gt;Mind the Gap Model (Kim et al., 2015)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Encoding Human Priors</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/encoding-human-priors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/encoding-human-priors/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Encoding Human Priors ‚Äì Data Augmentation and Prompt Engineering &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-what-is-the-main-focus-of-this-lecture&#34;&gt;&#xA;  Q1: What is the main focus of this lecture?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-what-is-the-main-focus-of-this-lecture&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How to &lt;strong&gt;encode human priors&lt;/strong&gt; into machine learning through:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Training data augmentation&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Prompt engineering at test time&lt;/strong&gt; (especially for LLMs).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-why-do-ml-models-need-human-priors&#34;&gt;&#xA;  Q2: Why do ML models need human priors?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-why-do-ml-models-need-human-priors&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ML models often &lt;strong&gt;fail in simple ways&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;They lack &lt;strong&gt;common sense&lt;/strong&gt; (e.g., failing to recognize a rotated dog image).&lt;/li&gt;&#xA;&lt;li&gt;Human priors &lt;strong&gt;capture invariances&lt;/strong&gt; and domain knowledge that models don&amp;rsquo;t inherently learn.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-what-is-data-augmentation-and-why-is-it-important&#34;&gt;&#xA;  Q3: What is data augmentation and why is it important?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-what-is-data-augmentation-and-why-is-it-important&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data augmentation&lt;/strong&gt; creates new training examples by applying transformations (e.g., rotation, flipping).&lt;/li&gt;&#xA;&lt;li&gt;Helps address:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt; (memorization)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Underfitting&lt;/strong&gt; (lack of data)&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Class imbalance&lt;/strong&gt; or &lt;strong&gt;biased datasets&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Saves time and cost, especially when labeled data is expensive (e.g., in healthcare).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-what-are-examples-of-data-augmentation-techniques&#34;&gt;&#xA;  Q4: What are examples of data augmentation techniques?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-what-are-examples-of-data-augmentation-techniques&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Simple methods&lt;/strong&gt;: Rotation, flipping.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Advanced methods&lt;/strong&gt;:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Mixup&lt;/strong&gt;: Blending images and labels (e.g., 60% cat + 40% dog).&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Synthetic generation&lt;/strong&gt;: Using DALL-E, Stable Diffusion to generate new data.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Simulation-to-real transfer&lt;/strong&gt;: e.g., Google&amp;rsquo;s RetinaGAN for robotics.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Text augmentation&lt;/strong&gt;: &lt;strong&gt;Back-translation&lt;/strong&gt; (English ‚Üí French ‚Üí English) to generate paraphrases.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-what-is-prompt-engineering&#34;&gt;&#xA;  Q5: What is prompt engineering?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-what-is-prompt-engineering&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Prompt engineering&lt;/strong&gt; manipulates inputs to LLMs &lt;strong&gt;at test time&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Example: Instead of ‚ÄúWrite a letter of recommendation,‚Äù prompt with ‚ÄúWrite a letter for a student who got into MIT‚Äù to get higher-quality output.&lt;/li&gt;&#xA;&lt;li&gt;Leverages the &lt;strong&gt;language interface&lt;/strong&gt; humans naturally use.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-why-does-prompt-engineering-work-especially-well-for-llms&#34;&gt;&#xA;  Q6: Why does prompt engineering work especially well for LLMs?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-why-does-prompt-engineering-work-especially-well-for-llms&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LLMs are trained on massive &lt;strong&gt;language datasets&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Humans can easily &lt;strong&gt;adapt prompts&lt;/strong&gt; to guide the model without retraining it.&lt;/li&gt;&#xA;&lt;li&gt;Providing &lt;strong&gt;context and examples&lt;/strong&gt; (&amp;ldquo;few-shot prompting&amp;rdquo;) improves results.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-how-are-gpt-3-and-chatgpt-different-in-handling-prompts&#34;&gt;&#xA;  Q7: How are GPT-3 and ChatGPT different in handling prompts?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-how-are-gpt-3-and-chatgpt-different-in-handling-prompts&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;GPT-3&lt;/strong&gt;: Predicts next token, assumes user might be creating forms/questions.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ChatGPT&lt;/strong&gt;: Trained for &lt;strong&gt;dialogue and commands&lt;/strong&gt;, better at &lt;strong&gt;instruction-following&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-what-are-best-practices-in-prompt-engineering&#34;&gt;&#xA;  Q8: What are best practices in prompt engineering?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-what-are-best-practices-in-prompt-engineering&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Add &lt;strong&gt;examples&lt;/strong&gt; (&amp;ldquo;few-shot&amp;rdquo;) to define task behavior.&lt;/li&gt;&#xA;&lt;li&gt;Build &lt;strong&gt;context templates&lt;/strong&gt; for reusability.&lt;/li&gt;&#xA;&lt;li&gt;Iteratively &lt;strong&gt;tweak prompts&lt;/strong&gt; to observe effects on output.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-how-does-data-augmentation-vs-prompt-engineering-differ&#34;&gt;&#xA;  Q9: How does data augmentation vs. prompt engineering differ?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-how-does-data-augmentation-vs-prompt-engineering-differ&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Aspect&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Data Augmentation&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Prompt Engineering&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;When applied&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Before training&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;At test time&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;What is changed&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Training dataset&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Input prompt&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Goal&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Teach model invariances&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Guide model behavior dynamically&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Typical models&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Any ML models&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Mainly LLMs (e.g., GPT family)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;q10-final-takeaway&#34;&gt;&#xA;  Q10: Final Takeaway&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-final-takeaway&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Encoding human priors via data&lt;/strong&gt; (training-time or test-time) dramatically improves model robustness.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Data is the bridge&lt;/strong&gt; to insert human knowledge into ML systems effectively.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;references&#34;&gt;&#xA;  References&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#references&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://dcai.csail.mit.edu/2023/human-priors/human-priors.pdf&#34;&gt;Lecture Slides PDF&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1710.09412&#34;&gt;Mixup Paper (Zhang et al., 2017)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2002.02917&#34;&gt;Mobius Transformations (Zhou et al., 2020)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2011.03148&#34;&gt;RetinaGAN (Ho et al., 2020)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;GPT-3 Paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://openai.com/index/dall-e-2/&#34;&gt;DALL-E&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://stability.ai/blog/stable-diffusion-public-release&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/dcai-course/dcai-lab/blob/master/prompt_engineering/Lab_Prompt_Engineering.ipynb&#34;&gt;Lab assignment: Prompt Engineering&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Data Privacy and Security in ML</title>
      <link>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/data-privacy-security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/data-modeling/data-centric-ai/data-privacy-security/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://imipark.github.io/images/AIR_logo.png&#34; alt=&#34;AI Reasoning Logo&#34; width=&#34;200&#34;/&gt;&#xA;&lt;strong style=&#34;font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;&#34;&gt;&#xA;Data Privacy and Security in ML &#xA;&lt;/strong&gt;&#xA;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;q1-why-is-data-privacy-and-security-important-in-ml-models&#34;&gt;&#xA;  Q1: Why is data privacy and security important in ML models?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q1-why-is-data-privacy-and-security-important-in-ml-models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ML models often leak information about their training data.&lt;/li&gt;&#xA;&lt;li&gt;Public models can reveal sensitive data either directly or through inference attacks.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q2-what-types-of-attacks-can-compromise-ml-models&#34;&gt;&#xA;  Q2: What types of attacks can compromise ML models?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q2-what-types-of-attacks-can-compromise-ml-models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Membership inference attacks: Determine if a datapoint was part of the training set.&lt;/li&gt;&#xA;&lt;li&gt;Data extraction attacks: Extract parts of the training data from the model.&lt;/li&gt;&#xA;&lt;li&gt;Other attacks include adversarial examples, data poisoning, model inversion, model extraction, and prompt injection.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q3-what-are-security-goals-and-threat-models&#34;&gt;&#xA;  Q3: What are security goals and threat models?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q3-what-are-security-goals-and-threat-models&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A security goal defines what must or must not happen.&lt;/li&gt;&#xA;&lt;li&gt;A threat model defines the adversary‚Äôs capabilities and limitations.&lt;/li&gt;&#xA;&lt;li&gt;Both are necessary to properly reason about a system‚Äôs security.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q4-how-does-threat-modeling-apply-to-ml-apis&#34;&gt;&#xA;  Q4: How does threat modeling apply to ML APIs?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q4-how-does-threat-modeling-apply-to-ml-apis&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Example: Google Vision API must prevent model extraction even when adversaries can query with arbitrary images.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q5-what-is-a-membership-inference-attack&#34;&gt;&#xA;  Q5: What is a membership inference attack?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q5-what-is-a-membership-inference-attack&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;An attack that identifies whether a specific data point was in the model‚Äôs training set.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q6-how-does-shadow-training-help-in-membership-inference&#34;&gt;&#xA;  Q6: How does shadow training help in membership inference?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q6-how-does-shadow-training-help-in-membership-inference&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Shadow models simulate the target model‚Äôs behavior on known datasets.&lt;/li&gt;&#xA;&lt;li&gt;An attack model is trained to classify whether a data point was part of the training set based on model outputs.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q7-what-are-simple-metric-based-membership-inference-attacks&#34;&gt;&#xA;  Q7: What are simple metric-based membership inference attacks?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q7-what-are-simple-metric-based-membership-inference-attacks&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Prediction correctness: whether model predicts correctly.&lt;/li&gt;&#xA;&lt;li&gt;Prediction loss: whether the model loss is low.&lt;/li&gt;&#xA;&lt;li&gt;Prediction confidence: model‚Äôs maximum output probability.&lt;/li&gt;&#xA;&lt;li&gt;Prediction entropy: uncertainty of model‚Äôs output distribution.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q8-what-is-a-data-extraction-attack&#34;&gt;&#xA;  Q8: What is a data extraction attack?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q8-what-is-a-data-extraction-attack&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Directly extracting memorized sequences or examples from a model, especially from large LLMs.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q9-how-is-perplexity-used-in-data-extraction&#34;&gt;&#xA;  Q9: How is perplexity used in data extraction?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q9-how-is-perplexity-used-in-data-extraction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Lower perplexity on sequences indicates that they were likely memorized during training.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q10-what-are-empirical-defenses-against-privacy-attacks&#34;&gt;&#xA;  Q10: What are empirical defenses against privacy attacks?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q10-what-are-empirical-defenses-against-privacy-attacks&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Limiting outputs (top-k predictions, quantization).&lt;/li&gt;&#xA;&lt;li&gt;Adding noise to predictions.&lt;/li&gt;&#xA;&lt;li&gt;Changing training methods (e.g., regularization).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q11-why-is-empirical-defense-evaluation-hard&#34;&gt;&#xA;  Q11: Why is empirical defense evaluation hard?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q11-why-is-empirical-defense-evaluation-hard&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Following Kerckhoffs‚Äôs principle, defenses must work even when attackers know the defense.&lt;/li&gt;&#xA;&lt;li&gt;Security is a cat-and-mouse game between defenders and attackers.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q12-what-is-differential-privacy-dp&#34;&gt;&#xA;  Q12: What is differential privacy (DP)?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q12-what-is-differential-privacy-dp&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A formal, mathematical definition of privacy that limits how much an algorithm‚Äôs output depends on any single input.&lt;/li&gt;&#xA;&lt;li&gt;Algorithms like DP-SGD make models less dependent on individual datapoints.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q13-what-challenges-exist-with-using-differential-privacy&#34;&gt;&#xA;  Q13: What challenges exist with using differential privacy?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q13-what-challenges-exist-with-using-differential-privacy&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It introduces parameters (Œµ, Œ¥) that are difficult to set.&lt;/li&gt;&#xA;&lt;li&gt;Strong privacy may come at the cost of degraded model performance.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q14-what-resources-were-recommended&#34;&gt;&#xA;  Q14: What resources were recommended?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q14-what-resources-were-recommended&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Surveys on membership inference attacks and privacy attacks.&lt;/li&gt;&#xA;&lt;li&gt;Awesome ML privacy attacks collection.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;q15-what-was-the-lab-assignment&#34;&gt;&#xA;  Q15: What was the lab assignment?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#q15-what-was-the-lab-assignment&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Implement a membership inference attack against a black-box model.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;hr&gt;&#xA;&lt;h3 id=&#34;heading&#34;&gt;&#xA;  &#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#heading&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34;&gt;OpenAI Codex paper&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://simonwillison.net/2022/Sep/12/prompt-injection/&#34;&gt;Prompt Injection&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.07853&#34;&gt;Membership Inference Survey&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2007.07646&#34;&gt;Privacy Attacks in ML Survey&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/stratosphereips/awesome-ml-privacy-attacks&#34;&gt;Awesome ML Privacy Attacks Repository&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://iacr.org/archive/tcc2006/38760266/38760266.pdf&#34;&gt;Differential Privacy (Dwork et al., 2006)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1607.00133&#34;&gt;DP-SGD (Abadi et al., 2016)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/dcai-course/dcai-lab/blob/master/membership_inference/Lab%20-%20Membership%20Inference.ipynb&#34;&gt;Membership Inference Lab Notebook&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
