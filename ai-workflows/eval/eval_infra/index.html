<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="


Eval Infra: Verifiable (STEM) vs Non-Verifiable vs Hybrid




  Why These 5 Layers? Separation of Concerns
  #

Each layer serves a distinct purpose in the AI evaluation lifecycle, from research to production deployment.

  
      
          Layer
          Purpose
          Key Question
          Stakeholder
          Output
      
  
  
      
          L1: Benchmark Design
          Define what &ldquo;good&rdquo; means
          What are we measuring?
          Research Scientists
          Test suite &#43; evaluation protocol
      
      
          L2: Evaluation Execution
          Actually measure performance
          How do we score it?
          ML Engineers
          Raw scores/labels per example
      
      
          L3: Scalability
          Handle volume &amp; iteration speed
          Can we do this 1000x?
          MLOps/Infrastructure
          Evaluation pipeline infrastructure
      
      
          L4: Metrics &amp; Reliability
          Trust the measurements
          Is this signal real?
          Data Scientists, Leadership
          Aggregate metrics &#43; confidence intervals
      
      
          L5: Production Monitoring
          Maintain quality in the wild
          Is it still working?
          SREs, Product Managers
          Live dashboards &#43; alerting systems
      
  


  How Layers Map to Natural Workflow
  #


RESEARCH PHASE (Offline Development)
â”‚
â”œâ”€ L1: Design Benchmarks
â”‚   â””â”€ &#34;What constitutes correct/good performance?&#34;
â”‚
â”œâ”€ L2: Run Evaluations  
â”‚   â””â”€ &#34;Generate responses and score them&#34;
â”‚
â”œâ”€ L3: Scale Infrastructure
â”‚   â””â”€ &#34;Need to iterate fast â†’ evaluate 10K examples/day&#34;
â”‚
â””â”€ L4: Analyze Results
    â””â”€ &#34;Aggregate metrics, validate reliability&#34;

DEPLOYMENT PHASE (Online Production)
â”‚
â””â”€ L5: Monitor Production
    â””â”€ &#34;Continuous validation, catch regressions&#34;
    â””â”€ Feed failures back to L1 (closed loop)


  Different Stakeholders Own Each Layern
  #


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1: Research Scientists                             â”‚
â”‚     â†’ Design evaluation protocols                   â”‚
â”‚     â†’ Define what &#34;good&#34; means for the domain       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L2: ML Engineers                                    â”‚
â”‚     â†’ Implement evaluation scripts                  â”‚
â”‚     â†’ Run model inference &#43; scoring                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L3: MLOps / Infrastructure Engineers                â”‚
â”‚     â†’ Build scalable eval pipelines                 â”‚
â”‚     â†’ Manage compute resources                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L4: Data Scientists / Research Leadership           â”‚
â”‚     â†’ Statistical analysis of eval results          â”‚
â”‚     â†’ Validate metric reliability                   â”‚
â”‚     â†’ Make deployment decisions                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L5: SREs / Product Managers                         â”‚
â”‚     â†’ Monitor production performance                â”‚
â”‚     â†’ Alert on regressions                          â”‚
â”‚     â†’ Coordinate incident response                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


  The Spectrum of Verifiability
  #


Fully Verifiable â†â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â†’ Non-Verifiable
      â”‚                    â”‚                      â”‚
   Code/Math          Technical Writing      Creative/Social
   (external          (hybrid: facts &#43;       (judgment only)
    oracle)            style/clarity)


  Examples by Category
  #


  
      
          Fully Verifiable
          Hybrid (Partially Verifiable)
          Non-Verifiable
      
  
  
      
          â€¢ Code execution
          â€¢ Technical writing (facts âœ“, clarity âœ—)
          â€¢ Creative writing
      
      
          â€¢ Math computation
          â€¢ Translation (accuracy âœ“, style âœ—)
          â€¢ Persuasive marketing
      
      
          â€¢ Logic proofs
          â€¢ Legal analysis (precedent âœ“, judgment âœ—)
          â€¢ Empathetic therapy
      
      
          â€¢ Data extraction
          â€¢ Medical diagnosis (tests âœ“, manner âœ—)
          â€¢ Humor generation
      
      
          â€¢ Fact checking
          â€¢ Recipe generation (chemistry âœ“, taste âœ—)
          â€¢ Art critique
      
      
          â€¢ Physics simulation
          â€¢ Code review (bugs âœ“, readability âœ—)
          â€¢ Storytelling
      
  


  LAYER 1: BENCHMARK DESIGN
  #


  
      
          Verifiable (Code/Math/Logic)
          Hybrid (Technical/Professional)
          Non-Verifiable (Creative/Social)
      
  
  
      
          Structure: Problem &#43; Test Suite â†’ Automated Oracle
          Structure: Task &#43; Mixed Criteria â†’ Automated &#43; Human
          Structure: Prompt &#43; Rubric â†’ Human/AI Judgment
      
      
          Key Benchmarks:â€¢ HumanEval (164 code problems)â€¢ MATH (12K competition problems)â€¢ GPQA (448 PhD questions)
          Key Benchmarks:â€¢ Technical Writing Qualityâ€¢ Translation (BLEU &#43; human eval)â€¢ Medical Q&amp;A (facts &#43; empathy)
          Key Benchmarks:â€¢ MT-Bench (80 conversations)â€¢ AlpacaEval (805 prompts)â€¢ Chatbot Arena (live voting)
      
      
          Properties:âœ… Tests are deterministicâœ… Can generate infinite variantsâœ… No human disagreement
          Properties:âš ï¸ Some aspects objectiveâš ï¸ Some aspects subjectiveâš ï¸ Requires dual evaluation
          Properties:âŒ Ratings are subjectiveâŒ Context-dependent qualityâŒ Humans disagree frequently
      
      
          Creation Time: 1-2 weeks
          Creation Time: 2-3 weeks
          Creation Time: 3-4 weeks
      
      
          Example:assert reverse([1,2,3]) == [3,2,1]
          Example:Accuracy: Does translation preserve meaning? (âœ“)Fluency: Does it sound natural? (human)
          Example:&ldquo;Is this story engaging?&quot;â†’ 5-point Likert scale (human)
      
  


  LAYER 2: EVALUATION EXECUTION
  #


  
      
          Verifiable
          Hybrid
          Non-Verifiable
      
  
  
      
          Pipeline:1. Generate solutions (10 min)2. Execute &amp; verify (5 min)3. Compute metrics (&lt;1 min)
          Pipeline:1. Generate outputs (10 min)2. Automated checks (5 min)3. Human evaluation (4-20 hours)4. Combine scores (30 min)
          Pipeline:1. Generate responses (5 min)2. Human/AI rating (8-40 hours)3. Aggregate &amp; validate (1 hour)
      
      
          Verification:python result = execute_code(solution)&lt;br&gt;label = PASS if tests_pass else FAIL&lt;br&gt;
          Verification:python # Automated component&lt;br&gt;facts_correct = verify_facts(output)&lt;br&gt;# Human component&lt;br&gt;clarity = human_rate_clarity(output)&lt;br&gt;score = 0.5*facts &#43; 0.5*clarity&lt;br&gt;
          Verification:python ratings = get_human_ratings(n=3)&lt;br&gt;score = mean(ratings)&lt;br&gt;# Then validate inter-rater agreement&lt;br&gt;
      
      
          Throughput: 10K-100K evals/hour
          Throughput: 500-5K evals/hour
          Throughput: 100-1K evals/hour
      
      
          Cost per eval: $0.001-0.01
          Cost per eval: $0.05-1.00
          Cost per eval: $0.10-5.00
      
      
          Human time: 0 hours
          Human time: 4-20 hours (partial)
          Human time: 24-120 hours (full)
      
  


  LAYER 3: SCALABILITY
  #


  
      
          Verifiable
          Hybrid
          Non-Verifiable
      
  
  
      
          Bottleneck: Compute (GPU time)
          Bottleneck: Human time for subjective parts
          Bottleneck: Human bandwidth
      
      
          Scaling Examples:â€¢ 164 problems â†’ $2, 15 minâ€¢ 10K problems â†’ $122, 2 hoursâ€¢ 100K problems â†’ $1,220, 8 hours
          Scaling Examples:â€¢ 100 docs â†’ $50, 4 hoursâ€¢ 1K docs â†’ $500, 20 hoursâ€¢ 10K docs â†’ $5K, 200 hours(Mix of automated &#43; human)
          Scaling Examples:â€¢ 80 prompts â†’ $400, 8-40 hoursâ€¢ 10K prompts â†’ $37,500, 2,500 hoursâ€¢ (Or $8K hybrid with AI judges)
      
      
          Human scaling: 0 hours regardless of scale
          Human scaling: Sub-linear (automate what&rsquo;s possible)
          Human scaling: Linear or super-linear
      
      
          Constraint: Money (buy more GPUs)
          Constraint: Time &#43; money (human for quality checks)
          Constraint: Time (recruit, train raters)
      
      
          Automation: 99.9%
          Automation: 40-70% (depends on domain)
          Automation: 5-30% (AI judges need validation)
      
      
          Scalability Strategy:â€¢ Parallelize across GPUsâ€¢ Generate synthetic test casesâ€¢ Cost scales linearly
          Scalability Strategy:â€¢ Automate objective criteriaâ€¢ Sample human evaluation (10-20%)â€¢ Use AI judges for subjective parts (with validation)
          Scalability Strategy:â€¢ Train reward models on human labelsâ€¢ Use LLM-as-judge (must validate)â€¢ Spot-check 5-10% with humans
      
  


  LAYER 4: METRICS &amp; RELIABILITY
  #


  
      
          Verifiable
          Hybrid
          Non-Verifiable
      
  
  
      
          Metrics:â€¢ pass@k (% solved in k tries)â€¢ Compile rateâ€¢ Exact match accuracyâ€¢ Error tolerance
          Metrics:â€¢ Factual accuracy (automated)â€¢ Readability score (formula)â€¢ Clarity rating (human)â€¢ Combined weighted score
          Metrics:â€¢ Likert scale (1-5 ratings)â€¢ Win rate vs baselineâ€¢ Elo ratings (head-to-head)â€¢ Thumbs up/down ratio
      
      
          Properties:âœ… Objective &amp; reproducibleâœ… Labs can compare directlyâœ… No gaming (oracle is external)âœ… Leaderboards meaningful
          Properties:âš ï¸ Partially objectiveâš ï¸ Requires careful weightingâš ï¸ Some gaming risk on subjective partsâš ï¸ Need to report both auto &#43; human metrics
          Properties:âŒ Subjective &amp; noisyâŒ Different protocols â†’ incomparableâŒ Gaming risk (optimize for judge)âŒ Leaderboards have selection bias
      
      
          Inter-evaluator agreement: 100%
          Inter-evaluator agreement:Facts: 95-100%Quality: 70-85%
          Inter-evaluator agreement: 60-80%
      
      
          Example Metrics:â€¢ HumanEval pass@1: 67.8%â€¢ MATH accuracy: 82.3%â€¢ Error rate: 5.2%
          Example Metrics:â€¢ Translation BLEU: 45.2 (auto)â€¢ Fluency: 4.1/5 (human)â€¢ Medical accuracy: 94% (auto), Empathy: 3.8/5 (human)
          Example Metrics:â€¢ MT-Bench: 7.9/10 (GPT-4 judge)â€¢ Human preference: 78% win rateâ€¢ Elo rating: 1,245
      
  


  LAYER 5: PRODUCTION MONITORING
  #


  
      
          Verifiable
          Hybrid
          Non-Verifiable
      
  
  
      
          Real-time signals:â€¢ Does code compile? âœ“/âœ—â€¢ Tests pass? âœ“/âœ—â€¢ User accepted? âœ“/âœ—â€¢ Execution time OK? âœ“/âœ—
          Real-time signals:â€¢ Facts verified? âœ“/âœ—â€¢ Format correct? âœ“/âœ—â€¢ User satisfaction proxy (usage time)â€¢ Error rate
          Proxy signals:â€¢ Thumbs up/down ratioâ€¢ Session lengthâ€¢ Regeneration rateâ€¢ Response length
      
      
          Monitoring:â€¢ Every request â†’ Automated checkâ€¢ Dashboard updates: Real-timeâ€¢ Regression alerts: Instant
          Monitoring:â€¢ Automated checks: Real-timeâ€¢ Human spot-checks: Weekly (10% sample)â€¢ Combined quality score trending
          Monitoring:â€¢ Sample 100 conversations/weekâ€¢ 3 humans rate eachâ€¢ Compare to last month
      
      
          Action:â€¢ Compile rate &lt;90% â†’ Auto rollbackâ€¢ Pass@1 drops &gt;5% â†’ Alert engineer
          Action:â€¢ Fact accuracy &lt;95% â†’ Auto rollbackâ€¢ Quality score drops &gt;0.3 â†’ Investigateâ€¢ Run deeper human eval if needed
          Action:â€¢ Quality drops &gt;0.3 â†’ Investigateâ€¢ A/B test for 7 daysâ€¢ Need human eval to decide
      
      
          Human role: Only when alerts fire
          Human role: Weekly spot-checks (10%)
          Human role: Continuous (weekly audits)
      
      
          Dashboard Example:Compile Rate: 94.2% ğŸŸ¢Pass@1: 67.8% ğŸŸ¢Latency: 1.2s ğŸŸ¡
          Dashboard Example:Fact Check: 96.1% ğŸŸ¢User Rating: 4.2/5 ğŸŸ¢Clarity (sampled): 3.9/5 ğŸŸ¡
          Dashboard Example:Human Rating: 4.2/5 ğŸ”´Thumbs Up: 78% ğŸŸ¢Session Time: 8.2min ğŸŸ¢
      
  


  EACH LAYER HAS DIFFERENT BOTTLENECK
  #


  
      
          Layer
          Verifiable Bottleneck
          Hybrid Bottleneck
          Non-Verifiable Bottleneck
      
  
  
      
          L1: Design
          Writing test suite
          Defining which parts are verifiable
          Getting human agreement on rubric
      
      
          L2: Execute
          GPU inference time
          Human time for quality checks
          Human annotation time
      
      
          L3: Scale
          Compute budget
          Hiring raters for quality
          Hiring/training many raters
      
      
          L4: Metrics
          Statistical analysis
          Balancing auto vs human metrics
          Inter-rater reliability
      
      
          L5: Monitor
          Infrastructure cost
          Continuous spot-checking
          Continuous human auditing
      
  



  REAL-WORLD EXAMPLES BY CATEGORY
  #



Verifiable:">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="https://imipark.github.io/ai-workflows/eval/eval_infra/">
  <meta property="og:site_name" content="AI Reasoning">
  <meta property="og:title" content="Eval Infra: Verifiable vs Non-Verifiable vs Hybrid">
  <meta property="og:description" content="Eval Infra: Verifiable (STEM) vs Non-Verifiable vs Hybrid Why These 5 Layers? Separation of Concerns # Each layer serves a distinct purpose in the AI evaluation lifecycle, from research to production deployment.
Layer Purpose Key Question Stakeholder Output L1: Benchmark Design Define what â€œgoodâ€ means What are we measuring? Research Scientists Test suite &#43; evaluation protocol L2: Evaluation Execution Actually measure performance How do we score it? ML Engineers Raw scores/labels per example L3: Scalability Handle volume &amp; iteration speed Can we do this 1000x? MLOps/Infrastructure Evaluation pipeline infrastructure L4: Metrics &amp; Reliability Trust the measurements Is this signal real? Data Scientists, Leadership Aggregate metrics &#43; confidence intervals L5: Production Monitoring Maintain quality in the wild Is it still working? SREs, Product Managers Live dashboards &#43; alerting systems How Layers Map to Natural Workflow # RESEARCH PHASE (Offline Development) â”‚ â”œâ”€ L1: Design Benchmarks â”‚ â””â”€ &#34;What constitutes correct/good performance?&#34; â”‚ â”œâ”€ L2: Run Evaluations â”‚ â””â”€ &#34;Generate responses and score them&#34; â”‚ â”œâ”€ L3: Scale Infrastructure â”‚ â””â”€ &#34;Need to iterate fast â†’ evaluate 10K examples/day&#34; â”‚ â””â”€ L4: Analyze Results â””â”€ &#34;Aggregate metrics, validate reliability&#34; DEPLOYMENT PHASE (Online Production) â”‚ â””â”€ L5: Monitor Production â””â”€ &#34;Continuous validation, catch regressions&#34; â””â”€ Feed failures back to L1 (closed loop) Different Stakeholders Own Each Layern # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ L1: Research Scientists â”‚ â”‚ â†’ Design evaluation protocols â”‚ â”‚ â†’ Define what &#34;good&#34; means for the domain â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ L2: ML Engineers â”‚ â”‚ â†’ Implement evaluation scripts â”‚ â”‚ â†’ Run model inference &#43; scoring â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ L3: MLOps / Infrastructure Engineers â”‚ â”‚ â†’ Build scalable eval pipelines â”‚ â”‚ â†’ Manage compute resources â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ L4: Data Scientists / Research Leadership â”‚ â”‚ â†’ Statistical analysis of eval results â”‚ â”‚ â†’ Validate metric reliability â”‚ â”‚ â†’ Make deployment decisions â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ L5: SREs / Product Managers â”‚ â”‚ â†’ Monitor production performance â”‚ â”‚ â†’ Alert on regressions â”‚ â”‚ â†’ Coordinate incident response â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ The Spectrum of Verifiability # Fully Verifiable â†â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â†’ Non-Verifiable â”‚ â”‚ â”‚ Code/Math Technical Writing Creative/Social (external (hybrid: facts &#43; (judgment only) oracle) style/clarity) Examples by Category # Fully Verifiable Hybrid (Partially Verifiable) Non-Verifiable â€¢ Code execution â€¢ Technical writing (facts âœ“, clarity âœ—) â€¢ Creative writing â€¢ Math computation â€¢ Translation (accuracy âœ“, style âœ—) â€¢ Persuasive marketing â€¢ Logic proofs â€¢ Legal analysis (precedent âœ“, judgment âœ—) â€¢ Empathetic therapy â€¢ Data extraction â€¢ Medical diagnosis (tests âœ“, manner âœ—) â€¢ Humor generation â€¢ Fact checking â€¢ Recipe generation (chemistry âœ“, taste âœ—) â€¢ Art critique â€¢ Physics simulation â€¢ Code review (bugs âœ“, readability âœ—) â€¢ Storytelling LAYER 1: BENCHMARK DESIGN # Verifiable (Code/Math/Logic) Hybrid (Technical/Professional) Non-Verifiable (Creative/Social) Structure: Problem &#43; Test Suite â†’ Automated Oracle Structure: Task &#43; Mixed Criteria â†’ Automated &#43; Human Structure: Prompt &#43; Rubric â†’ Human/AI Judgment Key Benchmarks:
â€¢ HumanEval (164 code problems)
â€¢ MATH (12K competition problems)
â€¢ GPQA (448 PhD questions) Key Benchmarks:
â€¢ Technical Writing Quality
â€¢ Translation (BLEU &#43; human eval)
â€¢ Medical Q&amp;A (facts &#43; empathy) Key Benchmarks:
â€¢ MT-Bench (80 conversations)
â€¢ AlpacaEval (805 prompts)
â€¢ Chatbot Arena (live voting) Properties:
âœ… Tests are deterministic
âœ… Can generate infinite variants
âœ… No human disagreement Properties:
âš ï¸ Some aspects objective
âš ï¸ Some aspects subjective
âš ï¸ Requires dual evaluation Properties:
âŒ Ratings are subjective
âŒ Context-dependent quality
âŒ Humans disagree frequently Creation Time: 1-2 weeks Creation Time: 2-3 weeks Creation Time: 3-4 weeks Example:
assert reverse([1,2,3]) == [3,2,1] Example:
Accuracy: Does translation preserve meaning? (âœ“)
Fluency: Does it sound natural? (human) Example:
â€œIs this story engaging?&#34;
â†’ 5-point Likert scale (human) LAYER 2: EVALUATION EXECUTION # Verifiable Hybrid Non-Verifiable Pipeline:
1. Generate solutions (10 min)
2. Execute &amp; verify (5 min)
3. Compute metrics (&lt;1 min) Pipeline:
1. Generate outputs (10 min)
2. Automated checks (5 min)
3. Human evaluation (4-20 hours)
4. Combine scores (30 min) Pipeline:
1. Generate responses (5 min)
2. Human/AI rating (8-40 hours)
3. Aggregate &amp; validate (1 hour) Verification:
python result = execute_code(solution)&lt;br&gt;label = PASS if tests_pass else FAIL&lt;br&gt; Verification:
python # Automated component&lt;br&gt;facts_correct = verify_facts(output)&lt;br&gt;# Human component&lt;br&gt;clarity = human_rate_clarity(output)&lt;br&gt;score = 0.5*facts &#43; 0.5*clarity&lt;br&gt; Verification:
python ratings = get_human_ratings(n=3)&lt;br&gt;score = mean(ratings)&lt;br&gt;# Then validate inter-rater agreement&lt;br&gt; Throughput: 10K-100K evals/hour Throughput: 500-5K evals/hour Throughput: 100-1K evals/hour Cost per eval: $0.001-0.01 Cost per eval: $0.05-1.00 Cost per eval: $0.10-5.00 Human time: 0 hours Human time: 4-20 hours (partial) Human time: 24-120 hours (full) LAYER 3: SCALABILITY # Verifiable Hybrid Non-Verifiable Bottleneck: Compute (GPU time) Bottleneck: Human time for subjective parts Bottleneck: Human bandwidth Scaling Examples:
â€¢ 164 problems â†’ $2, 15 min
â€¢ 10K problems â†’ $122, 2 hours
â€¢ 100K problems â†’ $1,220, 8 hours Scaling Examples:
â€¢ 100 docs â†’ $50, 4 hours
â€¢ 1K docs â†’ $500, 20 hours
â€¢ 10K docs â†’ $5K, 200 hours
(Mix of automated &#43; human) Scaling Examples:
â€¢ 80 prompts â†’ $400, 8-40 hours
â€¢ 10K prompts â†’ $37,500, 2,500 hours
â€¢ (Or $8K hybrid with AI judges) Human scaling: 0 hours regardless of scale Human scaling: Sub-linear (automate whatâ€™s possible) Human scaling: Linear or super-linear Constraint: Money (buy more GPUs) Constraint: Time &#43; money (human for quality checks) Constraint: Time (recruit, train raters) Automation: 99.9% Automation: 40-70% (depends on domain) Automation: 5-30% (AI judges need validation) Scalability Strategy:
â€¢ Parallelize across GPUs
â€¢ Generate synthetic test cases
â€¢ Cost scales linearly Scalability Strategy:
â€¢ Automate objective criteria
â€¢ Sample human evaluation (10-20%)
â€¢ Use AI judges for subjective parts (with validation) Scalability Strategy:
â€¢ Train reward models on human labels
â€¢ Use LLM-as-judge (must validate)
â€¢ Spot-check 5-10% with humans LAYER 4: METRICS &amp; RELIABILITY # Verifiable Hybrid Non-Verifiable Metrics:
â€¢ pass@k (% solved in k tries)
â€¢ Compile rate
â€¢ Exact match accuracy
â€¢ Error tolerance Metrics:
â€¢ Factual accuracy (automated)
â€¢ Readability score (formula)
â€¢ Clarity rating (human)
â€¢ Combined weighted score Metrics:
â€¢ Likert scale (1-5 ratings)
â€¢ Win rate vs baseline
â€¢ Elo ratings (head-to-head)
â€¢ Thumbs up/down ratio Properties:
âœ… Objective &amp; reproducible
âœ… Labs can compare directly
âœ… No gaming (oracle is external)
âœ… Leaderboards meaningful Properties:
âš ï¸ Partially objective
âš ï¸ Requires careful weighting
âš ï¸ Some gaming risk on subjective parts
âš ï¸ Need to report both auto &#43; human metrics Properties:
âŒ Subjective &amp; noisy
âŒ Different protocols â†’ incomparable
âŒ Gaming risk (optimize for judge)
âŒ Leaderboards have selection bias Inter-evaluator agreement: 100% Inter-evaluator agreement:
Facts: 95-100%
Quality: 70-85% Inter-evaluator agreement: 60-80% Example Metrics:
â€¢ HumanEval pass@1: 67.8%
â€¢ MATH accuracy: 82.3%
â€¢ Error rate: 5.2% Example Metrics:
â€¢ Translation BLEU: 45.2 (auto)
â€¢ Fluency: 4.1/5 (human)
â€¢ Medical accuracy: 94% (auto), Empathy: 3.8/5 (human) Example Metrics:
â€¢ MT-Bench: 7.9/10 (GPT-4 judge)
â€¢ Human preference: 78% win rate
â€¢ Elo rating: 1,245 LAYER 5: PRODUCTION MONITORING # Verifiable Hybrid Non-Verifiable Real-time signals:
â€¢ Does code compile? âœ“/âœ—
â€¢ Tests pass? âœ“/âœ—
â€¢ User accepted? âœ“/âœ—
â€¢ Execution time OK? âœ“/âœ— Real-time signals:
â€¢ Facts verified? âœ“/âœ—
â€¢ Format correct? âœ“/âœ—
â€¢ User satisfaction proxy (usage time)
â€¢ Error rate Proxy signals:
â€¢ Thumbs up/down ratio
â€¢ Session length
â€¢ Regeneration rate
â€¢ Response length Monitoring:
â€¢ Every request â†’ Automated check
â€¢ Dashboard updates: Real-time
â€¢ Regression alerts: Instant Monitoring:
â€¢ Automated checks: Real-time
â€¢ Human spot-checks: Weekly (10% sample)
â€¢ Combined quality score trending Monitoring:
â€¢ Sample 100 conversations/week
â€¢ 3 humans rate each
â€¢ Compare to last month Action:
â€¢ Compile rate &lt;90% â†’ Auto rollback
â€¢ Pass@1 drops &gt;5% â†’ Alert engineer Action:
â€¢ Fact accuracy &lt;95% â†’ Auto rollback
â€¢ Quality score drops &gt;0.3 â†’ Investigate
â€¢ Run deeper human eval if needed Action:
â€¢ Quality drops &gt;0.3 â†’ Investigate
â€¢ A/B test for 7 days
â€¢ Need human eval to decide Human role: Only when alerts fire Human role: Weekly spot-checks (10%) Human role: Continuous (weekly audits) Dashboard Example:
Compile Rate: 94.2% ğŸŸ¢
Pass@1: 67.8% ğŸŸ¢
Latency: 1.2s ğŸŸ¡ Dashboard Example:
Fact Check: 96.1% ğŸŸ¢
User Rating: 4.2/5 ğŸŸ¢
Clarity (sampled): 3.9/5 ğŸŸ¡ Dashboard Example:
Human Rating: 4.2/5 ğŸ”´
Thumbs Up: 78% ğŸŸ¢
Session Time: 8.2min ğŸŸ¢ EACH LAYER HAS DIFFERENT BOTTLENECK # Layer Verifiable Bottleneck Hybrid Bottleneck Non-Verifiable Bottleneck L1: Design Writing test suite Defining which parts are verifiable Getting human agreement on rubric L2: Execute GPU inference time Human time for quality checks Human annotation time L3: Scale Compute budget Hiring raters for quality Hiring/training many raters L4: Metrics Statistical analysis Balancing auto vs human metrics Inter-rater reliability L5: Monitor Infrastructure cost Continuous spot-checking Continuous human auditing REAL-WORLD EXAMPLES BY CATEGORY # Verifiable:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="ai-workflows">
<title>Eval Infra: Verifiable vs Non-Verifiable vs Hybrid | AI Reasoning</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="https://imipark.github.io/ai-workflows/eval/eval_infra/">
<link rel="stylesheet" href="/book.min.6c8b9d2a1fc95075ed7da46ca81060b39add8fff6741ac51259f768929281e2c.css" integrity="sha256-bIudKh/JUHXtfaRsqBBgs5rdj/9nQaxRJZ92iSkoHiw=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/en.search.min.4014c58b4ff93b711a1e1d78f5ec4df7c78de5b479f64af51be6af1f4dace7eb.js" integrity="sha256-QBTFi0/5O3EaHh149exN98eN5bR59kr1G&#43;avH02s5&#43;s=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>AI Reasoning</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>













  



  
  <ul>
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ai-workflows/" class="">AI Reasoning Stack</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-f8e502c34e18a04e1c3aecf192b0355a" class="toggle"  />
    <label for="section-f8e502c34e18a04e1c3aecf192b0355a" class="flex justify-between">
      <a href="/ai-workflows/data-modeling/" class="">Data Modeling</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-906a2243f4790a09188fae70fbc32dbd" class="toggle"  />
    <label for="section-906a2243f4790a09188fae70fbc32dbd" class="flex justify-between">
      <a href="/ai-workflows/data-modeling/data-centric-ai/" class="">Data-Centric AI (DCAI)</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-5bf1c216f4b747bd7c2f992550ff09a1" class="toggle"  />
    <label for="section-5bf1c216f4b747bd7c2f992550ff09a1" class="flex justify-between">
      <a href="/ai-workflows/genai-systems/" class="">GenAI Systems</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-8cbfd420d91de13f61964bf971127312" class="toggle"  />
    <label for="section-8cbfd420d91de13f61964bf971127312" class="flex justify-between">
      <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/" class="">5-Day GenAI with Google 2005</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day1_foundational_llm_text_generation/" class="">Day 1 - Foundational LLMs &amp; Text Generation</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day1_prompt_engineering/" class="">Day 1 â€“ Prompt Engineering</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day2_embeddings_vectordb/" class="">Day 2 â€“ Embeddings &amp; Vector Databases</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day3_generative_agents/" class="">Day 3 â€“ Generative Agents</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day4_domainspecific_llms/" class="">Day 4 â€“ Domain-Specific LLMs</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/5-day-genai-google-2025/day5_mlops/" class="">Day 5 â€“ MLOps for Generative AI</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/genai-systems/multimodel_llms/" class="">Multimodal LLMs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-17ba62e37c896ad50105fedeb71549dd" class="toggle"  />
    <label for="section-17ba62e37c896ad50105fedeb71549dd" class="flex justify-between">
      <a href="/ai-workflows/reasoning/" class="">Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-cd94e161670d28ecff992edf840d76e8" class="toggle"  />
    <label for="section-cd94e161670d28ecff992edf840d76e8" class="flex justify-between">
      <a href="/ai-workflows/reasoning/causality/" class="">Causality</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-ai/" class="">Causal AI</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/causality/causal-inference/" class="">Causal Inference</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-7e468aa05ceb7c844f07a2e754606b76" class="toggle"  />
    <label for="section-7e468aa05ceb7c844f07a2e754606b76" class="flex justify-between">
      <a href="/ai-workflows/reasoning/graph-reasoning/" class="">Graph Reasoning</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/graphrag/" class="">GraphRAG</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/reasoning/graph-reasoning/knowledge-graphs/" class="">Knowledge Graphs</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="toggle"  />
    <label for="section-1b0623a0f68c821d1e7c5de8bd43d2fb" class="flex justify-between">
      <a href="/ai-workflows/rlhf/" class="">RLHF</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-b1afcafdefac57f3420f64e23d73f05d" class="toggle"  />
    <label for="section-b1afcafdefac57f3420f64e23d73f05d" class="flex justify-between">
      <a href="/ai-workflows/rlhf/rlhf2006/" class="">RLHF 2006</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/" class="">Instruct Gpt Codes Params</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-ca53d32fab0e1a54fdf5627349d86bfc" class="toggle" checked />
    <label for="section-ca53d32fab0e1a54fdf5627349d86bfc" class="flex justify-between">
      <a href="/ai-workflows/eval/" class="">AI Evaluation</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/healthcare/" class="">Healthcare</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="toggle"  />
    <label for="section-fd1d2eede2c2d7e81bbbc100c0b57829" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/" class="">Domain</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-45ba5974905f86df95925365835eadbb" class="toggle"  />
    <label for="section-45ba5974905f86df95925365835eadbb" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/ai-in-healthcare/" class="">AI in Healthcare</a>
    </label>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/" class="">C2 Clinical Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/" class="">C3 ML Healthcare</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/" class="">C4 AI Evaluations</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/domain_knowledge/ai-in-healthcare/c5_capstone/" class="">C5 Capstone Projects</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <input type="checkbox" id="section-9722ba71bf098ab02c3220d6e8d9056f" class="toggle"  />
    <label for="section-9722ba71bf098ab02c3220d6e8d9056f" class="flex justify-between">
      <a href="/healthcare/domain_knowledge/hands-on-healthcare-data/" class="">Hands-On Healthcare Data</a>
    </label>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/data/" class="">Data</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/healthcare/clinical_ai/" class="">AI Applications</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li class="book-section-flat" >
          
  
  

  
    <a href="/ipark/" class="">Inhee Park, PhD - Resume</a>
  

          
  <ul>
    
  </ul>

        </li>
      
    
  </ul>










  
<ul>
  
  <li>
    <a href="https://www.linkedin.com/in/inheepark/"  target="_blank" rel="noopener">
        â•°â”€â”€LinkedIn
      </a>
  </li>
  
  <li>
    <a href="https://github.com/imipark/"  target="_blank" rel="noopener">
        â•°â”€â”€GitHub
      </a>
  </li>
  
  <li>
    <a href="/posts/"  target="_blank" rel="noopener">
        â•°â”€â”€Blog
      </a>
  </li>
  
  <li>
    <a href="https://iparkirk.github.io"  target="_blank" rel="noopener">
        â•°â”€â”€Old Web
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Eval Infra: Verifiable vs Non-Verifiable vs Hybrid</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#why-these-5-layers-separation-of-concerns">Why These 5 Layers? Separation of Concerns</a></li>
            <li><a href="#how-layers-map-to-natural-workflow">How Layers Map to Natural Workflow</a></li>
            <li><a href="#different-stakeholders-own-each-layern">Different Stakeholders Own Each Layern</a></li>
            <li><a href="#the-spectrum-of-verifiability">The Spectrum of Verifiability</a></li>
            <li><a href="#examples-by-category">Examples by Category</a></li>
            <li><a href="#layer-1-benchmark-design"><strong>LAYER 1: BENCHMARK DESIGN</strong></a></li>
            <li><a href="#layer-2-evaluation-execution"><strong>LAYER 2: EVALUATION EXECUTION</strong></a></li>
            <li><a href="#layer-3-scalability"><strong>LAYER 3: SCALABILITY</strong></a></li>
            <li><a href="#layer-4-metrics--reliability"><strong>LAYER 4: METRICS &amp; RELIABILITY</strong></a></li>
            <li><a href="#layer-5-production-monitoring"><strong>LAYER 5: PRODUCTION MONITORING</strong></a></li>
            <li><a href="#each-layer-has-different-bottleneck">EACH LAYER HAS DIFFERENT BOTTLENECK</a></li>
            <li><a href="#real-world-examples-by-category">REAL-WORLD EXAMPLES BY CATEGORY</a></li>
            <li><a href="#the-three-way-split">The Three-Way Split:</a></li>
            <li><a href="#why-this-matters">Why This Matters:</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article"><p align="center">
<img src="/images/AIR_logo.png" alt="AI Reasoning Logo" width="200"/>
<strong style="font-size:1.4rem; color:#374151; display:block; margin-bottom:0.8rem; text-transform:uppercase; letter-spacing:0.0rem;">
Eval Infra: Verifiable (STEM) vs Non-Verifiable vs Hybrid
</strong>
</p>
<hr>
<h3 id="why-these-5-layers-separation-of-concerns">
  Why These 5 Layers? Separation of Concerns
  <a class="anchor" href="#why-these-5-layers-separation-of-concerns">#</a>
</h3>
<p>Each layer serves a <strong>distinct purpose</strong> in the AI evaluation lifecycle, from research to production deployment.</p>
<table>
  <thead>
      <tr>
          <th><strong>Layer</strong></th>
          <th><strong>Purpose</strong></th>
          <th><strong>Key Question</strong></th>
          <th><strong>Stakeholder</strong></th>
          <th><strong>Output</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>L1: Benchmark Design</strong></td>
          <td>Define what &ldquo;good&rdquo; means</td>
          <td>What are we measuring?</td>
          <td>Research Scientists</td>
          <td>Test suite + evaluation protocol</td>
      </tr>
      <tr>
          <td><strong>L2: Evaluation Execution</strong></td>
          <td>Actually measure performance</td>
          <td>How do we score it?</td>
          <td>ML Engineers</td>
          <td>Raw scores/labels per example</td>
      </tr>
      <tr>
          <td><strong>L3: Scalability</strong></td>
          <td>Handle volume &amp; iteration speed</td>
          <td>Can we do this 1000x?</td>
          <td>MLOps/Infrastructure</td>
          <td>Evaluation pipeline infrastructure</td>
      </tr>
      <tr>
          <td><strong>L4: Metrics &amp; Reliability</strong></td>
          <td>Trust the measurements</td>
          <td>Is this signal real?</td>
          <td>Data Scientists, Leadership</td>
          <td>Aggregate metrics + confidence intervals</td>
      </tr>
      <tr>
          <td><strong>L5: Production Monitoring</strong></td>
          <td>Maintain quality in the wild</td>
          <td>Is it still working?</td>
          <td>SREs, Product Managers</td>
          <td>Live dashboards + alerting systems</td>
      </tr>
  </tbody>
</table>
<h3 id="how-layers-map-to-natural-workflow">
  How Layers Map to Natural Workflow
  <a class="anchor" href="#how-layers-map-to-natural-workflow">#</a>
</h3>
<pre style="line-height: 1.0;">
RESEARCH PHASE (Offline Development)
â”‚
â”œâ”€ L1: Design Benchmarks
â”‚   â””â”€ "What constitutes correct/good performance?"
â”‚
â”œâ”€ L2: Run Evaluations  
â”‚   â””â”€ "Generate responses and score them"
â”‚
â”œâ”€ L3: Scale Infrastructure
â”‚   â””â”€ "Need to iterate fast â†’ evaluate 10K examples/day"
â”‚
â””â”€ L4: Analyze Results
    â””â”€ "Aggregate metrics, validate reliability"

DEPLOYMENT PHASE (Online Production)
â”‚
â””â”€ L5: Monitor Production
    â””â”€ "Continuous validation, catch regressions"
    â””â”€ Feed failures back to L1 (closed loop)
</pre>
<h3 id="different-stakeholders-own-each-layern">
  Different Stakeholders Own Each Layern
  <a class="anchor" href="#different-stakeholders-own-each-layern">#</a>
</h3>
<pre style="line-height: 1.0;">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1: Research Scientists                             â”‚
â”‚     â†’ Design evaluation protocols                   â”‚
â”‚     â†’ Define what "good" means for the domain       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L2: ML Engineers                                    â”‚
â”‚     â†’ Implement evaluation scripts                  â”‚
â”‚     â†’ Run model inference + scoring                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L3: MLOps / Infrastructure Engineers                â”‚
â”‚     â†’ Build scalable eval pipelines                 â”‚
â”‚     â†’ Manage compute resources                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L4: Data Scientists / Research Leadership           â”‚
â”‚     â†’ Statistical analysis of eval results          â”‚
â”‚     â†’ Validate metric reliability                   â”‚
â”‚     â†’ Make deployment decisions                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L5: SREs / Product Managers                         â”‚
â”‚     â†’ Monitor production performance                â”‚
â”‚     â†’ Alert on regressions                          â”‚
â”‚     â†’ Coordinate incident response                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
<h3 id="the-spectrum-of-verifiability">
  The Spectrum of Verifiability
  <a class="anchor" href="#the-spectrum-of-verifiability">#</a>
</h3>
<pre style="line-height: 1.0;">
Fully Verifiable â†â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â€•â†’ Non-Verifiable
      â”‚                    â”‚                      â”‚
   Code/Math          Technical Writing      Creative/Social
   (external          (hybrid: facts +       (judgment only)
    oracle)            style/clarity)
</pre>
<h3 id="examples-by-category">
  Examples by Category
  <a class="anchor" href="#examples-by-category">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Fully Verifiable</strong></th>
          <th><strong>Hybrid (Partially Verifiable)</strong></th>
          <th><strong>Non-Verifiable</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>â€¢ Code execution</td>
          <td>â€¢ Technical writing (facts âœ“, clarity âœ—)</td>
          <td>â€¢ Creative writing</td>
      </tr>
      <tr>
          <td>â€¢ Math computation</td>
          <td>â€¢ Translation (accuracy âœ“, style âœ—)</td>
          <td>â€¢ Persuasive marketing</td>
      </tr>
      <tr>
          <td>â€¢ Logic proofs</td>
          <td>â€¢ Legal analysis (precedent âœ“, judgment âœ—)</td>
          <td>â€¢ Empathetic therapy</td>
      </tr>
      <tr>
          <td>â€¢ Data extraction</td>
          <td>â€¢ Medical diagnosis (tests âœ“, manner âœ—)</td>
          <td>â€¢ Humor generation</td>
      </tr>
      <tr>
          <td>â€¢ Fact checking</td>
          <td>â€¢ Recipe generation (chemistry âœ“, taste âœ—)</td>
          <td>â€¢ Art critique</td>
      </tr>
      <tr>
          <td>â€¢ Physics simulation</td>
          <td>â€¢ Code review (bugs âœ“, readability âœ—)</td>
          <td>â€¢ Storytelling</td>
      </tr>
  </tbody>
</table>
<h3 id="layer-1-benchmark-design">
  <strong>LAYER 1: BENCHMARK DESIGN</strong>
  <a class="anchor" href="#layer-1-benchmark-design">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Verifiable (Code/Math/Logic)</strong></th>
          <th><strong>Hybrid (Technical/Professional)</strong></th>
          <th><strong>Non-Verifiable (Creative/Social)</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Structure:</strong> Problem + Test Suite â†’ Automated Oracle</td>
          <td><strong>Structure:</strong> Task + Mixed Criteria â†’ Automated + Human</td>
          <td><strong>Structure:</strong> Prompt + Rubric â†’ Human/AI Judgment</td>
      </tr>
      <tr>
          <td><strong>Key Benchmarks:</strong><br>â€¢ HumanEval (164 code problems)<br>â€¢ MATH (12K competition problems)<br>â€¢ GPQA (448 PhD questions)</td>
          <td><strong>Key Benchmarks:</strong><br>â€¢ Technical Writing Quality<br>â€¢ Translation (BLEU + human eval)<br>â€¢ Medical Q&amp;A (facts + empathy)</td>
          <td><strong>Key Benchmarks:</strong><br>â€¢ MT-Bench (80 conversations)<br>â€¢ AlpacaEval (805 prompts)<br>â€¢ Chatbot Arena (live voting)</td>
      </tr>
      <tr>
          <td><strong>Properties:</strong><br>âœ… Tests are deterministic<br>âœ… Can generate infinite variants<br>âœ… No human disagreement</td>
          <td><strong>Properties:</strong><br>âš ï¸ Some aspects objective<br>âš ï¸ Some aspects subjective<br>âš ï¸ Requires dual evaluation</td>
          <td><strong>Properties:</strong><br>âŒ Ratings are subjective<br>âŒ Context-dependent quality<br>âŒ Humans disagree frequently</td>
      </tr>
      <tr>
          <td><strong>Creation Time:</strong> 1-2 weeks</td>
          <td><strong>Creation Time:</strong> 2-3 weeks</td>
          <td><strong>Creation Time:</strong> 3-4 weeks</td>
      </tr>
      <tr>
          <td><strong>Example:</strong><br><code>assert reverse([1,2,3]) == [3,2,1]</code></td>
          <td><strong>Example:</strong><br>Accuracy: Does translation preserve meaning? (âœ“)<br>Fluency: Does it sound natural? (human)</td>
          <td><strong>Example:</strong><br>&ldquo;Is this story engaging?&quot;<br>â†’ 5-point Likert scale (human)</td>
      </tr>
  </tbody>
</table>
<h3 id="layer-2-evaluation-execution">
  <strong>LAYER 2: EVALUATION EXECUTION</strong>
  <a class="anchor" href="#layer-2-evaluation-execution">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Verifiable</strong></th>
          <th><strong>Hybrid</strong></th>
          <th><strong>Non-Verifiable</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Pipeline:</strong><br>1. Generate solutions (10 min)<br>2. <strong>Execute &amp; verify (5 min)</strong><br>3. Compute metrics (&lt;1 min)</td>
          <td><strong>Pipeline:</strong><br>1. Generate outputs (10 min)<br>2. <strong>Automated checks (5 min)</strong><br>3. <strong>Human evaluation (4-20 hours)</strong><br>4. Combine scores (30 min)</td>
          <td><strong>Pipeline:</strong><br>1. Generate responses (5 min)<br>2. <strong>Human/AI rating (8-40 hours)</strong><br>3. Aggregate &amp; validate (1 hour)</td>
      </tr>
      <tr>
          <td><strong>Verification:</strong><br><code>python result = execute_code(solution)&lt;br&gt;label = PASS if tests_pass else FAIL&lt;br&gt;</code></td>
          <td><strong>Verification:</strong><br><code>python # Automated component&lt;br&gt;facts_correct = verify_facts(output)&lt;br&gt;# Human component&lt;br&gt;clarity = human_rate_clarity(output)&lt;br&gt;score = 0.5*facts + 0.5*clarity&lt;br&gt;</code></td>
          <td><strong>Verification:</strong><br><code>python ratings = get_human_ratings(n=3)&lt;br&gt;score = mean(ratings)&lt;br&gt;# Then validate inter-rater agreement&lt;br&gt;</code></td>
      </tr>
      <tr>
          <td><strong>Throughput:</strong> 10K-100K evals/hour</td>
          <td><strong>Throughput:</strong> 500-5K evals/hour</td>
          <td><strong>Throughput:</strong> 100-1K evals/hour</td>
      </tr>
      <tr>
          <td><strong>Cost per eval:</strong> $0.001-0.01</td>
          <td><strong>Cost per eval:</strong> $0.05-1.00</td>
          <td><strong>Cost per eval:</strong> $0.10-5.00</td>
      </tr>
      <tr>
          <td><strong>Human time:</strong> 0 hours</td>
          <td><strong>Human time:</strong> 4-20 hours (partial)</td>
          <td><strong>Human time:</strong> 24-120 hours (full)</td>
      </tr>
  </tbody>
</table>
<h3 id="layer-3-scalability">
  <strong>LAYER 3: SCALABILITY</strong>
  <a class="anchor" href="#layer-3-scalability">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Verifiable</strong></th>
          <th><strong>Hybrid</strong></th>
          <th><strong>Non-Verifiable</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Bottleneck:</strong> Compute (GPU time)</td>
          <td><strong>Bottleneck:</strong> Human time for subjective parts</td>
          <td><strong>Bottleneck:</strong> Human bandwidth</td>
      </tr>
      <tr>
          <td><strong>Scaling Examples:</strong><br>â€¢ 164 problems â†’ $2, 15 min<br>â€¢ 10K problems â†’ $122, 2 hours<br>â€¢ 100K problems â†’ $1,220, 8 hours</td>
          <td><strong>Scaling Examples:</strong><br>â€¢ 100 docs â†’ $50, 4 hours<br>â€¢ 1K docs â†’ $500, 20 hours<br>â€¢ 10K docs â†’ $5K, 200 hours<br>(Mix of automated + human)</td>
          <td><strong>Scaling Examples:</strong><br>â€¢ 80 prompts â†’ $400, 8-40 hours<br>â€¢ 10K prompts â†’ $37,500, 2,500 hours<br>â€¢ (Or $8K hybrid with AI judges)</td>
      </tr>
      <tr>
          <td><strong>Human scaling:</strong> 0 hours regardless of scale</td>
          <td><strong>Human scaling:</strong> Sub-linear (automate what&rsquo;s possible)</td>
          <td><strong>Human scaling:</strong> Linear or super-linear</td>
      </tr>
      <tr>
          <td><strong>Constraint:</strong> Money (buy more GPUs)</td>
          <td><strong>Constraint:</strong> Time + money (human for quality checks)</td>
          <td><strong>Constraint:</strong> Time (recruit, train raters)</td>
      </tr>
      <tr>
          <td><strong>Automation:</strong> 99.9%</td>
          <td><strong>Automation:</strong> 40-70% (depends on domain)</td>
          <td><strong>Automation:</strong> 5-30% (AI judges need validation)</td>
      </tr>
      <tr>
          <td><strong>Scalability Strategy:</strong><br>â€¢ Parallelize across GPUs<br>â€¢ Generate synthetic test cases<br>â€¢ Cost scales linearly</td>
          <td><strong>Scalability Strategy:</strong><br>â€¢ Automate objective criteria<br>â€¢ Sample human evaluation (10-20%)<br>â€¢ Use AI judges for subjective parts (with validation)</td>
          <td><strong>Scalability Strategy:</strong><br>â€¢ Train reward models on human labels<br>â€¢ Use LLM-as-judge (must validate)<br>â€¢ Spot-check 5-10% with humans</td>
      </tr>
  </tbody>
</table>
<h3 id="layer-4-metrics--reliability">
  <strong>LAYER 4: METRICS &amp; RELIABILITY</strong>
  <a class="anchor" href="#layer-4-metrics--reliability">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Verifiable</strong></th>
          <th><strong>Hybrid</strong></th>
          <th><strong>Non-Verifiable</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Metrics:</strong><br>â€¢ pass@k (% solved in k tries)<br>â€¢ Compile rate<br>â€¢ Exact match accuracy<br>â€¢ Error tolerance</td>
          <td><strong>Metrics:</strong><br>â€¢ Factual accuracy (automated)<br>â€¢ Readability score (formula)<br>â€¢ Clarity rating (human)<br>â€¢ Combined weighted score</td>
          <td><strong>Metrics:</strong><br>â€¢ Likert scale (1-5 ratings)<br>â€¢ Win rate vs baseline<br>â€¢ Elo ratings (head-to-head)<br>â€¢ Thumbs up/down ratio</td>
      </tr>
      <tr>
          <td><strong>Properties:</strong><br>âœ… Objective &amp; reproducible<br>âœ… Labs can compare directly<br>âœ… No gaming (oracle is external)<br>âœ… Leaderboards meaningful</td>
          <td><strong>Properties:</strong><br>âš ï¸ Partially objective<br>âš ï¸ Requires careful weighting<br>âš ï¸ Some gaming risk on subjective parts<br>âš ï¸ Need to report both auto + human metrics</td>
          <td><strong>Properties:</strong><br>âŒ Subjective &amp; noisy<br>âŒ Different protocols â†’ incomparable<br>âŒ Gaming risk (optimize for judge)<br>âŒ Leaderboards have selection bias</td>
      </tr>
      <tr>
          <td><strong>Inter-evaluator agreement:</strong> 100%</td>
          <td><strong>Inter-evaluator agreement:</strong><br>Facts: 95-100%<br>Quality: 70-85%</td>
          <td><strong>Inter-evaluator agreement:</strong> 60-80%</td>
      </tr>
      <tr>
          <td><strong>Example Metrics:</strong><br>â€¢ HumanEval pass@1: 67.8%<br>â€¢ MATH accuracy: 82.3%<br>â€¢ Error rate: 5.2%</td>
          <td><strong>Example Metrics:</strong><br>â€¢ Translation BLEU: 45.2 (auto)<br>â€¢ Fluency: 4.1/5 (human)<br>â€¢ Medical accuracy: 94% (auto), Empathy: 3.8/5 (human)</td>
          <td><strong>Example Metrics:</strong><br>â€¢ MT-Bench: 7.9/10 (GPT-4 judge)<br>â€¢ Human preference: 78% win rate<br>â€¢ Elo rating: 1,245</td>
      </tr>
  </tbody>
</table>
<h3 id="layer-5-production-monitoring">
  <strong>LAYER 5: PRODUCTION MONITORING</strong>
  <a class="anchor" href="#layer-5-production-monitoring">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Verifiable</strong></th>
          <th><strong>Hybrid</strong></th>
          <th><strong>Non-Verifiable</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Real-time signals:</strong><br>â€¢ Does code compile? âœ“/âœ—<br>â€¢ Tests pass? âœ“/âœ—<br>â€¢ User accepted? âœ“/âœ—<br>â€¢ Execution time OK? âœ“/âœ—</td>
          <td><strong>Real-time signals:</strong><br>â€¢ Facts verified? âœ“/âœ—<br>â€¢ Format correct? âœ“/âœ—<br>â€¢ User satisfaction proxy (usage time)<br>â€¢ Error rate</td>
          <td><strong>Proxy signals:</strong><br>â€¢ Thumbs up/down ratio<br>â€¢ Session length<br>â€¢ Regeneration rate<br>â€¢ Response length</td>
      </tr>
      <tr>
          <td><strong>Monitoring:</strong><br>â€¢ Every request â†’ Automated check<br>â€¢ Dashboard updates: Real-time<br>â€¢ Regression alerts: Instant</td>
          <td><strong>Monitoring:</strong><br>â€¢ Automated checks: Real-time<br>â€¢ Human spot-checks: Weekly (10% sample)<br>â€¢ Combined quality score trending</td>
          <td><strong>Monitoring:</strong><br>â€¢ Sample 100 conversations/week<br>â€¢ 3 humans rate each<br>â€¢ Compare to last month</td>
      </tr>
      <tr>
          <td><strong>Action:</strong><br>â€¢ Compile rate &lt;90% â†’ Auto rollback<br>â€¢ Pass@1 drops &gt;5% â†’ Alert engineer</td>
          <td><strong>Action:</strong><br>â€¢ Fact accuracy &lt;95% â†’ Auto rollback<br>â€¢ Quality score drops &gt;0.3 â†’ Investigate<br>â€¢ Run deeper human eval if needed</td>
          <td><strong>Action:</strong><br>â€¢ Quality drops &gt;0.3 â†’ Investigate<br>â€¢ A/B test for 7 days<br>â€¢ Need human eval to decide</td>
      </tr>
      <tr>
          <td><strong>Human role:</strong> Only when alerts fire</td>
          <td><strong>Human role:</strong> Weekly spot-checks (10%)</td>
          <td><strong>Human role:</strong> Continuous (weekly audits)</td>
      </tr>
      <tr>
          <td><strong>Dashboard Example:</strong><br>Compile Rate: 94.2% ğŸŸ¢<br>Pass@1: 67.8% ğŸŸ¢<br>Latency: 1.2s ğŸŸ¡</td>
          <td><strong>Dashboard Example:</strong><br>Fact Check: 96.1% ğŸŸ¢<br>User Rating: 4.2/5 ğŸŸ¢<br>Clarity (sampled): 3.9/5 ğŸŸ¡</td>
          <td><strong>Dashboard Example:</strong><br>Human Rating: 4.2/5 ğŸ”´<br>Thumbs Up: 78% ğŸŸ¢<br>Session Time: 8.2min ğŸŸ¢</td>
      </tr>
  </tbody>
</table>
<h3 id="each-layer-has-different-bottleneck">
  EACH LAYER HAS DIFFERENT BOTTLENECK
  <a class="anchor" href="#each-layer-has-different-bottleneck">#</a>
</h3>
<table>
  <thead>
      <tr>
          <th><strong>Layer</strong></th>
          <th><strong>Verifiable Bottleneck</strong></th>
          <th><strong>Hybrid Bottleneck</strong></th>
          <th><strong>Non-Verifiable Bottleneck</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>L1: Design</strong></td>
          <td>Writing test suite</td>
          <td>Defining which parts are verifiable</td>
          <td>Getting human agreement on rubric</td>
      </tr>
      <tr>
          <td><strong>L2: Execute</strong></td>
          <td>GPU inference time</td>
          <td><strong>Human time for quality checks</strong></td>
          <td><strong>Human annotation time</strong></td>
      </tr>
      <tr>
          <td><strong>L3: Scale</strong></td>
          <td>Compute budget</td>
          <td><strong>Hiring raters for quality</strong></td>
          <td><strong>Hiring/training many raters</strong></td>
      </tr>
      <tr>
          <td><strong>L4: Metrics</strong></td>
          <td>Statistical analysis</td>
          <td><strong>Balancing auto vs human metrics</strong></td>
          <td><strong>Inter-rater reliability</strong></td>
      </tr>
      <tr>
          <td><strong>L5: Monitor</strong></td>
          <td>Infrastructure cost</td>
          <td><strong>Continuous spot-checking</strong></td>
          <td><strong>Continuous human auditing</strong></td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="real-world-examples-by-category">
  REAL-WORLD EXAMPLES BY CATEGORY
  <a class="anchor" href="#real-world-examples-by-category">#</a>
</h3>
<ul>
<li>
<p><strong>Verifiable:</strong></p>
<pre tabindex="0"><code>    âœ… Code Generation (GitHub Copilot, Cursor)
       â†’ Unit tests verify correctness
       â†’ Compile rate is objective

    âœ… Math Problem Solving (Khan Academy AI)
       â†’ Symbolic solver verifies answers
       â†’ Can generate infinite practice problems

    âœ… Data Extraction (GPT-4 with function calling)
       â†’ Schema validation is deterministic
       â†’ JSON parsing either works or fails
    ```
</code></pre></li>
<li>
<p><strong>Hybrid:</strong></p>
<pre tabindex="0"><code>    âš ï¸ Medical Diagnosis Assistant
       â†’ Facts: Test results, drug interactions (verifiable)
       â†’ Quality: Bedside manner, explanation clarity (human eval)

    âš ï¸ Legal Document Analysis
       â†’ Facts: Case precedents, statutes (verifiable)
       â†’ Quality: Argument strength, writing quality (human eval)

    âš ï¸ Translation Systems
       â†’ Accuracy: BLEU score, term consistency (automated)
       â†’ Fluency: Natural phrasing, cultural adaptation (human eval)
    ```
</code></pre></li>
<li>
<p><strong>Non-Verifiable:</strong></p>
<pre tabindex="0"><code>    âŒ Creative Writing (Claude, ChatGPT creative mode)
       â†’ &#34;Is this story engaging?&#34; â†’ Subjective
       â†’ No automated test possible

    âŒ Therapy Chatbots (Woebot, Replika)
       â†’ &#34;Is this empathetic?&#34; â†’ Cultural/personal
       â†’ Requires human evaluation

    âŒ Marketing Copy Generation
       â†’ &#34;Is this persuasive?&#34; â†’ Audience-dependent
       â†’ A/B testing required (slow, expensive)
    ```
</code></pre></li>
</ul>
<hr>
<h3 id="the-three-way-split">
  The Three-Way Split:
  <a class="anchor" href="#the-three-way-split">#</a>
</h3>
<p><strong>Verifiable</strong> = Evaluation bottlenecked by <strong>compute budget</strong></p>
<ul>
<li>Fast iteration, predictable costs, objective metrics</li>
<li>Future: Unlimited synthetic data generation</li>
</ul>
<p><strong>Hybrid</strong> = Evaluation bottlenecked by <strong>smart automation + targeted human input</strong></p>
<ul>
<li>Medium iteration speed, mixed costs, dual metrics</li>
<li>Future: Better AI judges for subjective aspects</li>
</ul>
<p><strong>Non-Verifiable</strong> = Evaluation bottlenecked by <strong>human labor availability</strong></p>
<ul>
<li>Slow iteration, uncertain costs, noisy metrics</li>
<li>Future: Constitutional AI, better preference learning</li>
</ul>
<hr>
<h3 id="why-this-matters">
  Why This Matters:
  <a class="anchor" href="#why-this-matters">#</a>
</h3>
<p>This three-way framework explains why:</p>
<ul>
<li>âœ… Coding assistants improve faster than creative writing tools</li>
<li>âœ… Math tutors are more reliable than therapy chatbots</li>
<li>âœ… Technical Q&amp;A is easier to align than open-ended conversation</li>
<li>âš ï¸ Medical AI needs dual evaluation (facts + empathy)</li>
<li>âš ï¸ Translation quality requires both automated + human metrics</li>
</ul>
<p><strong>The future of AI alignment depends on:</strong></p>
<ol>
<li>For verifiable domains: More efficient compute</li>
<li>For hybrid domains: Better decomposition of verifiable vs subjective aspects</li>
<li>For non-verifiable domains: Creating reliable &ldquo;oracles&rdquo; (AI judges as good as compilers)</li>
</ol>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#why-these-5-layers-separation-of-concerns">Why These 5 Layers? Separation of Concerns</a></li>
            <li><a href="#how-layers-map-to-natural-workflow">How Layers Map to Natural Workflow</a></li>
            <li><a href="#different-stakeholders-own-each-layern">Different Stakeholders Own Each Layern</a></li>
            <li><a href="#the-spectrum-of-verifiability">The Spectrum of Verifiability</a></li>
            <li><a href="#examples-by-category">Examples by Category</a></li>
            <li><a href="#layer-1-benchmark-design"><strong>LAYER 1: BENCHMARK DESIGN</strong></a></li>
            <li><a href="#layer-2-evaluation-execution"><strong>LAYER 2: EVALUATION EXECUTION</strong></a></li>
            <li><a href="#layer-3-scalability"><strong>LAYER 3: SCALABILITY</strong></a></li>
            <li><a href="#layer-4-metrics--reliability"><strong>LAYER 4: METRICS &amp; RELIABILITY</strong></a></li>
            <li><a href="#layer-5-production-monitoring"><strong>LAYER 5: PRODUCTION MONITORING</strong></a></li>
            <li><a href="#each-layer-has-different-bottleneck">EACH LAYER HAS DIFFERENT BOTTLENECK</a></li>
            <li><a href="#real-world-examples-by-category">REAL-WORLD EXAMPLES BY CATEGORY</a></li>
            <li><a href="#the-three-way-split">The Three-Way Split:</a></li>
            <li><a href="#why-this-matters">Why This Matters:</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












