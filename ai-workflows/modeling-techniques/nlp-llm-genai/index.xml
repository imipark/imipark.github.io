<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP/LLM/GenAI on AI in Healthcare</title>
    <link>https://imipark.github.io/ai-workflows/modeling-techniques/nlp-llm-genai/</link>
    <description>Recent content in NLP/LLM/GenAI on AI in Healthcare</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://imipark.github.io/ai-workflows/modeling-techniques/nlp-llm-genai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title></title>
      <link>https://imipark.github.io/ai-workflows/modeling-techniques/nlp-llm-genai/bert_cls_classification_summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/modeling-techniques/nlp-llm-genai/bert_cls_classification_summary/</guid>
      <description>&lt;h1 id=&#34;understanding-how-to-use-berts-cls-token-for-classification&#34;&gt;&#xA;  Understanding How to Use BERT&amp;rsquo;s CLS Token for Classification&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#understanding-how-to-use-berts-cls-token-for-classification&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Date:&lt;/strong&gt; 2025-03-31&lt;/p&gt;&#xA;&lt;h2 id=&#34;-question&#34;&gt;&#xA;  ‚ùì Question&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-question&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;How can we use the &lt;code&gt;[CLS]&lt;/code&gt; token (i.e., &lt;code&gt;h_cls&lt;/code&gt;) from the last layer of BERT for classification tasks? Given that the BERT output has shape &lt;code&gt;[batch_size, sequence_length, hidden_size]&lt;/code&gt;, how is it valid to pass only &lt;code&gt;[batch_size, hidden_size]&lt;/code&gt; to a &lt;code&gt;nn.Linear(hidden_size, num_classes)&lt;/code&gt; without flattening the sequence? And why don&amp;rsquo;t we flatten the whole sequence ‚Äî wouldn&amp;rsquo;t that destroy order?&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://imipark.github.io/ai-workflows/modeling-techniques/nlp-llm-genai/self_attention_summary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://imipark.github.io/ai-workflows/modeling-techniques/nlp-llm-genai/self_attention_summary/</guid>
      <description>&lt;h1 id=&#34;-understanding-self-attention-in-transformers-a-visual-breakdown&#34;&gt;&#xA;  üîç Understanding Self-Attention in Transformers: A Visual Breakdown&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-understanding-self-attention-in-transformers-a-visual-breakdown&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;This document summarizes key questions about self-attention, embedding vectors, positions, and the input matrix in Transformers ‚Äî using the image you provided as the foundation.&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;-what-is-happening-in-the-diagram&#34;&gt;&#xA;  üß† What Is Happening in the Diagram?&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#-what-is-happening-in-the-diagram&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The figure shows how &lt;strong&gt;self-attention&lt;/strong&gt; computes the output for a specific position (&amp;ldquo;detection&amp;rdquo;) by:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Generating &lt;strong&gt;attention weights&lt;/strong&gt; between that position and &lt;strong&gt;all other positions&lt;/strong&gt;.&lt;/li&gt;&#xA;&lt;li&gt;Using those weights to compute a &lt;strong&gt;weighted sum&lt;/strong&gt; of the input feature vectors.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;img src=&#34;Screenshot_from_2025-03-30_13-29-43.png&#34; alt=&#34;Self-Attention Diagram&#34; /&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
