[{"id":0,"href":"/healthcare-domain/","title":"Healthcare Domain","section":"","content":"Content for the Healthcare Domain section.\n"},{"id":1,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/toc_course3/","title":"ToC of Course 3","section":"C3 ML Healthcare","content":" ToC of Course 3/5: Fundamentals of Machine Learning for Healthcare # Module 3: Concepts and Principles of Machine Learning in Healthcare # Introduction to Deep Learning and Neural Networks Deep Learning and Neural Networks Cross Entropy Loss Gradient Descent Representing Unstructured Image and Text Data Convolutional Neural Networks Natural Language Processing and Recurrent Neural Networks The Transformer Architecture for Sequences Commonly Used and Advanced Neural Network Architectures Advanced Computer Vision Tasks and Wrap-Up Module 4: Evaluation and Metrics for Machine Learning in Healthcare # Introduction to Model Performance Evaluation Overfitting and Underfitting Strategies to Address Overfitting, Underfitting and Introduction to Regularization Statistical Approaches to Model Evaluation Receiver Operator and Precision Recall Curves as Evaluation Metrics Module 5: Strategies and Challenges in Machine Learning in Healthcare # Introduction to Common Clinical Machine Learning Challenges Utility of Causative Model Predictions Context in Clinical Machine Learning Intrinsic Interpretability Medical Data Challenges in Machine Learning Part 1 Medical Data Challenges in Machine Learning Part 2 How Much Data Do We Need? Retrospective Data in Medicine and \u0026ldquo;Shelf Life\u0026rdquo; for Data Medical Data: Quality vs Quantity Module 6: Best Practices, Teams, and Launching Your Machine Learning Journey # Clinical Utility and Output Action Pairing Taking Action - Utilizing the OAP Framework Building Multidisciplinary Teams for Clinical Machine Learning Governance, Ethics, and Best Practices On Being Human in the Era of Clinical Machine Learning Death by GPS and Other Lessons of Automation Bias Module 7: Foundation Models # Introduction to Foundation Models Adapting to Technology General AI and Emergent Behavior How Foundation Models Work Healthcare Use Cases for Text Data Healthcare Use Cases for Non-textual Unstructured Data Challenges and Pitfalls Conclusion "},{"id":2,"href":"/healthcare-domain/learning/ai-in-healthcare/c2_clinical_data/","title":"C2 Clinical Data","section":"AI in Healthcare","content":" üìò Course 2: Clinical Data \u0026ndash; 7 Modules # Topics Table Clinical Text Feature Extraction Using Dictionary-Based Filtering Clinical Text Mining Pipeline (Steps 1‚Äì5) Ethics in AI for Healthcare Missing Data Scenarios in Healthcare Modeling OMOP vs. RLHF Rule-Based Electronic Phenotyping Example: Type 2 Diabetes üß≠ Module 1: Asking and Answering Questions via Clinical Data Mining # 1. What\u0026rsquo;s the Problem?\nClinicians and researchers have important questions but lack a structured approach to answering them using clinical data.\n2. Why Does It Matter?\nWithout a systematic workflow, decisions may rely on anecdotal evidence or outdated knowledge, leading to suboptimal care.\n3. What\u0026rsquo;s the Core Idea?\nThe 4-step clinical data mining workflow: (1) Ask the right question ‚Üí (2) Find suitable data ‚Üí (3) Extract/transform data ‚Üí (4) Analyze and iterate.\n4. How Does It Work?\nStart with a real clinical scenario, define inclusion/exclusion criteria, search EMRs using codes/tests, and compute outcomes. Use a timeline and patient-feature matrix to support decisions.\n5. What\u0026rsquo;s Next?\nThis foundation enables accurate data selection (Module 2), temporal modeling (Module 3), and building datasets (Module 4).\nüè• Module 2: Data Available from Healthcare Systems # 1. What\u0026rsquo;s the Problem?\nHealthcare data is fragmented, inconsistently coded, and filled with biases and errors.\n2. Why Does It Matter?\nUsing flawed or incomplete data without understanding its origin can lead to misleading conclusions or unsafe decisions.\n3. What\u0026rsquo;s the Core Idea?\nCategorize and understand different healthcare data types, sources, and their limitations, including EMR, claims, registries, and patient-generated data.\n4. How Does It Work?\nStudy the roles of key actors (patients, providers, payers), structured vs. unstructured data types, and typical biases (selection, misclassification, incentives).\n5. What\u0026rsquo;s Next?\nProvides the context for building timelines (Module 3) and feature matrices (Module 4) while recognizing biases that need correction.\nüï∞Ô∏è Module 3: Representing Time in Clinical Data # 1. What\u0026rsquo;s the Problem?\nMost databases don\u0026rsquo;t represent or reason well about time, yet clinical reasoning depends heavily on event timing.\n2. Why Does It Matter?\nIncorrect ordering or missing timestamps can invalidate exposure-outcome relationships and confuse chronic vs. acute processes.\n3. What\u0026rsquo;s the Core Idea?\nUse patient timelines and time-aware logic to represent, bin, and reason about clinical events over time.\n4. How Does It Work?\nDefine index times, use bins to aggregate events, calculate time-to-event, handle censoring, and test for non-stationarity.\n5. What\u0026rsquo;s Next?\nEstablishes the temporal framework needed for building structured datasets (Module 4) and modeling disease progression (Module 6).\nüß± Module 4: Creating Analysis-Ready Datasets # 1. What\u0026rsquo;s the Problem?\nRaw timelines are complex and inconsistent ‚Äî they can\u0026rsquo;t be directly used in analysis or machine learning.\n2. Why Does It Matter?\nPoor feature engineering or ignoring missingness leads to weak, biased, or uninterpretable models.\n3. What\u0026rsquo;s the Core Idea?\nBuild a patient-feature matrix by selecting, cleaning, imputing, and engineering features from structured/unstructured data.\n4. How Does It Work?\nStandardize features, reduce dimensionality, handle missingness with imputation or removal, and use domain knowledge or PCA to create meaningful features.\n5. What\u0026rsquo;s Next?\nFeeds directly into downstream modeling, classification (Module 6), and cohort identification with better interpretability.\nüìÑ Module 5: Handling Unstructured Data # 1. What\u0026rsquo;s the Problem?\nValuable clinical information is trapped in unstructured formats like notes, images, and signals.\n2. Why Does It Matter?\nFailing to extract this information limits your ability to detect key conditions, traits, or outcomes that are not coded elsewhere.\n3. What\u0026rsquo;s the Core Idea?\nUse text mining, signal processing, and image interpretation to turn unstructured data into usable features.\n4. How Does It Work?\nApply NLP (e.g., negation/context detection), use knowledge graphs for term recognition, and process signals/images with appropriate tools.\n5. What\u0026rsquo;s Next?\nEnhances the patient-feature matrix (Module 4) and improves phenotyping accuracy and completeness (Module 6).\nüß¨ Module 6: Electronic Phenotyping # 1. What\u0026rsquo;s the Problem?\nIdentifying who truly has a disease or condition is challenging using only raw or coded data.\n2. Why Does It Matter?\nMisclassified patients lead to invalid cohorts, incorrect inferences, and flawed clinical decisions or model training.\n3. What\u0026rsquo;s the Core Idea?\nDefine phenotypes using rule-based or probabilistic methods to accurately identify conditions of interest.\n4. How Does It Work?\nUse inclusion/exclusion criteria (rule-based) or train classifiers (probabilistic) with anchors, weak labels, and features from Modules 4‚Äì5.\n5. What\u0026rsquo;s Next?\nEnables reliable cohort creation for clinical trials, observational studies, and AI/ML applications.\n‚öñÔ∏è Module 7: Clinical Data Ethics # 1. What\u0026rsquo;s the Problem?\nUsing patient data without safeguards risks violating privacy, losing trust, and causing harm.\n2. Why Does It Matter?\nUnethical data use can lead to legal issues, exclusion of vulnerable groups, and poor public perception of healthcare AI.\n3. What\u0026rsquo;s the Core Idea?\nApply ethical frameworks like the Belmont Report and Learning Health System to govern data use, consent, and fairness.\n4. How Does It Work?\nEnsure de-identification, obtain proper consent (or waiver), handle return of results thoughtfully, and consider justice in access and outcomes.\n5. What\u0026rsquo;s Next?\nProvides ethical boundaries and practices for applying all previous modules responsibly in real-world systems.\nClinical Data Modules # Clinical Text Feature Extraction Using Dictionary-Based Filtering Clinical Text Mining Pipeline (Steps 1‚Äì5) Ethics in AI for Healthcare Missing Data Scenarios in Healthcare Modeling OMOP vs. RLHF Rule-Based Electronic Phenotyping Example: Type 2 Diabetes "},{"id":3,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/","title":"C3 ML Healthcare","section":"AI in Healthcare","content":" üìò Course 3: Fundamentals of Machine Learning for Healthcare \u0026ndash; 8 Modules # Topics Table ToC of Course 3 Case Study: The Hidden Danger of Correlation in Healthcare AI Data Quality, Labeling, and Weak Supervision in Clinical ML Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations Healthcare Use Cases for Non-textual Unstructured Data Healthcare Use Cases for Text Data How Foundation Models Work Output-Action Pairing (OAP) Framework in Healthcare Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare MODULE 3: Concepts and Principles of ML in Healthcare # 1. What makes deep learning different from traditional ML?\nDeep learning organizes parameters into hierarchical layers allowing models to learn complex functions beyond input feature limits.\n2. How does a deep learning model transform data through layers?\nBy repeatedly applying linear combinations and nonlinear activation functions (e.g., ReLU), the model generates meaningful features.\n3. What is a dense layer in neural networks?\nA dense or fully connected layer has neurons applying transformations to the same input, forming a building block of deep nets.\n4. What does the training loop in deep learning involve?\nPrediction ‚Üí Loss calculation ‚Üí Parameter update (gradient descent) ‚Üí Validation ‚Üí Hyperparameter tuning.\n5. How do different loss functions behave?\nMSE penalizes large errors. MAE treats all errors equally. Cross-entropy is preferred for classification tasks. 6. How does gradient descent work?\nIt computes the gradient (slope) of the loss and adjusts model parameters to reduce the error.\n7. Why is backpropagation crucial?\nIt enables computing gradients layer-by-layer from output to input, allowing efficient updates in deep networks.\nREPRESENTING UNSTRUCTURED DATA # 8. How are images and text represented?\nImages: Pixel grids, 0‚Äì255 intensity values. Text: Word embeddings in high-dimensional space (300‚Äì1024 dims). 9. Why use CNNs for image tasks?\nCNNs preserve spatial structure and use filters to detect visual patterns efficiently.\n10. How do CNN layers increase feature abstraction?\nEach layer has a larger receptive field, allowing detection of complex features from simple edges.\n11. What makes RNNs fit for NLP tasks?\nThey handle sequential data by maintaining a context vector across time steps.\n12. How do Transformers improve over RNNs?\nThey use self-attention to capture long-term dependencies in parallel, improving speed and accuracy.\nADVANCED NETWORKS AND TASKS # 13. What are key CNN architectures?\nAlexNet: First with ReLU, 8 layers. VGG: Deeper with small filters. GoogLeNet/Inception: Parallel filter paths. ResNet: Skip connections, 152+ layers. 14. What are vision tasks beyond classification?\nSemantic segmentation: Per-pixel classification. Object detection: Bounding boxes. Instance segmentation: Combines both. 15. What is reinforcement learning (RL) and why is it hard in healthcare?\nRL needs simulatable environments and reward structures, both difficult in clinical settings.\nMODULE 4: Evaluation and Metrics for ML in Healthcare # 16. How should we split data for evaluation?\nTypical splits: Train (70‚Äì80%), Validation, Test. Use k-fold cross-validation for small datasets.\n17. What is underfitting vs overfitting?\nUnderfitting: Too simple to learn patterns. Overfitting: Memorizes training data noise. 18. How do you detect fitting issues?\nUse learning curves to visualize loss and accuracy trends on train vs validation data.\n19. How to combat overfitting?\nRegularization (L1/L2, Dropout), data augmentation, simpler models.\n20. Why is accuracy misleading in imbalanced data?\nIt may hide poor minority-class performance; use metrics like precision, recall, ROC/PR curves.\n21. What are ROC and PR curves?\nROC: TPR vs FPR across thresholds. PR: Precision vs Recall; better for imbalanced datasets. MODULE 5: Strategies and Challenges in ML for Healthcare # 22. Why is causation vs correlation important?\nML often learns correlations that may not reflect true medical causes‚Äîleading to misleading outcomes.\n23. What are confounding variables in healthcare ML?\nSpurious patterns (e.g., gray hair as predictor of heart disease) that mislead models.\n24. What is the ‚Äúblack box‚Äù problem?\nComplex models lack interpretability. Trade-off exists between accuracy and explainability.\n25. How can we interpret black-box models?\nUse saliency maps, gradients, CAMs, and visualize neuron activations.\n26. What makes a model interpretable?\nTransparent: Easily understood logic. Explainable: Clear rationale for output. Inspectable: Internally examinable. 27. What is intrinsic vs post-hoc interpretability?\nIntrinsic: Built-in simplicity (e.g., LACE index). Post-hoc: Retrospective interpretation (e.g., heatmaps). DATA QUALITY AND BIAS CHALLENGES # 28. Why is data quality so crucial?\n‚ÄúGarbage in, garbage out.‚Äù Bad data ‚Üí bad models. Data should be clean, recent, relevant, labeled.\n29. What is the class imbalance problem?\nRare-event labels skew performance and metrics. Address via resampling, proper metrics, or specialized models.\n30. Does more data always help?\nNot necessarily‚Äîolder data may degrade performance. Focus on adding information, not just volume.\n31. How to handle label noise?\nUse expert-reviewed subsets, triangulation with multiple labels, and accept some noise with large datasets.\nMODULE 6: Best Practices, Teams, and Launching ML in Healthcare # 32. What is the Output-Action Pair (OAP)?\nA framework linking model prediction to clinical action (e.g., sepsis alert ‚Üí early intervention).\n33. Why multidisciplinary teams matter?\nClinicians, statisticians, ML engineers, IT staff, and ethicists ensure models are usable, reliable, ethical.\n34. What roles are essential in clinical ML teams?\nData scientist (features, metrics) ML engineer (model building \u0026amp; deployment) Statistician (study design, validation) Healthcare IT (integration) Domain expert (clinical relevance) 35. What governance and ethics concerns exist?\nRisk of re-identification Informed consent Algorithmic bias Health equity 36. What is automation bias?\nOver-trusting ML systems leads to reduced vigilance‚Äîcounteract with calibrated risk scores and human oversight.\n37. Will ML replace healthcare jobs?\nNot fully‚ÄîML will augment routine tasks but jobs requiring empathy, judgment, and interaction remain essential.\nFINAL TAKEAWAYS # Simpler models are often more deployable. Be skeptical of great metrics‚Äîinvestigate false positives/negatives. Use external datasets and real-world data for true performance estimates. Multidisciplinary feedback loops are essential for trust and improvement. "},{"id":4,"href":"/healthcare-domain/learning/ai-in-healthcare/c4_ai_evaluation/","title":"C4 AI Evaluations","section":"AI in Healthcare","content":" üìò Course 4: Evaluations of AI Applications in Healthcare # Topics Table Module 1: AI in Healthcare # üöÄ Why AI Matters Enhances research, diagnosis, operations üìâ Current Gaps AI often evaluated on accuracy alone üîó Outcome-Action Pairing (OAP) Match model predictions with actionable steps Module 2: Evaluation Frameworks # ‚öñÔ∏è Beyond Accuracy Clinical utility, feasibility, net benefit üß© OAP in Detail Who acts? What is the benefit? How soon? üßº Data Quality Bias, missingness, outdated practices Module 3: Deployment Pathways # üß≠ Four Phases Design \u0026amp; Development Evaluate \u0026amp; Validate Diffuse \u0026amp; Scale Monitor \u0026amp; Maintain üõ†Ô∏è Challenges Workflow fit, interoperability, performance drift Module 4: Bias and Fairness # ‚ö†Ô∏è Bias Types Historical, representation, measurement, aggregation ‚úÖ Fairness Metrics Calibration, classification parity, anti-classification üîç Auditing and Reporting Use tools like MINIMAR and perform external validation Module 5: Regulatory and Ethical Landscape # üßë‚Äç‚öñÔ∏è Regulatory Pathways FDA SaMD framework (Valid Association, Analytical, Clinical Validation) üîÅ Adaptive Models Require lifecycle monitoring (TPLC) üåê Global Views GDPR in EU, FDA in US, centralized innovation in China Best Ethical Practices # üß† Key Principles Define problems clearly Mitigate bias Report conflicts of interest Ensure transparency and stakeholder involvement "},{"id":5,"href":"/healthcare-domain/learning/","title":"Learning","section":"Healthcare Domain","content":"Content for the Learning section.\nAI in Healthcare Specialization - Coursera AI in Medicine Specialization - Coursera Data Science for Clinical Data Specialization - Coursera Crash Course in Causal Inference in Observational Data - Coursera Hands-on Healthcare Data - Book "},{"id":6,"href":"/healthcare-domain/data/","title":"Healthcare Data","section":"Healthcare Domain","content":" üîó Healthcare Data Layers üìö Healthcare Data Sources "},{"id":7,"href":"/healthcare-domain/terminology/","title":"Healthcare Glossary","section":"Healthcare Domain","content":" Healthcare Glossary # Welcome to the Healthcare Glossary, a reference list of essential terms across clinical, regulatory, and AI healthcare domains.\nA # Adverse Event # An adverse event is any undesirable experience associated with the use of a medical product in a patient. These can be mild, moderate, or severe, and may or may not be caused by the product itself.\nTags: clinical, patient safety Related: Side Effect, Complication\nI # Interoperability # An Interoperability is the ability of different healthcare systems and software to exchange, interpret, and use data effectively. It is essential for integrated care across hospitals, labs, payers, and providers. Interoperability exists on multiple levels‚Äîfoundational (basic exchange), structural (common formats like HL7, FHIR), semantic (shared vocabularies like SNOMED, LOINC), and organizational (policies and workflows that support smooth sharing).\nTags: data standards, health IT Related: HL7, FHIR, SNOMED CT, Health Information Exchange\nT # Time of Event # Time of event refers to the specific point in time at which a defined event occurs in a clinical, observational, or healthcare setting.\nTags: temporal, clinical trials, causal inference Related: Event Onset, Timestamp, Exposure Time\nS # Side Effect # A side effect is a secondary, typically undesirable effect of a drug or medical treatment that occurs along with the desired therapeutic effect.\nTags: pharmacology, patient safety\nRelated: Adverse Event, Drug Reaction\n"},{"id":8,"href":"/healthcare-domain/tools/","title":"Infromatics Tools","section":"Healthcare Domain","content":"Content for the Tools section.\n"},{"id":9,"href":"/ai-workflows/","title":"AI Workflows","section":"","content":" In-Demand Skillsets for Healthcare LLM/NLP/GenAI Data Scientists # Core Priority: Retrieval-Augmented Generation (RAG) # RAG is one of the most in-demand skills in clinical GenAI due to:\nThe need to ground LLMs in real patient data Compliance, privacy, and traceability Applications like: Clinical Question Answering Summarization of EHRs Evidence-based recommendations Key Tools: # Vector DBs: Vertex AI Search, Pinecone, FAISS LLMs: Gemini, GPT-4, PaLM, Med-PaLM Frameworks: LangChain, LlamaIndex, Vertex Extensions Other High-Demand Skillsets # 1. Clinical NLP \u0026amp; Information Extraction # Named Entity Recognition (NER) Negation detection Temporal event extraction Tools: scispaCy, MedSpaCy, cTAKES, ClinicalBERT 2. LLMOps \u0026amp; GenAI Engineering # Prompt tracking and versioning Chain-of-Thought reasoning pipelines RAG monitoring and evaluation Tools: LangChain, LangSmith, PromptLayer, Trulens 3. Knowledge Graphs \u0026amp; Ontologies # UMLS, SNOMED, HPO integration Graph-based document ranking Symbolic-neural hybrid reasoning Tools: Neo4j, BioPortal APIs, KG-BERT 4. Temporal Modeling \u0026amp; Phenotyping # Patient timeline extraction Longitudinal modeling Conversion to OMOP/FHIR representations Tools: PyOMOP, Synthea, FHIR parsers 5. Multimodal Clinical AI # OCR and document understanding Fusion of tables, images, and text Radiology + Report generation Tools: Document AI (GCP), Form Recognizer (Azure), BioGPT-Vision 6. Evaluation \u0026amp; Explainability # Hallucination detection Factual consistency checks Traceability in RAG chains Tools: Trulens, PromptLayer, Giskard, LangSmith ‚úÖ Top Priority Summary Table # Skill Area Priority RAG (Retrieval-Augmented Generation) ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ LLMOps (Eval, Monitoring, Pipelines) ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ Clinical NLP (NER, Temporal Reasoning) ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ Knowledge Graph Integration ‚òÖ‚òÖ‚òÖ‚òÖ‚òÜ FHIR/OMOP Structured Modeling ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ Explainability \u0026amp; Evaluation ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Multimodal AI (Text + Tables + Images) ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ "},{"id":10,"href":"/ai-workflows/nlp-llm-genai/","title":"NLP‚ÜíLLMs‚ÜíGenAI","section":"AI Workflows","content":"Content for the NLP‚ÜíLLMs‚ÜíGenAI section.\n5-Day GenAI with Google Transformer Attention: Full Conceptual Breakdown Understanding How to Use BERT\u0026#39;s CLS Token for Classification Understanding Self-Attention in Transformers: A Visual Breakdown "},{"id":11,"href":"/ai-workflows/structural-reasoning/","title":"Structural Reasoning","section":"AI Workflows","content":"Content for the Structural Reasoning section.\nCausality : Causal Inference, Causal AI Graphs : Knowledge Graphs, GraphRAG "},{"id":12,"href":"/ai-workflows/mlops/","title":"MLOps","section":"AI Workflows","content":" Minimal MLOps + LLMOps Roadmap for Healthcare GenAI Scientists # 1. Foundation: Cloud + Security + Healthcare Data # Learn basic cloud concepts (GCP, AWS, or Azure) Understand IAM (Identity \u0026amp; Access Management) Get familiar with HIPAA \u0026amp; PHI compliance in cloud Study healthcare datasets (MIMIC-III, Synthea, FHIR, OMOP) 2. MLOps Essentials # Use cloud notebooks (Vertex AI Workbench / SageMaker Studio / Azure ML) Build training pipelines using Kubeflow or Vertex Pipelines Register and version models (Vertex Registry, SageMaker Registry) Deploy models to endpoints (REST, gRPC) Monitor models (drift, performance, fairness) 3. LLMOps Fundamentals # Prompt engineering + prompt version control Build simple RAG pipelines (LangChain + vector DB + LLM) Evaluate LLMs for hallucination, grounding, trustworthiness Use LangSmith / Trulens / PromptEval for eval tracking Fine-tune LLMs with domain data (clinical notes, ICD) 4. DevOps + CI/CD + Automation # Automate retraining + deployment with GitHub Actions / Cloud Build Containerize models with Docker Store + serve model artifacts from Cloud Storage / Registry 5. Bonus Skills # Use vector search tools (FAISS, Pinecone, Vertex AI Search) Try LangGraph for stateful agent workflows Learn about explainability in LLMs (SHAP, saliency, tracing) "},{"id":13,"href":"/projects/","title":"Projects","section":"","content":" Starter Project Plan: LLMOps Pipeline for Summarizing Clinical Notes (RAG-based) # Goal # Build a simple yet realistic pipeline that summarizes patient clinical notes using a Retrieval-Augmented Generation (RAG) approach, with monitoring and prompt evaluation.\nTools # GCP: Vertex AI, Cloud Storage, Vertex AI Search LangChain for RAG LangSmith or Trulens for eval Clinical data: MIMIC-III sample notes 1. Setup # Set up a GCP project with billing Create a storage bucket for clinical notes Install LangChain, LangSmith SDKs Get access to Gemini API or use OpenAI GPT-4 2. Data Ingestion # Upload sample clinical notes to GCS Preprocess notes (remove PHI, segment into documents) Index documents using Vertex AI Search or FAISS 3. RAG Pipeline # Build a retriever (vector DB or AI Search) Create a summarization chain (Retriever ‚Üí LLM ‚Üí Summarizer) Run test queries (e.g., \u0026ldquo;Summarize last 24h progress note\u0026rdquo;) 4. Evaluation \u0026amp; Logging # Use LangSmith to trace RAG runs Evaluate summaries on: Grounding (does it match source text?) Hallucination (is any info invented?) Clinical usefulness (optional human-in-the-loop) 5. Automation (Optional) # Create a pipeline (Cloud Functions + Pub/Sub) for new notes Deploy as API via FastAPI or Vertex Endpoint 6. Bonus # Add user feedback logging Fine-tune LLM on clinical corpus (optional) "},{"id":14,"href":"/posts/","title":"Blog","section":"","content":"This is the blog index page. Here you\u0026rsquo;ll find all posts.\n"},{"id":15,"href":"/posts/ai_engineer_path_toc/","title":"The AI Engineer Path ‚Äì Scrimba","section":"Blog","content":" The AI Engineer Path ‚Äì Scrimba # https://www.coursera.org/specializations/ai-engineering#courses\nIntro to AI Engineering (104 min) # Welcome to The AI Engineer Path! AI Engineering basics The code so far Polygon API sign-up \u0026amp; key Get an OpenAI API Key Overview of how the API works An API call: OpenAI dependency An API call: Instance and model An API call: The messages array A quick word about models Prompt Engineering and a challenge Adding AI to the App Tokens The OpenAI Playground Temperature The \u0026ldquo;Few Shot\u0026rdquo; Approach Adding Examples Stop Sequence Frequency and Presence Penalties Fine-tuning Creating Images with the DALL¬∑E 3 API Intro to AI Safety Safety Best Practices Solo Project - PollyGlot You made it! Deployment (50 min) # Learn secure \u0026amp; robust deployment strategies Create a Cloudflare worker Connect your worker to OpenAI Update client side data fetching Handle CORS and preflight requests OpenAI API requests \u0026amp; responses Create an AI Gateway Error handling Create \u0026amp; deploy the Polygon API worker Fetch the stock data Download files and push to GitHub Deploy your site with Cloudflare Pages Custom domains with Cloudflare Recap \u0026amp; next steps Open-source Models (33 min) # Open source vs closed source Intro To HuggingFace.js Inference Text To Speech With HuggingFace.js Inference Transforming Images with HuggingFace.js Inference AI Models In The Browser With Transformers.js Download and Run AI Models on Your Computer with Ollama Section Recap Embeddings and Vector Databases (94 min) # Your next big step in AI engineering What are embeddings? Set up environment variables Create an embedding Challenge: Pair text with embedding Vector databases Set up your vector database Store vector embeddings Semantic search Query embeddings using similarity search Create a conversational response using OpenAI Chunking text from documents Challenge: Split text, get vectors, insert into Supabase Error handling Query database and manage multiple matches AI chatbot proof of concept Retrieval-augmented generation (RAG) Solo Project: PopChoice Agents (117 min) # AI Agent Intro Prompt Engineering 101 Control Response Formats Zooming Out Agent Setup Introduction to ReAct prompting Build action functions Write ReAct prompt - part 1 - planning ReAct Agent - part 2 - ReAct prompt ReAct Agent - part 3 - how does the \u0026ldquo;loop\u0026rdquo; work? ReAct Agent - part 4 - code setup ReAct Agent - part 5 - Plan for parsing the response ReAct Agent - part 6 - Parsing the Action ReAct Agent - part 7 - Calling the function ReAct Agent - part 8 - Housekeeping ReAct Agent - part 9 - Finally! The loop! OpenAI Functions Agent - part 1 - Intro OpenAI Functions Agent - part 2 - Demo day OpenAI Functions Agent - part 3 - Tools OpenAI Functions Agent - Part 4 - Loop Logic OpenAI Functions Agent - Part 5 - Setup Challenge OpenAI Functions Agent - Part 6 - Tool Calls OpenAI Functions Agent - Part 7 - Pushing to messages OpenAI Functions Agent - Part 8 - Adding arguments OpenAI Functions Agent - Part 9 - Automatic function calls Adding UI to agent - proof of concept Solo Project - AI Travel Agent Nice work! Multimodality (62 min) # Introduction Generate original images from a text prompt Response formats Prompting for image generation Size, quality and style Editing images Image generation challenge Image generation challenge solution GPT-4 with Vision - Part 1 GPT-4 with Vision - Part 2 Image generation \u0026amp; Vision recap OpenAI\u0026rsquo;s Assistants API (30 min) # Introducing the Assistants API How OpenAI Assistants work Create an Assistant Create a thread and messages Running an Assistant Bring it all together More to explore "},{"id":16,"href":"/posts/5_steps_learning_template/","title":"5 Steps Learning Template","section":"Blog","content":" What\u0026rsquo;s the Problem? What is the issue, gap, or challenge this module/concept is trying to address? ‚Üí Transition: ‚ÄúSo what if this problem exists?‚Äù\nWhy Does It Matter? What are the real-world stakes or consequences of not solving this problem? Who or what is affected? ‚Üí Transition: ‚ÄúGiven this urgency, what‚Äôs the smart way to tackle it?‚Äù\nWhat‚Äôs the Core Idea? What is the central concept, structure, or strategy introduced to solve the problem? ‚Üí Transition: ‚ÄúOkay, so how would I actually apply or build this?‚Äù\nHow Does It Work? How is the idea implemented in practice? What are the steps, inputs, mechanics, or workflows? Transition: ‚ÄúWhere does this take us next? What does it enable?‚Äù\nWhat‚Äôs Next? How does this fit into the bigger picture? What future task, analysis, or module does it support or prepare for?\n"},{"id":17,"href":"/posts/hugo-setup/","title":"Hugo Setup and Deploy","section":"Blog","content":" üöÄ Hugo + GitHub Pages Setup (User Site) # Minimal setup using hugo-book theme inside a Conda environment, with GitHub Pages deployment.\n1. Create and Activate Conda Environment # conda create -n hugo-env conda activate hugo-env 2. Install Hugo \u0026amp; Create Hugo Site with hugo-book Theme # # Install Hugo sudo apt install hugo # Or: brew install hugo # Create Hugo site hugo new site hugo-site cd hugo-site # Initialize git and add theme git init git submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book 3. Configure config.toml # baseURL = \u0026#39;https://your-username.github.io/\u0026#39; languageCode = \u0026#39;en-us\u0026#39; title = \u0026#39;My Hugo Site\u0026#39; theme = \u0026#39;hugo-book\u0026#39; [params] BookTheme = \u0026#39;light\u0026#39; BookToC = true BookCollapseSection = true BookFlatSection = false [[menu.sidebar]] name = \u0026#34;Knowledge Graph\u0026#34; url = \u0026#34;/kg/\u0026#34; weight = 1 4. Create Content and _index.md Files # # Create directories and content mkdir -p content/kg/topic1 touch content/_index.md touch content/kg/_index.md touch content/kg/topic1/_index.md hugo new kg/topic1/intro.md Directory Structure # content/ ‚îú‚îÄ‚îÄ _index.md ‚îú‚îÄ‚îÄ kg/ ‚îÇ ‚îú‚îÄ‚îÄ _index.md ‚îÇ ‚îî‚îÄ‚îÄ topic1/ ‚îÇ ‚îú‚îÄ‚îÄ _index.md ‚îÇ ‚îî‚îÄ‚îÄ intro.md _index.md contents # content/_index.md\n--- title: \u0026#34;Home\u0026#34; --- content/kg/_index.md\n--- title: \u0026#34;Knowledge Graph\u0026#34; bookFlatSection: false bookCollapseSection: true --- content/kg/topic1/_index.md\n--- title: \u0026#34;Topic 1\u0026#34; --- 5. Create GitHub Repository # Create repo: your-username.github.io\n(Required for GitHub User Pages) 6. GitHub Deployment # a. Generate a Personal Access Token (PAT) # Visit: https://github.com/settings/tokens Create a classic token with repo scope b. Initial Deployment (One-Time) # hugo cd public git init git checkout -b main git remote add origin https://github.com/your-username/your-username.github.io.git git add . git commit -m \u0026#34;Initial deploy\u0026#34; git push -u origin main cd .. c. Create Auto Deploy Script # deploy.sh\n#!/bin/bash hugo -D \u0026amp;\u0026amp; cd public \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -m \u0026#34;Updated site\u0026#34; \u0026amp;\u0026amp; git push origin main \u0026amp;\u0026amp; cd .. echo \u0026#34;‚úÖ Deployment Complete!\u0026#34; Make executable:\nchmod +x deploy.sh Run anytime:\n./deploy.sh 7. Check Deployment # GitHub ‚Üí Repository ‚Üí Settings ‚Üí Pages Source: main Folder: / (root) Live Site: https://your-username.github.io/ 8. Notes # _index.md files define sections and sidebar headings bookFlatSection = false preserves folder hierarchy bookCollapseSection = true enables collapsible sidebar hugo -D includes drafts when building "},{"id":18,"href":"/posts/hugo-source-backup/","title":"Hugo Source Backup","section":"Blog","content":" üîí Hugo Source Backup # This guide outlines how to back up your Hugo source files (excluding the public/ folder) to a private GitHub repository.\nüìÅ Folder Structure # Typical Hugo project structure:\nhugo-site/ ‚îú‚îÄ‚îÄ archetypes/ ‚îú‚îÄ‚îÄ content/ ‚îú‚îÄ‚îÄ layouts/ ‚îú‚îÄ‚îÄ static/ ‚îú‚îÄ‚îÄ themes/ ‚îú‚îÄ‚îÄ config.toml ‚îú‚îÄ‚îÄ public/ # \u0026lt;- This is ignored for source backup ‚îî‚îÄ‚îÄ backup.sh # Backup script ‚úÖ 1. Create a Private GitHub Repo # Go to https://github.com/new Name it something like hugo-source Set visibility to Private Don‚Äôt initialize with README or license ‚úÖ 2. Initialize Git in Your Hugo Site (if not already) # git init git remote add origin https://github.com/\u0026lt;your-username\u0026gt;/hugo-source.git echo \u0026#34;public/\u0026#34; \u0026gt;\u0026gt; .gitignore ‚úÖ 3. Create the Backup Script # Create a file named backup.sh in the root of your Hugo project:\n#!/bin/bash git add . git commit -m \u0026#34;üîí Backup: $(date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)\u0026#34; git push origin main echo \u0026#34;‚úÖ Backup Complete!\u0026#34; Make it executable:\nchmod +x backup.sh ‚úÖ 4. Use It! # To back up your source files:\n./backup.sh üìù Notes # Only your source files are backed up. The public/ folder is excluded (it‚Äôs where the generated site lives). Combine with deploy.sh for full workflow automation. "},{"id":19,"href":"/ai-workflows/mlops/ai_cloud_comparision/","title":"Ai Cloud Comparision","section":"MLOps","content":" Market Analysis: Azure vs AWS for AI/ML/GenAI # Feature Azure AWS Market Share ~23% ~31% (still largest) Enterprise Adoption Strong in healthcare, finance, gov (esp. with Microsoft 365/Teams/EHR ties) Strong with startups, research, media, big tech AI/ML Tools Azure Machine Learning, OpenAI on Azure, Synapse, Cognitive Services SageMaker, Bedrock, Comprehend, Rekognition GenAI Integration üî• Deep OpenAI partnership (GPT, Codex, DALL¬∑E via Azure OpenAI Service) Bedrock (Anthropic, Stability, Cohere), Titan (Amazon‚Äôs own) Ease of Use More integrated across MS ecosystem (Power BI, Excel, VS Code) More flexible but often messier to set up Learning Curve Smoother onboarding if familiar with Microsoft tools More customizable, but steeper learning curve Certifications Azure AI Engineer, Data Scientist, OpenAI Engineer (in preview) AWS ML Specialty, Solutions Architect, Bedrock tracks "},{"id":20,"href":"/ai-workflows/mlops/clinical_nlp_genai/","title":"Clinical Nlp Gen Ai","section":"MLOps","content":" üöÄ Why Clinical NLP \u0026amp; GenAI Are Growing in Healthcare # Clinical NLP \u0026amp; GenAI are growing rapidly in healthcare because they unlock massive untapped value in unstructured data ‚Äî which has historically been hard to use, yet contains the richest clinical context.\nüß† 1. 80% of Clinical Data is Unstructured # EHRs are full of free-text clinical notes, discharge summaries, radiology reports, operative notes, etc. Traditional models work well with structured data (ICD, labs), but miss context like: ‚ÄúPatient denies chest pain‚Äù ‚ÄúFamily history of diabetes‚Äù ‚ÄúPatient expressed concern about medication side effects‚Äù üß© NLP allows us to extract clinical meaning from this text and turn it into computable features.\nüîì 2. LLMs Unlocked Previously Impossible Use Cases # Older NLP methods (regex, rule-based, small transformers) had limited scope and brittle performance. LLMs like GPT-4, BioGPT, Med-PaLM, ClinicalBERT now: Understand clinical language Handle ambiguity and nuance (negation, temporality, coreference) Can answer questions, summarize, or extract entities with minimal supervision üß† We now have zero-shot/few-shot models that can generalize better and faster.\nüõ†Ô∏è 3. Tooling and Ecosystem Improvements # LLMOps tools (e.g., LangChain, LlamaIndex) make it easy to build: RAG pipelines from medical knowledge bases (e.g., UpToDate, PubMed) Clinical chatbots, document summarizers, question-answering tools Medical NLP toolkits are becoming better: scispaCy, medspaCy, MetaMap, cTAKES, MedCAT HuggingFace models like BioClinicalBERT, BlueBERT, PubMedBERT üè• 4. Real Clinical Needs Driving Demand # Physicians are overwhelmed by documentation ‚Äî GenAI is helping with: Ambient scribes (auto-documenting patient visits) Auto-summarization of notes, referrals, discharge instructions Researchers want to extract phenotypes or chart review signals at scale Payers want to mine notes for HCC coding or prior authorization info ‚è≥ NLP reduces chart review time from hours to seconds\n‚öñÔ∏è 5. Regulatory and Business Shifts # FDA and CMS are recognizing NLP-derived features in trials and risk models Private sector is investing heavily (e.g., Nuance, Abridge, AWS HealthScribe, Epic‚Äôs NoteReader, Google Med-PaLM) NLP applications align with value-based care and documentation burden reduction, two big industry trends üìà 6. Surge in Research and Commercial Applications # Explosion of clinical NLP papers, open datasets (MIMIC-III notes, i2b2, n2c2), and competitions Many startups and research labs focus entirely on GenAI for clinical use cases üî¨ Oncology-Specific Use Case for Clinical NLP # üß™ Use Case: Automated Tumor Board Summarization # Problem: Oncologists review vast free-text data for tumor board meetings, including pathology, radiology, and progress notes. NLP Solution: Extracts key findings (e.g., tumor staging, mutations, response to therapy) from notes Summarizes patient‚Äôs oncologic timeline Suggests evidence-based treatment pathways using integrated knowledge bases Impact: Saves time preparing for multi-disciplinary meetings Ensures consistent and comprehensive reviews Enables decision support and documentation automation ‚úÖ Summary: Why Clinical NLP \u0026amp; GenAI Are Growing # Factor Description üß† Untapped Data 80% of EHR is free text ‚Äî highly valuable, underused üîì LLM Capabilities GPT, BioGPT, etc., can extract, summarize, reason üõ† Tooling Libraries and APIs make NLP workflows easier to deploy üè• Clinical Demand Ambient documentation, summarization, triage tools ‚öñÔ∏è Market Forces Reimbursement, policy, burnout, and value-based care üìä Research Fuel Rich open datasets (MIMIC), benchmarks, HuggingFace "},{"id":21,"href":"/ai-workflows/nlp-llm-genai/5-day-genai-google/day1_foundational_llm_text_generation/","title":"Day1 Foundational Llm Text Generation","section":"5-Day GenAI with Google","content":" Day 1 - Foundational LLMs \u0026amp; Text Generation ‚Äì CoT Summary # Foundations of LLMs # 1. Why Language Models Matter # We start with the need for understanding and generating human language. Traditional NLP systems were narrow, but Large Language Models (LLMs) offer general-purpose capabilities like translation, Q\u0026amp;A, and summarization‚Äîall without explicit task-specific programming.\n‚Üí how do LLMs work under the hood?\n2. What Powers LLMs: The Transformer # The Transformer is the core architecture enabling LLMs. Unlike RNNs that process data sequentially, Transformers handle inputs in parallel using self-attention, allowing them to model long-range dependencies more efficiently and scale training.\nBut to understand how Transformers process input, we need to examine how input data is prepared.\n3. Input Preparation \u0026amp; Embedding # Before data enters the transformer, it‚Äôs tokenized, embedded into high-dimensional vectors, and enhanced with positional encodings to preserve word order. These embeddings become the input that feeds into attention mechanisms.\nSo, once we have these embeddings‚Äîhow does the model understand relationships within the input?\n4. Self-Attention and Multi-Head Attention # The self-attention mechanism calculates how each word relates to every other word. Multi-head attention expands on this by letting the model attend to different relationships in parallel (e.g., syntax, co-reference). This enables rich, contextual understanding.\nTo manage this complexity across layers, the architecture needs stabilization techniques.\n5. Layer Normalization and Residual Connections # To avoid training instability and gradient issues, Transformers use residual connections and layer normalization, ensuring smooth learning across deep layers.\nAfter stabilizing, each layer further transforms the data with an extra module‚Ä¶\n6. Feedforward Layers # Each token\u0026rsquo;s representation is independently refined using position-wise feedforward networks that add depth and non-linearity‚Äîenhancing the model‚Äôs ability to capture abstract patterns.\nWith these components, we now have building blocks for the full Transformer structure.\n7. Encoder-Decoder Architecture # In the original Transformer, the encoder turns input text into a contextual representation, and the decoder autoregressively generates output using that context. However, modern LLMs like GPT simplify this by using decoder-only models for direct generation.\nAs LLMs scale, new architectures emerge to improve efficiency and specialization.\n8. Mixture of Experts (MoE) # MoE architectures use specialized sub-models (experts) activated selectively via a gating mechanism. This allows LLMs to scale massively while using only a portion of the model per input‚Äîenabling high performance with lower cost.\nBut performance isn‚Äôt just about architecture‚Äîreasoning capabilities are equally vital.\n9. Building Reasoning into LLMs # Reasoning is enabled via multiple strategies:\nChain-of-Thought prompting: Guide the model to generate intermediate steps. Tree-of-Thoughts: Explore reasoning paths via branching. Least-to-Most: Build up from simpler subproblems. Fine-tuning on reasoning datasets and RLHF further optimize for correctness and coherence. To train these reasoning patterns, we need carefully prepared data and efficient training pipelines.\n10. Training the Transformer # Training involves:\nData preparation: Clean, tokenize, and build vocabulary. Loss calculation: Compare outputs to targets using cross-entropy. Backpropagation: Update weights with optimizers (e.g., Adam). Depending on the architecture, training objectives differ.\n11. Model-Specific Training Strategies # Decoder-only (e.g., GPT): Predict next token from prior sequence. Encoder-only (e.g., BERT): Mask tokens and reconstruct. Encoder-decoder (e.g., T5): Learn input-to-output mapping for tasks like translation or summarization. Training quality is influenced by context length‚Äîlonger context allows better modeling of dependencies, but at higher compute cost.\n‚úÖ Closing the Loop # The document sets a complete foundation‚Äîfrom why LLMs matter, to how they work, to how they are trained and made more intelligent through architectures like MoE and reasoning techniques. It ends by showing how these foundational elements culminate in real-world applications: from summarization to code generation to multimodal AI.\nEvolution of LLM Architectures # 12. The Evolution Begins: From Attention to Transformers # It started with the 2017 \u0026ldquo;Attention Is All You Need\u0026rdquo; paper‚Äîlaying the groundwork for all Transformer-based models. This sparked a sequence of breakthroughs in model architecture and training methods.\n13. GPT-1: Unsupervised Pre-training Breakthrough # GPT-1 was a decoder-only model trained on BooksCorpus with a pioneering strategy: pre-train on unlabeled data, fine-tune on supervised tasks. This showed that unsupervised learning scales better than purely supervised models and inspired the unified transformer approach.\n14. BERT: Deep Understanding through Masking # BERT introduced the encoder-only model that trains on masked tokens and sentence relationships. Unlike GPT, it‚Äôs not a generator but an understander, excelling in classification, NLU, and inference tasks.\n15. GPT-2: Scaling Up Leads to Zero-Shot Learning # By expanding to 1.5B parameters and using a diverse dataset (WebText), GPT-2 revealed that larger models can generalize better, even to unseen tasks‚Äîzero-shot prompting emerged as a surprising capability.\n16. GPT-3 to GPT-4: Generalist Reasoners with Instruction-Tuning # GPT-3 scaled to 175B parameters and needed no fine-tuning for many tasks. Later versions (GPT-3.5, GPT-4) added coding, longer context windows, multimodal inputs, and improved instruction following via InstructGPT and RLHF.\n17. LaMDA: Dialogue-Focused Language Modeling # Google‚Äôs LaMDA was purpose-built for open-ended conversations, emphasizing turn-based flow and topic diversity‚Äîunlike GPT, which handled more general tasks.\n18. Gopher: Bigger is Smarter (Sometimes) # DeepMind‚Äôs Gopher used high-quality MassiveText data. It scaled to 280B parameters and showed that size improves knowledge-intensive tasks, but not always reasoning‚Äîhinting at the importance of data quality and task balance.\n19. GLaM: Efficient Scaling with Mixture-of-Experts # GLaM pioneered sparse activation, using only parts of a trillion-parameter network per input. It demonstrated that MoE architectures can outperform dense ones using far less compute.\n20. Chinchilla: The Scaling Laws Revolution # Chinchilla showed that previous scaling laws (Kaplan et al.) were suboptimal. DeepMind proved that data-to-parameter ratio matters‚Äîa smaller model trained on more data can outperform much larger ones.\n21. PaLM and PaLM 2: Distributed and Smarter # PaLM (540B) used Google\u0026rsquo;s TPU Pathways for efficient large-scale training. PaLM 2 reduced parameters but improved performance via architectural tweaks, showcasing that smarter design beats brute force.\n22. Gemini Family: Multimodal, Efficient, and Scalable # Gemini models support text, images, audio, and video inputs. Key innovations include:\nMixture-of-experts backbone Context windows up to 10M tokens (Gemini 1.5 Pro) Versions for cloud (Pro), mobile (Nano), and ultra-scale inference (Ultra) Gemini 2.0 Flash enables fast, explainable reasoning for science/math tasks. 23. Gemma: Open-Sourced and Lightweight # Built on Gemini tech, Gemma models are optimized for accessibility. The 2B and 27B variants balance performance and efficiency, with Gemma 3 offering 128K token windows and 140-language support.\n24. LLaMA Series: Meta‚Äôs Open Challenger # Meta‚Äôs LLaMA models evolved with increased context length and safety. LLaMA 2 introduced chat-optimized variants; LLaMA 3.2 added multilingual and visual capabilities with quantization for on-device use.\n25. Mixtral: Sparse Experts and Open Access # Mistral AI‚Äôs Mixtral 8x7B uses sparse MoE with only 13B active params per token, excelling in code and long-context tasks. Instruction-tuned variants rival closed-source models.\n26. OpenAI O1: Internal Chain-of-Thought # OpenAI‚Äôs ‚Äúo1‚Äù models use deliberate internal CoT reasoning to excel in programming, science, and Olympiad-level tasks, aiming for thoughtful, high-accuracy outputs.\n27. DeepSeek: RL Without Labels # DeepSeek-R1 uses pure reinforcement learning without labeled data. Their GRPO method enables self-supervised reasoning with rejection sampling and multi-stage fine-tuning, matching ‚Äúo1‚Äù performance.\n28. The Open Frontier # Multiple open models are pushing the boundaries:\nQwen 1.5 (Alibaba): up to 72B params, strong multilingual support. Yi (01.AI): 3.1T token dataset, 200k context length, vision support. Grok 3 (xAI): 1M context tokens, trained with RL for strategic reasoning. 29. Comparing the Giants # Transformer models have scaled in size, context, and capability. From 117M to 1T+ parameters, from 512-token limits to 10M-token contexts. Key insights:\nBigger is not always better‚Äîefficiency, data quality, and training methods matter more. Reasoning and instruction-following are now central. Multimodality and retrieval-augmented generation are shaping next-gen LLMs. Fine-Tuning and Using LLMs # 30. From Pretraining to Specialization: Why Fine-Tune? # LLMs are pretrained on broad data to learn general language patterns. But for real-world use, we often need them to follow specific instructions, engage in safe dialogues, or behave reliably. This is where fine-tuning comes in.\n31. Supervised Fine-Tuning (SFT): The First Specialization Step # SFT improves LLM behavior using high-quality labeled datasets. Typical goals:\nBetter instruction-following Multi-turn dialogue (chat) Safer, less toxic outputs Example formats: Q\u0026amp;A, summarization, translations‚Äîeach with clear input-output training pairs.\n32. Reinforcement Learning from Human Feedback (RLHF) # SFT gives positive examples. But what about discouraging bad outputs? RLHF introduces a reward model trained on human preferences, which then helps guide the LLM via reinforcement learning to:\nPrefer helpful, safe, and fair responses Avoid toxic or misleading completions Advanced variants include RLAIF (AI feedback) and DPO (direct preference optimization) to reduce reliance on human labels.\n33. Parameter Efficient Fine-Tuning (PEFT): Adapting Without Full Retraining # Full fine-tuning is costly. PEFT methods train small, targeted modules instead:\nAdapters: Mini-modules injected into LLM layers, trained separately LoRA: Low-rank matrices update original weights efficiently QLoRA: Quantized LoRA for even lower memory Soft Prompting: Trainable vectors (not full prompts) condition the frozen model PEFT enables plug-and-play modules across tasks, saving memory and time.\n34. Fine-Tuning in Practice (Code Example) # Google Cloud\u0026rsquo;s Vertex AI supports SFT using Gemini models with JSONL datasets and APIs. A few lines of code initialize the model, start fine-tuning, and use the new endpoint‚Äîall on cloud infrastructure.\n35. Using LLMs Effectively: Prompt Engineering # LLMs respond differently based on how you ask:\nZero-shot: Just the instruction Few-shot: Add 2‚Äì5 examples Chain-of-thought: Show step-by-step reasoning Effective prompting is key to controlling tone, factuality, or creativity.\n36. Sampling Techniques: Controlling Output Style # After generating probabilities, sampling chooses the next token:\nGreedy: Always highest prob (safe but repetitive) Random/Temperature: More creativity Top-K / Top-P: Add diversity while maintaining focus Best-of-N: Generate multiple candidates, choose best Choose based on your goal: safety, creativity, or logic.\n37. Task-Based Evaluation: Beyond Accuracy # As LLMs become foundational platforms, reliable evaluation is critical:\nCustom datasets: Reflect real production use System-level context: Include RAG and workflows, not just model Multi-dimensional ‚Äúgood‚Äù: Not just matching ground truth but business outcomes 38. Evaluation Methods # Traditional metrics: Fast but rigid Human evaluation: Gold standard, but costly LLM-powered autoraters: Scalable evaluations with rubrics, rationales, and subtasks Meta-evaluation calibrates autoraters to human preferences‚Äîessential for trust.\nConclusion # This section links training, fine-tuning, and usage of LLMs in a production-ready loop:\nTrain generally ‚Üí fine-tune specifically Prompt smartly ‚Üí sample selectively Evaluate robustly Together, these techniques ensure LLMs are accurate, safe, helpful, and aligned with real-world needs.\nAccelerating Inference in LLMs # 39. Scaling vs Efficiency: Why Speed Matters Now # LLMs have grown 1000x in parameter count. While quality has improved, cost and latency of inference have also skyrocketed. Developers now face an essential tradeoff: balancing performance with resource efficiency for real-world deployments.\n40. The Big Tradeoffs # a. Quality vs Latency/Cost # Sacrifice a bit of quality for big speed gains (e.g., smaller models, quantization). Works well for simpler tasks where top-tier quality isn\u0026rsquo;t needed. b. Latency vs Cost (Throughput) # Trade speed for bulk efficiency (or vice versa). Useful in scenarios like chatbots (low latency) vs offline processing (high throughput). 41. Output-Approximating Methods # These techniques may slightly affect output quality, but yield major gains in performance.\nüîπ Quantization # Reduce weight/activation precision (e.g., 32-bit ‚Üí 8-bit). Saves memory and accelerates math operations. Some quality loss, but often negligible with tuning. üîπ Distillation # Use a smaller student model trained to mimic a larger teacher model. Techniques: Data distillation: Generate synthetic data with teacher. Knowledge distillation: Match student output distributions. On-policy distillation: Reinforcement learning feedback per token. 42. Output-Preserving Methods # These do not degrade quality and should be prioritized.\nüîπ Flash Attention # Optimizes memory movement during attention. 2‚Äì4x latency improvement with exact same output. üîπ Prefix Caching # Cache attention computations (KV Cache) for unchanged inputs. Ideal for chat histories or uploaded documents across multiple queries. üîπ Speculative Decoding # A small \u0026ldquo;drafter\u0026rdquo; model predicts tokens ahead. Main model verifies in parallel. Huge speed-up with no quality loss, if drafter is well aligned. 43. Batching and Parallelization # Beyond ML-specific tricks, use general system-level methods:\nBatching: Handle multiple decode requests at once. Parallelization: Distribute heavy compute ops across TPUs/GPUs. Decode is memory-bound and can benefit from parallel batching as long as memory limits aren‚Äôt exceeded.\nSummary # Inference optimization is about smarter engineering, not just faster chips. You can:\nTrade off quality when it‚Äôs safe. Preserve output via caching and algorithmic improvements. Use hybrid setups like speculative decoding + batching. Choose methods based on your task: low-latency chat, high-volume pipelines, or edge deployment. Speed and cost matter‚Äîespecially at scale.\nApplications and Outlook # 44. LLMs in Action: Real-World Applications # After mastering training, inference, and prompting, the final step is applying LLMs to real tasks. These models have transformed how we interact with information across modalities‚Äîtext, code, images, audio, and video.\n43. Core Text-Based Applications # üîπ Code and Mathematics # LLMs support:\nCode generation, completion, debugging, refactoring Test case and documentation generation Language translation between programming languages Tools like AlphaCode 2, FunSearch, and AlphaGeometry push competitive coding and theorem solving to new heights. üîπ Machine Translation # LLMs understand idioms and context:\nChat translations in apps Culturally-aware e-commerce descriptions Voice translations in travel apps üîπ Text Summarization # Use cases:\nSummarizing news with tone Creating abstracts for scientific research Thread summaries in chat apps üîπ Question-Answering # LLMs reason through queries with:\nPersonalization (e.g. in customer support) Depth (e.g. in academic platforms) RAG-enhanced factuality and improved prompts üîπ Chatbots # Unlike rule-based bots, LLMs handle:\nFashion + support on retail sites Sentiment-aware entertainment moderation üîπ Content Generation # Ads, marketing, blogs, scriptwriting Use creativity-vs-correctness sampling tuning üîπ Natural Language Inference # Legal analysis, diagnosis, sentiment detection LLMs bridge subtle context to derive conclusions üîπ Text Classification # Spam detection, news topic tagging Feedback triage, model scoring as \u0026ldquo;autoraters\u0026rdquo; üîπ Text Analysis # Market trends from social media Thematic and character analysis in literature 44. Multimodal Applications # Beyond text, multimodal LLMs analyze and generate across data types:\nCreative: Narrate stories from images or video Educational: Personalized visual+audio content Business: Chatbots using both image+text inputs Medical: Scans + notes = richer diagnostics Research: Drug discovery using cross-data fusion Multimodal systems build on unimodal strengths, scaling to more sensory and intelligent interactions.\nSummary # Transformer is the backbone of modern LLMs. Model performance depends on size and training data diversity. Fine-tuning strategies like SFT, RLHF, and safety tuning personalize models for real-world needs. Inference optimization is critical‚Äîuse PEFT, Flash Attention, prefix caching, and speculative decoding. Prompt engineering and sampling tuning matter for precision or creativity. Applications are exploding‚Äîtext, code, chat, multimodal interfaces. LLMs are not just tools‚Äîthey\u0026rsquo;re platforms. They‚Äôre reshaping how we search, chat, learn, create, and discover.\n"},{"id":22,"href":"/ai-workflows/nlp-llm-genai/5-day-genai-google/day1_prompt_engineering/","title":"Day1 Prompt Engineering","section":"5-Day GenAI with Google","content":" Day 1 ‚Äì Prompt Engineering ‚Äì CoT Summary # Introduction \u0026amp; Prompting Fundamentals # 1. Why Prompt Engineering Matters # We start with the need for controlling LLM behavior. Although everyone can write prompts, crafting high-quality prompts is complex. The model, structure, tone, and context all affect the outcome. Prompt engineering is an iterative process requiring optimization and experimentation.\n‚Üí how do we guide LLMs effectively without retraining them?\n2. How LLMs Predict Text # LLMs are token prediction machines. They predict the next likely token based on previous tokens and training data. Prompt engineering means designing inputs that lead the model toward the desired outputs using this prediction mechanism.\n‚Üí how do prompt structure and context impact token prediction?\nLLM Output Configuration # 3. Controlling Output Length # Setting the number of output tokens affects cost, latency, and completeness. Shorter outputs don‚Äôt make the model more concise‚Äîthey just truncate the output. Prompts must be adapted accordingly.\n‚Üí how do we engineer prompts to work well with shorter output limits?\n4. Temperature ‚Äì Controlling Randomness # Temperature tunes the creativity vs. determinism of responses. Lower = more deterministic. Higher = more diverse. Temperature = 0 means always selecting the highest-probability token.\n‚Üí what randomness level best matches your use case: precision or creativity?\nSampling Techniques: Top-K and Top-P # 5. Top-K vs. Top-P # Top-K selects from the K most likely tokens. Top-P includes tokens whose cumulative probability is under P. Together with temperature, they control diversity. Improper values can lead to repetition loops or incoherence.\n‚Üí what is the optimal balance between relevance and novelty?\n6. Putting Sampling Together # The sampling settings interact:\nTemp=0 overrides others (most probable token only) Top-K=1 behaves similarly (greedy decoding) At extremes, sampling settings may cancel out or be ignored ‚Üí how do we experiment with sampling settings to avoid repetition and improve quality?\nPrompting Techniques # 7. Zero-shot Prompting # The simplest form‚Äîjust a task or question without examples. Effective when LLMs are pre-trained well. Clarity in phrasing is key.\n‚Üí how do we design zero-shot prompts that still get structured, accurate answers?\nPrompting Techniques Continued # 8. One-shot and Few-shot Prompting # One-shot: One example is provided before the prompt. Few-shot: Multiple examples guide the model‚Äôs pattern recognition. Ideal for steering structure and increasing precision. ‚Üí how many examples are enough for complex or high-variance tasks?\n9. System Prompting # Defines the LLM‚Äôs role and constraints at a high level‚Äîsuch as format, safety rules, or output requirements. It‚Äôs useful to enforce style, format, or structure like JSON outputs.\n‚Üí how can we use system prompts to enforce safety and structured outputs?\n10. Role Prompting # Assigns a persona (e.g., travel guide, teacher). This helps shape tone, depth, and relevance of the response. Adding style (humorous, formal) further guides model behavior.\n‚Üí how do personas influence LLM outputs in nuanced ways?\n11. Contextual Prompting # Injects situational context into the prompt to make responses more accurate. Effective for dynamic environments (e.g., blogs, customer support).\n‚Üí how can contextual prompts adapt to real-time or user-specific tasks?\nReasoning Techniques # 12. Step-back Prompting # Starts with a general question to activate background knowledge before solving a specific task. Encourages critical thinking and can reduce bias.\n‚Üí how do we leverage LLMs\u0026rsquo; latent knowledge more strategically?\nReasoning Techniques Continued # 13. Chain of Thought (CoT) # LLMs struggle with math and logic unless they break problems into steps. Chain of Thought (CoT) prompting makes the model reason step by step. This adds interpretability, reduces drift across models, and improves answer accuracy.\n‚Üí how can we make reasoning visible to both developers and users?\n14. Self-Consistency # Instead of relying on a single reasoning path, Self-Consistency samples multiple responses (with higher temperature), then picks the majority answer. It increases reliability‚Äîespecially for ambiguous or hard-to-evaluate tasks.\n‚Üí how can we trade off cost for improved reliability in decision-critical tasks?\nAdvanced Prompting Patterns # 15. Tree of Thoughts (ToT) # Generalizes CoT by enabling multiple reasoning paths at once. Like a decision tree, the model explores and evaluates different intermediate steps before selecting the best route. Powerful for complex planning and exploration.\n‚Üí what‚Äôs the best way to structure exploration and backtracking in LLM reasoning?\n16. ReAct (Reason + Act) # ReAct prompts mix reasoning with external tool calls (e.g., Google Search). It creates a loop: Think ‚Üí Act ‚Üí Observe ‚Üí Rethink. This enables LLMs to handle multi-step problems using real-world data or APIs.\n‚Üí how can we design LLMs that interactively use tools and adapt in real time?\nAutomating Prompt Engineering # 17. Automatic Prompt Engineering (APE) # Prompts are hard to write. APE uses LLMs to generate and refine their own prompts. You ask the model to create N variations of a task prompt, then rank and select the best one based on performance metrics (e.g., BLEU, ROUGE).\nFinal insight: what happens when LLMs become their own prompt engineers‚Äîand how can we guide that process safely?\nCode Prompting Techniques # 18. Prompts for Writing Code # LLMs like Gemini can generate well-documented scripts, e.g., renaming files with Bash. It reduces developer overhead for common tasks. Prompts should include goal, language, and behavior clearly.\n‚Üí how do we craft prompts that result in reusable, safe, and tested code?\n19. Prompts for Explaining Code # LLMs can reverse-engineer logic from code. Useful in team settings or code reviews. Helps onboard new developers or document legacy scripts.\n‚Üí how do we evaluate explanation correctness‚Äîespecially for critical systems?\n20. Prompts for Translating Code # Language models can convert code between languages (e.g., Bash ‚Üí Python). This helps modernize or modularize projects while preserving logic.\n‚Üí what risks emerge in translation‚Äîsyntax, dependencies, or behavior drift?\n21. Prompts for Debugging and Reviewing Code # Prompting LLMs to diagnose bugs or suggest improvements enhances development speed. Common mistakes like undefined functions can be spotted easily.\n‚Üí how do we ensure debugging prompts scale with complex codebases?\nOther Techniques \u0026amp; Best Practices # 22. Multimodal Prompting # Combines inputs like text, images, and audio. Enables more flexible, human-like interaction. Useful in complex workflows or accessibility tasks.\n‚Üí how do we design for alignment across different input modalities?\n23. Best Practice ‚Äì Provide Examples # Incorporating examples (one-shot or few-shot) within prompts is the most reliable way to guide output. Acts as a template and sets style/tone expectations.\nFinal reflection: prompt engineering is more than writing‚Äîit\u0026rsquo;s designing a user interface for the LLM.\nBest Practices for Prompt Engineering # 24. Design with Simplicity # Clear, concise prompts yield better responses. Avoid overloading with context or ambiguous language. Use active verbs and break down complex requests.\n‚Üí how do we optimize for both human and machine comprehension in prompt design?\n25. Be Specific About Output # Specify the desired format, length, tone, and structure to reduce ambiguity and improve relevance. Instructions guide better than vague constraints.\n‚Üí how can we use instructions to improve precision without overconstraining the model?\n26. Use Variables # Abstract prompts using variables (e.g., {city}) to enhance reusability in apps or RAG systems. Helps modularize and scale prompt templates.\n‚Üí how can prompt modularity boost automation and maintainability?\n27. Experiment with Formats and Styles # Vary phrasing‚Äîquestions, statements, or instructions. Try structured outputs (e.g., JSON) and track results for consistency and quality.\n‚Üí how do output formats affect hallucination, readability, and parse-ability?\nAdvanced Prompting \u0026amp; CoT Guidelines # 28. Work With Schemas # Use structured input/output formats like JSON Schema to guide the model‚Äôs understanding. This enables field-level alignment and supports reasoning over attributes.\n‚Üí how do schemas improve accuracy in structured reasoning tasks like RAG or product gen?\n29. Best Practices for Chain-of-Thought (CoT) # Always place the final answer after reasoning Use temperature=0 for deterministic outputs Separate reasoning from the final output for evals ‚Üí how do we reliably extract and score CoT answers in evaluation pipelines?\nPrompt Documentation \u0026amp; Summary # 30. Document Prompt Experiments # Track prompt versions, models, sampling settings, and outputs using a table or Google Sheet. Log RAG details and system changes to trace variation.\nFinal principle: prompting is experimental‚Äîtrack what you try, and improve iteratively.\nSummary # Day 1 covered the full spectrum of prompt engineering:\nPrompt types: zero/few-shot, role, system, contextual Reasoning: CoT, ToT, ReAct, Self-consistency, Step-back Automation: APE Code prompting: generate, translate, debug Multimodal and schema-guided prompting Best practices: output format, variables, simplicity, documentation Core Insight: Prompting is a design discipline‚Äîblending clarity, creativity, logic, and iteration to unlock the full power of LLMs.\nFinal Best Practices and Wrap-Up # 24. Design with Simplicity # Use concise language and clear goals. Overly complex or vague prompts confuse both the user and the model.\n‚Üí how do we turn messy input into structured guidance for LLMs?\n25. Be Specific About Output # Specificity in instructions (e.g., format, style, length) yields more relevant, focused outputs.\n‚Üí how do we align prompts with precise user needs and formats?\n26. Prefer Instructions Over Constraints # Positive instructions are more intuitive than a list of ‚Äúdon‚Äôts.‚Äù Use constraints when safety or exact formatting is critical.\n‚Üí how do we encourage creativity while staying on target?\nVariables, Versions, and Formats # 27. Use Variables in Prompts # Dynamic placeholders (e.g., {city}) improve reusability and maintainability‚Äîespecially in production pipelines.\n‚Üí how do we modularize prompt logic for reuse across systems?\n28. Experiment with Input and Output Formats # Try different styles‚Äîquestion, statement, instruction‚Äîand output formats like JSON. JSON helps structure data and reduce hallucinations.\n‚Üí how do we balance human readability with system parsing needs?\nPrompt Engineering Discipline # 29. Use Schemas for Input and Output # JSON schemas define structure and types‚Äîgreat for grounding LLM understanding and making output usable in applications.\n‚Üí how can we give LLMs structured \u0026ldquo;expectations\u0026rdquo; to reduce drift?\n30. Document Prompt Versions # Track all attempts, model versions, and outcomes in structured logs. Helps with reproducibility, debugging, and future upgrades.\nFinal insight: prompt engineering is iterative design‚Äîevery prompt is a versioned artifact.\nSummary # This module explored the full scope of prompt engineering:\nPrompt types (zero-shot to step-back) Reasoning methods (CoT, ToT, Self-consistency, ReAct) Automated methods (APE) Code, multimodal, and structured prompting Evaluation, formatting, variables, and documentation The journey from text to structured reasoning begins with a well-crafted prompt.\n"},{"id":23,"href":"/ai-workflows/nlp-llm-genai/5-day-genai-google/day2_embeddings_vectordb/","title":"Day2 Embeddings Vector Db","section":"5-Day GenAI with Google","content":" Day 2 ‚Äì Embeddings \u0026amp; Vector Databases ‚Äì CoT Summary # Introduction # 1. Why Embeddings? # We begin with the core problem of representing diverse data types. Images, text, audio, and structured data all need to be compared, retrieved, and clustered. Embeddings map these into a shared vector space where similarity can be computed numerically.\n‚Üí how can we measure and preserve semantic meaning across different data types?\nEmbedding Design and Intuition # 2. Mapping Data to Vector Space # Embeddings reduce dimensionality while preserving meaning. For example, just like latitude and longitude embed Earth‚Äôs surface into 2D coordinates, BERT embeds text into 768D space. Distances represent semantic similarity.\n‚Üí how do different embedding models impact representation fidelity and downstream performance?\n3. Key Applications # Embeddings power:\nSearch (e.g., RAG, internet-scale) Recommendations Fraud detection Multimodal integration (e.g., text + image) ‚Üí how do we design joint embeddings for multi-modal tasks?\nEvaluating Embeddings # 4. Quality Metrics # Evaluation focuses on how well embeddings retrieve similar items:\nPrecision@k: Are top results relevant? Recall@k: Do we get all relevant items? nDCG: Are the most relevant ranked highest? ‚Üí how can evaluation help us improve and select embedding models for specific applications?\nEmbeddings in Retrieval # 5. RAG and Semantic Search # A standard setup involves:\nEmbedding documents and queries via a dual encoder Storing doc embeddings in a vector DB (e.g., Faiss) At query time, embedding the question and retrieving nearest neighbors Feeding results into an LLM for synthesis ‚Üí how does the embedding model choice impact the quality of LLM-augmented answers?\n6. Operational Considerations # Embedding models keep improving (e.g., BEIR from 10.6 to 55.7). Choose platforms that:\nAbstract away model versioning Enable easy re-evaluation Provide upgrade paths (e.g., Vertex AI APIs) ‚Üí how do we future-proof embedding systems in production?\nTypes of Embeddings # 7. Text Embedding Lifecycle # From raw strings to embedded vectors:\nTokenization ‚Üí Token IDs ‚Üí Optional one-hot encoding ‚Üí Dense embeddings Traditional one-hot lacks semantics, embeddings retain contextual meaning ‚Üí how does token context influence the quality of embeddings?\n8. Word Embeddings: GloVe, Word2Vec, SWIVEL # Early methods:\nWord2Vec (CBOW, Skip-Gram): Context windows define meaning GloVe: Combines global + local word statistics using matrix factorization SWIVEL: Fast training, handles rare terms, parallelizable ‚Üí are static embeddings enough, or do we need context-aware representations?\nDocument Embeddings # 9. Shallow and Deep Models # Two major paradigms:\nBoW Models (TF-IDF, LSA, LDA): Sparse, easy to compute but lack context Doc2Vec: Introduces a learned paragraph vector ‚Üí how do we encode long-range relationships and context in documents?\n10. BERT and Beyond # BERT revolutionized document embedding with:\nDeep bi-directional transformers Pretraining on masked tokens Next-sentence prediction It powers models like Sentence-BERT, SimCSE, E5, and now Gemini-based embeddings. ‚Üí what‚Äôs the trade-off between compute cost and performance in deep embeddings?\nMultimodal Embeddings # 11. Images and Multimodal Representations # Image embeddings from CNNs or ViTs (e.g., EfficientNet) Multimodal models (e.g., ColPali) map text + image into a shared space Enables querying images via text without OCR Closing this section: what are the infrastructure needs to support scalable multimodal embedding workflows?\nStructured and Graph Embeddings # 12. Embeddings for Structured Data # Use dimensionality reduction (e.g., PCA) or learned embeddings Enable anomaly detection or classification with fewer labeled examples Especially useful when labeled data is scarce ‚Üí how can we compress structured data while retaining signal?\n13. User-Item \u0026amp; Graph Embeddings # Embed users and items into the same space for recommender systems Graph embeddings (e.g., Node2Vec, DeepWalk) capture node relationships Useful for classification, clustering, and link prediction ‚Üí how do we preserve both entity and relational meaning in embeddings?\nEmbedding Training # 14. Dual Encoder \u0026amp; Contrastive Loss # Most embeddings today use dual encoders (e.g., query/doc or text/image towers) Trained with contrastive loss to pull positives close, push negatives away Often initialized from large foundation models (e.g., BERT, Gemini) ‚Üí how do we balance generalization vs. task-specific fine-tuning?\nVector Search Fundamentals # 15. Vector vs. Keyword Search # Keyword search fails for synonyms and semantic variants Vector search embeds documents and queries, enabling ‚Äúmeaning-based‚Äù retrieval Similarity measured via cosine similarity, dot product, or Euclidean distance ‚Üí what metric and database architecture optimize for your use case?\nScalable ANN Search # 16. Efficient Nearest Neighbor Techniques # Brute force is O(N)‚Äînot viable at scale LSH hashes similar vectors into the same bucket Tree-based methods (KD-tree, Ball-tree) work for low dimensions HNSW and ScaNN handle large-scale, high-dimensional spaces efficiently ‚Üí how can we trade off speed vs. accuracy using ANN techniques?\nVector Databases \u0026amp; ANN Infrastructure # 17. What Are Vector Databases? # Built specifically to index and search embeddings Combine ANN search (e.g., ScaNN, HNSW) with metadata filtering Support hybrid search (semantic + keyword) with pre- and post-filtering ‚Üí how do we ensure low-latency, high-recall vector search at scale?\n18. Operational Considerations # Embeddings evolve over time‚Äîupdates may be costly Combine vector + keyword search for literal queries (e.g., IDs) Choose vector DBs based on workload (e.g., AlloyDB for OLTP, BigQuery for OLAP) ‚Üí how do we manage model/version drift and storage efficiency?\nEmbedding Applications # 19. Core Use Cases # Embeddings power:\nSearch \u0026amp; retrieval Semantic similarity \u0026amp; deduplication Recommendations Clustering \u0026amp; anomaly detection Few-shot classification Retrieval Augmented Generation (RAG) ‚Üí how do embeddings improve relevance and trust in LLM outputs?\nRAG and Grounded Generation # 20. Retrieval-Augmented Generation (RAG) # RAG improves factual grounding and reduces hallucinations Retrieves documents ‚Üí augments prompt ‚Üí generates answer Return sources for transparency and human/LLM coherence check ‚Üí how do we design RAG workflows for auditability and safety?\nFinal Thoughts # 21. Key Takeaways # Choose models and vector stores based on data, latency, cost, and security needs Use ScaNN or HNSW for scalable ANN Use hybrid filtering to improve search accuracy RAG is critical for grounded LLMs Closing insight: Embeddings + ANN + RAG form the foundation of trustworthy, scalable, semantic applications.\n"},{"id":24,"href":"/ai-workflows/nlp-llm-genai/5-day-genai-google/day3_generative_agents/","title":"Day3 Generative Agents","section":"5-Day GenAI with Google","content":" Day 3 ‚Äì Generative Agents ‚Äì CoT Summary # Introduction # 1. What Are Generative Agents? # We start with the definition of agents‚ÄîAI systems designed to achieve goals by perceiving their environment and taking actions using tools. Unlike static LLMs, generative agents combine models, tools, and orchestration to interact with the world dynamically.\n‚Üí what components make these agents truly autonomous and intelligent?\n2. Agent Architecture Breakdown # An agent‚Äôs architecture includes:\nA language model for decision-making. Tools to interface with the outside world (APIs, functions, data). An orchestration layer to manage memory, state, and reasoning techniques like CoT, ReAct, and ToT. ‚Üí how do we structure agents to be effective in real-world applications?\nAgentOps: Operationalizing Agents # 3. From MLOps to AgentOps # AgentOps is introduced as a specialized branch of GenAIOps focused on the deployment and reliability of agents. It inherits from DevOps, MLOps, and FMOps, and introduces concepts like prompt orchestration, memory handling, and task decomposition.\n‚Üí how do organizations build scalable, production-grade agent systems?\n4. The Role of Observability and Metrics # Agents must be measured at every level‚Äîgoal success rates, user interactions, latency, errors, and human feedback. These form the KPIs for agents and inform ongoing improvements.\n‚Üí how do we move beyond proof-of-concept to reliable agent deployment?\nEvaluation and Feedback # 5. Evaluating Agents Effectively # Agent evaluation involves more than just checking output correctness. It requires tracing decision-making, assessing reasoning, evaluating intermediate steps, and gathering structured human feedback.\n‚Üí how do we evaluate agents on logic, tool use, and usefulness holistically?\n6. Instrumentation and Traceability # Traces provide fine-grained visibility into what the agent did and why. This supports debugging and performance tuning, enabling trust and iterative refinement.\n‚Üí what observability tools are best for multi-step agent workflows?\nDeeper Agent Evaluation # 7. Assessing Core Capabilities # Before deployment, it\u0026rsquo;s vital to evaluate agent capabilities like tool use and planning. Benchmarks such as BFCL and PlanBench test these abilities, but should be supplemented with task-specific tests that reflect real use cases.\n‚Üí what public benchmarks best reflect your agent\u0026rsquo;s core capabilities?\n8. Trajectory Evaluation # Agents often follow multi-step trajectories. Evaluation should compare expected vs. actual tool use paths using metrics like exact match, in-order, any-order, precision, recall, and single-tool usage.\n‚Üí is the agent taking optimal steps‚Äîor just getting lucky in the final answer?\n9. Evaluating Final Responses # The agent\u0026rsquo;s final output must be evaluated for correctness, relevance, and tone. LLM-based autoraters are useful, but need precisely defined criteria. Human evaluators still offer the gold standard for nuanced feedback.\n‚Üí can automated evaluation alone guarantee real-world readiness?\nHuman Feedback and Evaluation Challenges # 10. Human-in-the-Loop (HITL) # Subjectivity, nuance, and real-world implications often require human review. Direct scoring, comparative evaluations, and user studies are powerful tools to validate and calibrate automated metrics.\n‚Üí when should humans intervene in the agent evaluation loop?\n11. Challenges and Future Directions # Agent evaluation is still maturing. Current challenges include limited evaluation datasets, gaps in process reasoning metrics, difficulty with multimodal outputs, and handling dynamic environments.\nThe forward-looking insight: agent evaluation is shifting toward process-based, explainable, and real-world-grounded methods.\nMulti-Agent Systems # 12. From Single to Multi-Agent Evaluation # Multi-agent systems are the next evolution in generative AI‚Äîmultiple specialized agents collaborate like a team. Evaluation must now address not just individual outputs, but also cooperation, delegation, and plan adherence.\n‚Üí how do we measure coordination, not just correctness?\n13. Architecture of Multi-Agent Systems # Agents are modular and play distinct roles‚Äîplanner, retriever, executor, evaluator. Communication, routing, tool integration, memory, and feedback loops form the backbone. These components support dynamic and resilient reasoning.\n‚Üí what enables agents to act as a system, not just individuals?\nDesign Patterns and Business Impact # 14. Multi-Agent Design Patterns # Patterns like sequential, hierarchical, collaborative, and competitive enable scalable, adaptive, and parallel agent behavior. These patterns reduce bottlenecks and improve automation for complex workflows.\n‚Üí which pattern suits your domain‚Äîassembly line, team, tournament, or council?\n15. Evaluation at Scale # Evaluating multi-agent systems includes trajectory traceability, agent coordination, agent-tool selection, and system-wide goal success. Instrumenting each step and agent ensures deeper insights.\nThe closing reflection: multi-agent systems multiply both the potential and complexity of generative agents‚Äîevaluation must evolve accordingly.\nAdvanced Architectures: Agentic RAG # 16. From RAG to Agentic RAG # Traditional RAG pipelines retrieve static chunks of knowledge for LLMs. Agentic RAG innovates by embedding retrieval agents that:\nExpand queries contextually Plan multi-step retrieval Choose data sources adaptively Validate results via evaluator agents ‚Üí how can agents actively reason during retrieval to boost response quality?\n17. Engineering Better RAG # To improve any RAG implementation:\nParse and chunk documents semantically Enrich chunks with metadata Tune embeddings or adapt search space Use fast vector search + rankers Implement grounding checks ‚Üí is your RAG problem about generation‚Äîor poor search to begin with?\nEnterprise Agents and Ecosystems # 18. The Rise of Enterprise Agents # 2025 marks the rise of two agent types:\nAssistants: Interactive, task-oriented agents like schedulers or sales aides. Automators: Background agents that observe events and autonomously act. ‚Üí how do organizations orchestrate fleets of agents across roles and workflows?\n19. Agentspace and Agent Management # Google Agentspace provides enterprise-grade infrastructure for creating, deploying, and managing secure, multimodal AI agents. With features like:\nRBAC, SSO, and data governance Blended RAG and semantic search Scalable agent orchestration and monitoring ‚Üí what‚Äôs needed to manage AI agents as a virtual team at scale?\nResearch Agents and Notebooks # 20. NotebookLM Enterprise # NotebookLM allows users to upload documents, ask questions, and synthesize insights. Enterprise features include:\nAudio summaries via TTS Semantic linking across documents Role-based access and policy integration Final insight: intelligent notebooks + agents will redefine enterprise knowledge discovery and interaction.\nFrom Agents to Contractors # 21. Contract-Adhering Agents # Prototypical agent interfaces are too vague for real-world, high-stakes environments. Introducing contractor agents enables:\nClear outcome definitions Negotiation and refinement Self-validation of deliverables Structured decomposition into subcontracts ‚Üí how can formalized contracts make agents production-ready and trustworthy?\n22. Contract Lifecycle and Execution # Contractor agents follow a lifecycle: define ‚Üí negotiate ‚Üí execute ‚Üí validate. Execution may involve multiple LLM-generated solutions and iterative self-correction until the contract is fulfilled, optimizing for quality over latency.\n‚Üí what runtime capabilities are needed for contract-based agents?\nMulti-Agent Research at Scale # 23. Co-Scientist: A Real-World Case Study # Google‚Äôs AI co-scientist system uses multi-agent collaboration to accelerate hypothesis generation and validation in scientific research. Roles include:\nData processors Hypothesis generators Validators Cross-team communicators Final reflection: multi-agent systems, when built as collaborative contractors, can extend the scientific method itself.\nAutomotive Use Case: Real-World Multi-Agent Systems # 24. Specialized Agents in the Car # The automotive domain is a natural fit for multi-agent AI. Here are key agent roles:\nNavigation Agent: Plans routes, ranks POIs, and handles traffic awareness Media Agent: Plays contextually relevant music or podcasts Messaging Agent: Drafts, edits, and sends messages hands-free Car Manual Agent: Uses RAG to answer questions about car features General Knowledge Agent: Answers follow-up queries to enhance user experience ‚Üí how do you design agent roles that align with contextual user needs?\nDesign Patterns in Action # 25. Hierarchical and Diamond Patterns # Hierarchical: A central Orchestrator routes user input to the right agent Diamond: Adds a Rephraser agent for tone/style before speaking responses aloud ‚Üí when does orchestration alone fall short‚Äîrequiring tone-sensitive agents?\n26. Peer-to-Peer and Collaborative Patterns # Peer-to-Peer: Agents hand off queries among themselves for better routing resilience Collaborative: Multiple agents contribute partial answers; a Mixer Agent synthesizes the final response ‚Üí can agents collaborate without central control to produce superior outputs?\nPattern in Depth: Mixer and Adaptive Loop # 27. Response Mixer and Safety-Critical Use # The Response Mixer evaluates and combines outputs from several agents (e.g., knowledge + tips + manual) to form a cohesive answer, especially for safety-critical queries like aquaplaning.\n‚Üí how do we ensure safety-critical information is prioritized in generative settings?\n28. Adaptive Loop Pattern # Agents refine queries iteratively to meet vague or underspecified user needs‚Äîe.g., finding a vegan Italian restaurant with fallback strategies.\nClosing insight: multi-agent architectures thrive where adaptability, refinement, and specialization are essential.\nAutomotive AI \u0026amp; Final Takeaways # 29. Real-Time Performance and Resilience # Multi-agent systems in cars prioritize on-device responsiveness for safety (e.g., climate control), while using cloud-based agents for tasks like dining suggestions. This hybrid model balances latency, capability, and robustness.\n‚Üí how do agents coordinate local vs. remote processing for safety and personalization?\nBuilding Agents at Scale # 30. Vertex AI Agent Builder # Google‚Äôs Agent Builder platform integrates secure cloud services, open-source libraries, evals, and managed runtimes for enterprise-grade agent development. Features include:\nRetrieval via Vertex AI Search or RAG Engine Secure APIs via Apigee Gemini and Model Garden access Evaluation pipelines via Vertex AI Eval Service ‚Üí what developer tools are needed to build, scale, and evaluate enterprise-ready agents?\nFinal Summary \u0026amp; Forward Outlook # 31. Key Developer Principles # AgentOps matters: memory, tools, trace, orchestration Automate evals, but combine with HITL Design multi-agent architectures for complexity and scale Improve search before Agentic RAG Use agent/tool registries to reduce chaos Prioritize security, flexibility, and developer cycles 32. Future Directions # Research will focus on:\nProcess-based and AI-assisted evaluation Agent collaboration and communication protocols Memory, adaptivity, explainability, and contracting models Final insight: the future is agentic‚Äîdevelopers must blend engineering, ops, UX, and domain logic to build next-gen intelligent systems.\n"},{"id":25,"href":"/ai-workflows/nlp-llm-genai/5-day-genai-google/day4_domainspecific_llms/","title":"Day4 Domain Specific Llms","section":"5-Day GenAI with Google","content":" Day 4 ‚Äì Domain-Specific LLMs ‚Äì CoT Summary # Introduction # 1. The Rise of Specialized LLMs # We start with the evolution of LLMs from general-purpose to domain-specific tools. This shift was driven by challenges in fields like cybersecurity and medicine, where technical language and sensitive use cases demand more than general knowledge.\n‚Üí why do general-purpose LLMs struggle in specialized domains?\nSecLM and Cybersecurity # 2. The Challenges in Cybersecurity # Cybersecurity experts face three main issues: rapidly evolving threats, repetitive manual work (toil), and a shortage of skilled talent. These bottlenecks make it hard to keep up with modern security needs.\n‚Üí how can AI reduce toil, bridge talent gaps, and counter fast-evolving threats?\n3. The Role of GenAI in Security # GenAI can assist various security personas‚Äîfrom analysts to CISOs‚Äîby translating queries, reverse-engineering code, planning remediation, and summarizing threats. This enables both automation and augmentation of expertise.\n‚Üí what kind of architecture supports this AI augmentation effectively?\nSecLM Architecture # 4. Multi-layered System Design # SecLM combines tools (top layer), a reasoning API (middle layer), and secure data sources (bottom layer). This allows for contextual responses using live data and tailored planning.\n‚Üí how do we ensure accuracy and freshness without retraining LLMs constantly?\n5. Domain-Specific Model Training # SecLM trains on open-source and licensed security content, fine-tuned for tasks like alert summarization and command analysis. It uses parameter-efficient tuning and RAG for freshness.\n‚Üí how does the system generalize to unseen tasks or environments?\nReasoning Framework # 6. Flexible Planning and Execution # SecLM decomposes broad questions (e.g., about an APT group) into steps like retrieving intel, translating to SIEM queries, and synthesizing responses‚Äîshowcasing multi-agent, tool-augmented reasoning.\n‚Üí how does this compare with general-purpose LLMs?\n7. Performance and Evaluation # Through expert evaluations and automated metrics, SecLM outperforms general models on security-specific tasks, demonstrating the need for full-stack, domain-focused platforms.\n‚Üí can this platform be generalized to other domains like health tech?\nMedLM and Health Tech # 8. Medical Q\u0026amp;A as a Grand Challenge # Medical question-answering (QA) demands deep reasoning, evolving knowledge, and accurate synthesis. LLMs like Med-PaLM show potential by answering USMLE-style questions and sourcing info from varied medical content.\n‚Üí how can we ensure these answers are trustworthy and context-aware?\n9. Opportunities for GenAI in Medicine # Use cases range from contextual Q\u0026amp;A on patient history to triaging clinician messages and real-time patient-clinician dialogue support. GenAI systems can enhance decision-making, patient engagement, and clinician efficiency.\n‚Üí what safeguards ensure these capabilities are safe, equitable, and accurate?\nDesigning Med-PaLM # 10. Human-Centric and Conversational AI # Med-PaLM aims to support flexible interaction‚Äîcombining structured clinical expertise with empathetic, human-centric dialogue. It was built to scale reasoning and bring compassion into AI-assisted medicine.\n‚Üí how do we evaluate such models beyond technical accuracy?\n11. Evaluation Frameworks for Medical LLMs # Med-PaLM uses USMLE-style exams and qualitative rubrics to assess reasoning, factuality, and harm potential. Human experts assess each dimension, comparing outputs to clinicians\u0026rsquo; answers in blinded evaluations.\n‚Üí how does Med-PaLM compare to human experts in real scenarios?\nClinical Deployment Considerations # 12. From Benchmark to Bedside # Rigorous validation is required for real-world use‚Äîstarting with retrospective studies, then prospective ones, all before interventional deployment. Past learnings (e.g., from diabetic retinopathy screening) stress this need.\n‚Üí how can we responsibly scale these tools into clinical settings?\nAdvancing MedLM # 13. Task-Specific vs. Domain-General Models # While Med-PaLM 2 shows expert-level performance on QA tasks, its capabilities must be validated across each medical subdomain. Mental health assessments, for example, require specialized evaluation and adaptation.\n‚Üí can a high-performing model generalize across all medical use cases without fine-tuning?\n14. Toward Multimodal Healthcare AI # Medicine is inherently multimodal‚Äîspanning text, images, genomics, EHRs, and sensors. MedLM is expanding into multimodal models, which are in early research stages but promise broader clinical utility.\n‚Üí how can AI integrate and reason across multiple data modalities safely and meaningfully?\n15. Training Innovations in Med-PaLM 2 # Med-PaLM 2 leverages instruction tuning on diverse QA datasets and advanced prompting strategies like chain-of-thought, self-consistency, and ensemble refinement. These enhance stepwise reasoning and output reliability.\n‚Üí which techniques most boost reasoning performance in sensitive domains like healthcare?\nConclusion # 16. Key Takeaways # LLMs show tremendous promise in solving domain-specific problems. In cybersecurity, SecLM combines tools, reasoning, and authoritative data to empower practitioners. In healthcare, MedLM and Med-PaLM show how vertical fine-tuning, evaluation, and collaboration with clinicians drive real-world impact.\nThe overarching insight: LLMs require domain-specific architecture, data, tuning, and evaluation to move from general intelligence to real-world application.\n"},{"id":26,"href":"/ai-workflows/nlp-llm-genai/5-day-genai-google/day5_mlops/","title":"Day5 Mlops","section":"5-Day GenAI with Google","content":" Day 5 ‚Äì MLOps for Generative AI ‚Äì CoT Summary # 1. Introduction # The rise of foundation models and generative AI (gen AI) has brought a paradigm shift in how we build and deploy AI systems. From selecting architectures to managing prompts and grounding outputs in real data, traditional MLOps needs adaptation.\nSo how do we evolve MLOps for this new generative world?\n2. What Are DevOps and MLOps? # DevOps: Automation + collaboration for software delivery (CI/CD, testing, reliability) MLOps: Adds ML-specific needs: Data validation Model evaluation Monitoring Experiment tracking These core principles set the stage, but gen AI has unique needs.\n3. Lifecycle of a Gen AI System # The gen AI lifecycle introduces five major moments:\nDiscover ‚Äì Find suitable foundation models from a rapidly growing model zoo. Develop \u0026amp; Experiment ‚Äì Iterate on prompts, use few-shot examples, and chains. Train/Tune ‚Äì Use parameter-efficient fine-tuning. Deploy ‚Äì Includes chains, prompt templates, databases, retrieval systems. Monitor \u0026amp; Govern ‚Äì Ensure safety, fairness, drift detection, and lineage. Each stage requires new tooling and processes compared to traditional ML.\n4. Continuous Improvement in Gen AI # Gen AI focuses on adapting pre-trained models via: Prompt tweaks Model swaps Multi-model chaining Still uses fine-tuning and human feedback loops when needed. But not all orgs handle base model training‚Äîmany just adapt existing FMs.\n5. Discover Phase: Choosing the Right FM # Why it‚Äôs hard:\nExplosion of open-source and proprietary FMs Variation in architecture, performance, licensing Model selection is now a critical MLOps task.\n6. Model Discovery Criteria # Choosing a foundation model now involves nuanced trade-offs:\nQuality: Benchmarks, output inspection Latency \u0026amp; Throughput: Real-time chat ‚â† batch summarization Maintenance: Hosted vs self-managed models Cost: Compute, serving, data storage Compliance: Licensing, regulation Vertex Model Garden supports structured exploration of these options.\n7. Develop \u0026amp; Experiment # Building gen AI systems is iterative: prompt tweaks ‚Üí model swap ‚Üí eval ‚Üí repeat.\nThis loop mirrors traditional ML but centers around prompts, not raw data.\n8. Foundation Model Paradigm # Unlike predictive models, foundation models are multi-purpose. They show emergent behavior based on prompt structure. Prompts define task type (translation, generation, reasoning). Small changes in wording can completely shift model output.\n9. Prompted Model Component # The key unit of experimentation in gen AI is: Prompt + Model ‚Üí Prompted Model Component\nThis redefines MLOps: you now track prompt templates as first-class artifacts.\n10. Prompt = Code + Data # Prompts often include:\nCode-like structures (templates, control flow, guardrails) Data-like elements (examples, contexts, user input) MLOps must version prompts, track results, and match to model versions.\n11. Chains \u0026amp; Augmentation # When prompts alone aren‚Äôt enough:\nChains: Link multiple prompted models + APIs RAG: Retrieve relevant info before generation Agents: LLMs choose tools dynamically (ReAct) MLOps must manage chains end-to-end, not just components.\n12. Chain MLOps Needs # Evaluation: Run full chains to measure behavior Versioning: Chains need config + history Monitoring: Track outputs + intermediate steps Introspection: Debug chain inputs/outputs Vertex AI + LangChain integration supports these needs.\n13. Tuning \u0026amp; Training # Some tasks require fine-tuning:\nSFT: Teach model to produce specific outputs RLHF: Use human feedback to improve alignment Tune as needed‚Äîespecially if prompt engineering hits limits.\n14. Continuous Tuning # Static tasks = low frequency. Dynamic tasks (chatbots) = frequent RLHF.\nBalance GPU/TPU cost with improvement needs Consider quantization to lower costs Vertex AI provides tuning infra + registry + pipelines + governance.\n15. Data in Gen AI # Unlike predictive ML, gen AI uses:\nPrompts \u0026amp; examples Grounding sources (APIs, vectors) Human preference data Task-specific tuning sets Synthetic + curated data Each has different MLOps needs: validation, versioning, lineage.\n16. Synthetic Data Use Cases # Generation: Fill in training gaps Correction: Flag label errors Augmentation: Introduce diversity Use large FMs to generate training or eval data when needed.\n17. Evaluation in Gen AI # Evaluation is hard:\nComplex, open-ended outputs Metrics (BLEU, ROUGE) often miss the mark Auto-evals (e.g. AutoSxS) use FMs as judges Align automated metrics with human judgment early on.\n18. Evaluation Best Practices # Stabilize metrics, approaches, datasets early Include adversarial prompts in test set Use synthetic ground truth if needed Evaluation = cornerstone of experimentation in gen AI MLOps\n19. Deployment in Gen AI Systems # Gen AI apps involve multiple components:\nLLMs Chains Prompts Adapters External APIs Two main deployment types:\nFull Gen AI Systems (custom apps) Foundation Model Deployments (standalone models) 20. Version Control # Key assets to version:\nPrompt templates Chain definitions Datasets (e.g. RAG sources) Adapter models Git, BigQuery, AlloyDB, and Vertex Feature Store help manage assets.\n21. Continuous Integration (CI) # CI ensures reliability through:\nUnit + integration tests Automated pipelines Challenges:\nTest generation is hard due to open-ended outputs Reproducibility is limited due to LLM randomness Solutions draw from earlier evaluation methods.\n22. Continuous Delivery (CD) # CD moves tested systems into staging/production.\nTwo flavors:\nBatch delivery: Schedule-driven, test pipeline throughput Online delivery: API-based, test latency, infra, scalability Chains are the new \u0026ldquo;deployment unit\u0026rdquo;‚Äînot just models.\n23. Foundation Model Deployment # Heavy resource demands ‚Üí need:\nGPU/TPU allocation Scalable data stores Optimization (distillation, quantization, pruning) 24. Infrastructure Validation # Check:\nHardware compatibility Serving configuration GPU/TPU availability Tools: TFX infra validation, manual provisioning checks\n25. Compression \u0026amp; Optimization # Strategies:\nQuantization: 32-bit ‚Üí 8-bit Pruning: Remove unneeded weights Distillation: Train small model from a larger \u0026ldquo;teacher\u0026rdquo; Step-by-step distillation can reduce size and improve performance.\n26. Deployment Checklist # Steps to productionize:\nVersion control Optimize model Containerize Define hardware and endpoints Allocate resources Secure access Monitor, log, and alert Real-time infra: Cloud Functions + Cloud Run 27. Logging \u0026amp; Monitoring # Track both:\nApp-level inputs/outputs Component-level details (chain steps, prompts, models) Needed for tracing bugs, debugging drift, and transparency.\n28. Drift \u0026amp; Skew Detection # Compare:\nEvaluation-time data vs. Production input Topics, vocab, token count, embeddings Techniques:\nMMD, least squares density, learned kernels Signals shift in user behavior or data domains.\n29. Continuous Evaluation # Capture live outputs Evaluate vs. ground truth or human feedback Track metric degradation Alert on failures or decay Production = where real testing happens.\n30. Governance # Governs:\nChains + components Prompts Data Models Evaluation metrics and lineage Full lifecycle governance = essential for compliance and maintainability.\n31. Role of an AI Platform # Vertex AI acts as an end-to-end platform for developing and operationalizing Gen AI. It supports:\nData prep Training/tuning Deployment Evaluation CI/CD Monitoring Governance It enables reuse, scalability, and full-stack observability for Gen AI teams.\n32. Model Discovery: Vertex Model Garden # Model Garden includes:\n150+ models: Google, OSS, third-party (e.g., Gemini, Claude, Llama 3, T5, Imagen) Modalities: Language, Vision, Multimodal, Speech, Video Tasks: Generation, classification, moderation, detection, etc. Each model has a card with use cases and tuning options.\n33. Prototyping: Vertex AI Studio # Vertex AI Studio offers:\nPlayground for trying models (Gemini, Codey, Imagen) UI + SDKs (Python, NodeJS, Java) Prompt testing + management One-click deploy Built-in notebooks (Colab Enterprise, Workbench) Low barrier for users from business analysts to ML engineers.\n34. Training: Full LLM Training on Vertex AI # TPU and GPU infrastructure for fast, large-scale training Vertex AI supports training from scratch and adapting open-weight models 35. Tuning: Five Key Methods # Prompt engineering ‚Äì no retraining SFT (Supervised Fine-Tuning) ‚Äì train on labeled examples RLHF ‚Äì learn from human preferences Distillation ‚Äì compress knowledge from large to small models Step-by-step distillation ‚Äì Google-optimized, fewer data needs Each method balances cost, performance, and latency.\n36. Orchestration: Vertex Pipelines # Define pipelines with Kubeflow SDK Automate tuning, evaluation, and deployment Managed pipelines for Vertex foundation models Enables production-readiness and repeatability.\n37. Chain \u0026amp; Augmentation: Grounding + Function Calling # Vertex AI supports:\nRAG systems ‚Äì real-time document retrieval Agent-based chains ‚Äì dynamic tool use via ReAct Function calling ‚Äì LLM picks which API to use, returns JSON Grounding ‚Äì verifies/model output via search or private corpora Agent Builder ‚Äì build search/chat agents grounded on any source Simplifies chaining, reasoning, and integrating internal data.\n38. Vector Search # Vertex AI Vector Search enables:\nHigh-scale, low-latency ANN search Billions of embeddings using ScaNN Use with text, images, hybrid metadata search Works with custom embeddings (e.g., textembedding-gecko) Choose this when you need control over chunking, retrieval, or models.\n39. Evaluate: Vertex AI Experiments \u0026amp; TensorBoard # Experimentation is essential for iterating and improving Gen AI models. Tools include:\nVertex AI Experiments: Track model runs, hyperparams, training environments Vertex AI TensorBoard: Visualize loss, accuracy, embeddings, model graphs Supports reproducibility, debugging, and collaboration.\n40. Evaluation Techniques # Ground truth metrics: Automatic Metrics using reference datasets LLM-based eval: Auto Side-by-Side (Auto SxS) with model judges Rapid Evaluation API: Fast SDK-based eval for prototyping Evaluation is deeply integrated into the development lifecycle.\n41. Predict: Vertex Endpoints # Deploy models to Vertex Endpoints for online prediction Features: Autoscaling Access control Monitoring Works with open-source and Google models 42. Safety, Bias, and Moderation # Built-in responsible AI features:\nCitation checkers: Track and quote data sources Safety scores: Detect harmful content and flag sensitive topics Watermarking: Identify AI-generated content (via SynthID) Bias detection: Ensure fairness and appropriateness Moderation: Filter unsafe responses These ensure ethical and trustworthy AI deployments.\n43. Governance Tools # Vertex Feature Store:\nTrack embedding + feature lineage Drift monitoring Feature reuse + formulas Model Registry:\nLifecycle tracking (versioning, evaluation, deployment) One-click deployment Access to evaluation, monitoring, and aliasing Dataplex:\nCross-product lineage (e.g., Vertex + BigQuery) Golden datasets/models Access governance + IAM integration These unify observability, reproducibility, and compliance across Gen AI assets.\n44. Conclusion # MLOps principles‚Äîreliability, scalability, repeatability‚Äîfully extend into Gen AI.\nGen AI adds prompt chaining, grounding, function calling, etc. Vertex AI unifies the full lifecycle across models, pipelines, and governance It supports both predictive and Gen AI use cases MLOps isn‚Äôt replaced‚Äîit‚Äôs expanded for the age of foundation models.\n"},{"id":27,"href":"/healthcare-domain/learning/ai-in-healthcare/c1_healthcare/toc_course1/","title":"To C Course1","section":"C1 Healthcare - Intro","content":" üì∫ Table of Contents: Video Lectures # Course 1/5: Introduction to Healthcare # Module 1: Overview of Health Care Systems and Key Challenges They Face # Introduction A Simple Interaction Between Providers and Patients The Problem of Risk Solving the Problem of Risk: Risk Pooling Insurance and Intermediaries for Risk Pooling Beyond Patients, Providers, and Intermediaries: Other Players in the Health Care System Overview of the Types and Roles of Intermediaries Overview of the Types and Roles of Providers Providers and Levels of Care The Challenge of Rising Health Care Costs The Challenges of Quality and Access Lessons for AI and Data Module 2: Physicians, Physician Practices, and Physician Payment # Characteristics of Physician Practices Physicians, Intermediaries, and Networks Fee for Service Payment Procedure Codes and Diagnosis Codes The Medicare Fee Schedule Capitation Payment Systems: Overview and Structure Capitation Payment Systems: Scope of Capitation Episode-Based Payment Systems and Salary Systems Risk Shifting in Physician Payment and Multi-Layered Physician Payment Arrangements Incentives Created by Physician Payments Lessons for AI and Data Wrap Up Module 3: Hospitals, Other Provider Organizations, and Related Payment Systems # Basic Operations and Characteristics of Hospitals How Hospitals Relate to Physicians and Intermediaries Hospital Payment Methods: Charge Masters/FFS and Per Diem Hospital Payment Methods: DRGs Hospital Payment Methods: Global Budgets Hospital Payment Topics: Payments for Inpatient vs Outpatient Services, Hospital vs Physician Payments Risk and Incentives in Hospital Payment Independent Facilities - Structure and Payment Health Care Systems and Larger Provider Organizations Pay for Performance EMRs, EHRs, and PHRs Providers, Provider Incentives, Data, and Tools Wrap Up Module 4: Intermediaries, Health Insurance Plans, and Health Care Financing # Intermediaries and their Goals Intermediaries and the Broad Challenges Facing Health Care Systems Networks and Selective Contracting Provider Payment Methods and Levels Patient Cost Sharing Utilization Review, Gatekeepers, and Other Methods of Directly Influencing Care Coverage Decisions Combinations and Tradeoffs Three Stereotypical Plan Designs: \u0026ldquo;Traditional,\u0026rdquo; HMO, and PPO Some More Recent Trends in Plan Design Public and Private Plans (and Employer-Provided Private Insurance in the U.S.) The U.S. Medicare Program The U.S. Medicaid Program Intermediaries: Lessons for Innovators Wrap Up Module 5: Health Care Products, Prescription Drugs, and Quality Measurement # Health care products, approvals, and prescription drugs Prescription Drug Approval Processes Patents, Branded Drugs, and Generic Drugs Patients, Insurance, Formularies, and Prescription Drugs Intermediaries, Pharmacy Benefit Managers, Drug Prices, and Rebates Products and Prescription Drugs Wrap Up - Data and Opportunities for Innovation Quality of Care Overview and Key Organizing Concepts Overview, and Structural Quality Measures Process Quality Measures Outcome Quality Measures and Satisfaction Measures Overview of Some Approaches to Improving Quality Innovation and Data in Quality Improvement Wrap Up Module 6: Ethics # Overview of AI applications in delivery of health care services and ethical issues Ethical frameworks for health care and for AI AI and incentives in health care delivery and payment structures More examples of AI and incentives in health care delivery and payment structures Module 7: Course Wrap Up # Course Summary "},{"id":28,"href":"/ai-workflows/nlp-llm-genai/5-day-genai-google/","title":"5-Day GenAI with Google","section":"NLP‚ÜíLLMs‚ÜíGenAI","content":" 5-Day Gen AI Intensive Course with Google ‚Äì Resource Overview # GitHub for Notebooks\nDay Topic Whitepaper Code Labs Case Study 1 Foundational LLMs \u0026amp; Prompt Engineering Foundational LLMs \u0026amp; Text Generation\nPrompt Engineering 1. Prompting Fundamentals Case Study 2 Embeddings \u0026amp; Vector Stores/Databases Embeddings 2. RAG QA System\n3. Text Similarity\n4. Classification with Keras 3 Generative Agents Agents 5. Function Calling\n6. LangGraph Agent Case Study 4 Domain-Specific LLMs Domain-Specific LLMs 7. Google Search Grounding\n8. Custom Fine-Tuning 5 MLOps for Generative AI MLOps No code labs. See: E2E Gen AI Starter Pack FAQ on Large Language Models (LLMs) and Generative AI # 1. What are the fundamental components that enable Large Language Models (LLMs) to process and generate text? # LLMs are primarily powered by the Transformer architecture. This architecture utilizes mechanisms like self-attention and multi-head attention to weigh the importance of different words in the input sequence. Input text is prepared through tokenization and embedding into vector representations. The Transformer often employs encoder and decoder components, along with techniques like layer normalization and residual connections, and in some cases, Mixture of Experts (MoE) for efficient scaling. Training these models involves feeding them vast amounts of text data and employing various strategies to optimize their ability to predict the next word or token in a sequence.\n2. How have LLM architectures evolved over time, and what key breakthroughs characterize this evolution? # The evolution began with the shift towards attention mechanisms and culminated in the Transformer. Key breakthroughs include GPT-1\u0026rsquo;s unsupervised pre-training, BERT\u0026rsquo;s deep contextual understanding through masked language modeling, GPT-2\u0026rsquo;s zero-shot learning capabilities arising from scale, and the emergence of generalist reasoners like GPT-3 and GPT-4 through instruction tuning. Other notable developments include dialogue-focused models (LaMDA), explorations of scaling laws (Chinchilla), efficient scaling with MoE (GLaM, Mixtral), the development of multimodal models (Gemini), and the rise of open-source alternatives (Gemma, LLaMA series). These advancements highlight a trend towards larger, more capable models with improved reasoning, generalization, and multimodal understanding.\n3. What are the primary techniques for adapting pre-trained LLMs for specific tasks or domains? # The main techniques for adapting LLMs include fine-tuning, which involves further training the model on a smaller, task-specific dataset. Supervised Fine-Tuning (SFT) is a common approach. Reinforcement Learning from Human Feedback (RLHF) is used to align models with human preferences. Parameter Efficient Fine-Tuning (PEFT) methods allow for adaptation with fewer trainable parameters. Effective use of LLMs also relies heavily on prompt engineering, which involves crafting specific instructions to guide the model\u0026rsquo;s output, along with selecting appropriate sampling techniques to control the style and randomness of the generated text.\n4. Why is prompt engineering crucial for effectively utilizing LLMs, and what are some key prompting techniques? # Prompt engineering is critical because it directly influences the output and behavior of LLMs. By carefully designing prompts, users can guide the model to perform specific tasks, adopt certain roles, and reason through complex problems. Key techniques include zero-shot prompting (relying solely on the prompt), one-shot and few-shot prompting (providing examples), system prompting (setting the overall context), role prompting (assigning a persona), contextual prompting (providing relevant information), and advanced reasoning techniques like Chain of Thought (CoT), Step-back Prompting, and Tree of Thoughts (ToT).\n5. What are embeddings and vector databases, and how do they facilitate advanced applications of LLMs like Retrieval-Augmented Generation (RAG)? # Embeddings are vector representations of data (text, images, etc.) that capture their semantic meaning, allowing for similarity comparisons. Vector databases are specialized databases designed to efficiently store and search these high-dimensional vector embeddings. In Retrieval-Augmented Generation (RAG), user queries are embedded and used to retrieve relevant information from a knowledge base stored as vector embeddings. This retrieved information is then incorporated into the prompt, allowing the LLM to generate more accurate and contextually grounded responses.\n6. What are generative agents, and what considerations are important when developing and evaluating them, particularly in multi-agent systems? # Generative agents are autonomous entities powered by LLMs that can perceive their environment, make decisions, and take actions. Their architecture typically involves components for planning, memory, and action execution. Operationalizing agents (AgentOps) requires attention to observability and metrics. Evaluation involves assessing core capabilities, the trajectory of agent behavior, and the quality of final responses, often incorporating human feedback. In multi-agent systems, evaluating the interactions and coordination between agents becomes crucial, and specialized architectures and design patterns are employed.\n7. How are domain-specific LLMs being developed and applied in fields like cybersecurity (SecLM) and healthcare (MedLM)? # Domain-specific LLMs are created by training models on large datasets specific to a particular domain, often combined with general pre-training. SecLM for cybersecurity aims to assist with tasks like threat detection and analysis by understanding security-related language and concepts. MedLM in healthcare focuses on medical knowledge and reasoning, with applications in medical Q\u0026amp;A, diagnosis support, and clinical documentation. The development of these models requires careful consideration of domain-specific challenges, such as data privacy and the need for high accuracy, as well as specialized evaluation frameworks and deployment considerations.\n8. What are the key aspects of MLOps for Generative AI systems, and how does it differ from traditional MLOps? # MLOps for Generative AI addresses the lifecycle of these complex systems, including model discovery, development, tuning, deployment, monitoring, and governance. It shares core principles with traditional MLOps but has unique considerations due to the nature of foundation models and prompted systems. This includes managing and versioning prompts, dealing with synthetic data, specialized evaluation techniques, the deployment of large foundation models, and the importance of continuous tuning and monitoring for drift and safety. AI platforms provide tools and infrastructure to support these GenAI-specific MLOps workflows.\n"},{"id":29,"href":"/healthcare-domain/learning/ai-in-healthcare/","title":"AI in Healthcare","section":"Learning","content":"Content for the AI In Healthcare Specialization section.\n"},{"id":30,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/ehr_model_failure_case_study/","title":"Case Study: The Hidden Danger of Correlation in Healthcare AI","section":"C3 ML Healthcare","content":" Case Study: The Hidden Danger of Correlation in Healthcare AI # üéØ The Goal # Build a machine learning model to predict which pneumonia patients might die, using historical Electronic Health Record (EHR) data. This would help doctors:\nIdentify high-risk patients who need ICU care Identify low-risk patients who could recover at home ‚úÖ What Actually Happened # The model was trained on real EHR data and found a surprising pattern:\nPneumonia patients with asthma had better outcomes.\nSo the model decided:\n‚ÄúAsthma patients must be low-risk!‚Äù\nü§î Sounds wrong, right? Asthma usually makes pneumonia worse, not better.\nüß© The Hidden Problem: A Confounding Variable # Turns out, the hospital had a policy:\nAutomatically admit asthma patients with pneumonia to the ICU for aggressive treatment.\nBecause of this:\nAsthma patients got better care Asthma patients survived more The model assumed asthma caused survival The model didn‚Äôt know about the hospital‚Äôs policy‚Äîit just saw the outcome.\n‚ö†Ô∏è Why This Is a Causality Problem # Concept Explanation Correlation ‚â† Causation The model saw a pattern, but misunderstood the cause. Confounder (Lurking Variable) The real reason for better outcomes was ICU treatment, not asthma. Model Generalization Failure In other hospitals (with no asthma policy), this model could suggest unsafe care. False Security The model passed all metrics‚Äîaccuracy, validation, etc.‚Äîbut still made a dangerous inference. üß™ Real-World Impact (If Deployed) # If this model had been used in practice:\nAsthma patients could have been flagged as low-risk Sent home or under-treated Leading to severe illness or death üîç Key Takeaways # EHR models can learn patterns from policies, biases, or coincidences. Confounding variables mislead models when context is missing. Medical expertise is essential to catch errors before deployment. Always ask: Is this pattern causal, or just correlative? ‚úÖ What Should Be Done Instead? # Include domain experts when building and validating models Investigate surprising or counterintuitive predictions Test models on external datasets Use methods like causal inference to verify relationships Remember: Hope is not a strategy. "},{"id":31,"href":"/ai-workflows/structural-reasoning/causality/causal-ai/","title":"Causal AI","section":"Causality","content":"Content for the Causal AI section.\n"},{"id":32,"href":"/ai-workflows/structural-reasoning/causality/causal-inference/","title":"Causal Inference","section":"Causality","content":"Content for the Causal Inference section.\n"},{"id":33,"href":"/healthcare-domain/learning/causal-inference-rwd/","title":"Causal Inference RWD","section":"Learning","content":"Content for the A Crash Course in Causality: Inferring Causal Effects from Observational Data section.\n"},{"id":34,"href":"/ai-workflows/structural-reasoning/causality/","title":"Causality","section":"Structural Reasoning","content":" Causal Inference (CI) = Statistical Science for Understanding Cause-Effect # Core Goal: # To estimate the causal effect of one variable on another from data (e.g., does a treatment cause better outcomes?).\nKey Characteristics: # Criteria Description Scope Primarily focuses on estimating causal effects from observational or experimental data Typical Methods Propensity scores, matching, inverse probability weighting, instrumental variables, difference-in-differences, DAGs Data Tabular, typically structured (e.g., clinical trials, EHRs, economic datasets) Toolkits DoWhy, EconML, CausalML, R, Stata, Stan Theoretical Backbone Judea Pearl‚Äôs framework (do-calculus, SCMs), Rubin\u0026rsquo;s Potential Outcomes üéØ Common Use Cases Healthcare policy evaluation, drug effect estimation, A/B testing, economic policy modeling Audience Statisticians, epidemiologists, health economists, applied researchers Causal AI = Intelligent Systems that Reason About and Use Causality # Core Goal: # To build AI systems that can reason, plan, and generalize using causal understanding ‚Äî beyond pure prediction.\nKey Characteristics: # Criteria Description Scope Broader ‚Äî includes CI and building causal reasoning into AI agents, decision systems, simulations Typical Methods Causal discovery, SCMs, counterfactual reasoning, causal reinforcement learning, causal representation learning Data Includes structured, unstructured, time-series, multi-modal, even simulators AI Tools Combines causal inference + ML + planning: Pyro, DoWhy, NeurIPS CausalBench, Causal Transformers Emerging Work Counterfactual explanation for LLMs, causal structure in generative models, agent-based causal decision making Common Use Cases Building agents that can plan, explain decisions, simulate alternate futures (e.g., clinical decision AI, industrial control) Audience ML researchers, AI engineers, decision scientists, healthcare AI innovators "},{"id":35,"href":"/healthcare-domain/learning/hands-on-healthcare-data/ch4_ehr/","title":"Ch4 EHR","section":"Hands-On Healthcare Data","content":" Ch4 Deep Dive ‚Äì Electronic Health Records (EHR) - ConvSummary # üîç Q\u0026amp;A-Style Logical Summary # Q1: What is the central focus of Chapter 4? # Chapter 4 focuses on working with electronic health record (EHR) data using the MIMIC-III dataset, and explores medication harmonization using SQL, Neo4j (property graph), and TypeDB (typed hypergraph).\nQ2: What makes working with EHR data complex? # EHRs are highly structured but vary between implementations. Data is often redundant, inconsistent, or missing. Clinical context and domain knowledge are crucial for correct interpretation. Q3: Why was medication harmonization chosen as the use case? # Because medications are objective and widely used in EHRs, but the same drug can appear under multiple names or codes (e.g. NDCs). Harmonization is necessary to:\nNormalize names or codes. Filter out forms like \u0026ldquo;heparin flush\u0026rdquo; vs \u0026ldquo;therapeutic heparin\u0026rdquo;. Q4: How is this implemented using SQL (SQLite)? # CSVs are loaded into SQLite tables. Free-text string search is done across drug name columns (e.g., drug_name_generic LIKE \u0026ldquo;%heparin%\u0026rdquo;). NDC-based harmonization is done by: Extracting distinct NDCs. Filtering to include only valid/therapeutic ones. Rewriting queries using explicit WHERE ndc IN (...) clauses. Q5: What are the challenges of the SQL approach? # Hard to maintain mappings (e.g., repeated NDC lists). Poor separation of concerns (clinical logic leaks into queries). Not reusable across projects or datasets. Q6: How does Neo4j (property graph) improve the workflow? # Drug instances are modeled as nodes, and a new \u0026ldquo;heparin (non-flush)\u0026rdquo; concept is created. Relationships connect drug nodes to patients and prescriptions. Reusability improves because mappings are stored inside the graph. Queries become more semantic, e.g., follow a concept node rather than duplicating NDCs in every query. Q7: How are concepts created in Neo4j? # A node like Drug:Knowledge { drug: \u0026quot;Heparin (non-flush)\u0026quot;, ... } is created. Drug nodes are linked to it via [:for_drug {derived: true}] relationships. This enables querying \u0026ldquo;all patients on this drug concept.\u0026rdquo; Q8: How does TypeDB enhance this model further? # TypeDB uses strong typing and roles to model relations: patient plays prescription:patient druginstance plays prescription:prescribed_drug Separate druginstance entity and prescription relation. Can add inference rules to dynamically associate drugs with higher-level concepts. Q9: What are the two approaches to harmonization in TypeDB? # Persisted: Insert actual hierarchy facts: (parent: heparin, child: drug) isa hierarchy. Rule-based: Use rules like: rule heparin-rule: when { $d has ndc \u0026#34;xxxx\u0026#34;; $c has purl \u0026#34;...\u0026#34;; } then { (parent: $c, child: $d) isa hierarchy; } Q10: What are the tradeoffs between approaches? # Model Pros Cons SQL Ubiquitous, accessible Poor semantics, duplication of logic Neo4j Intuitive graph model, reusable concepts Fragile schemas, performance concerns TypeDB Semantic precision, rule engine Newer ecosystem, complexity, fewer tools RDF Graph Web standard, portable Very steep learning curve Q11: What are the broader lessons? # Separate clinical knowledge from code. Choose modeling strategies based on: Project duration Tooling maturity Frequency of schema changes Collaboration needs Use graphs or TypeDB to encode reusable logic and keep queries clean. üß† Curriculum Task-Based Summary (Chapter 4) # üîπ 1. Understanding EHR Data Models # Compare OMOP, FHIR, i2b2, PCORnet, ADaM, SDTM. Explore role of implementation guides and FHIR profiles. üîπ 2. Setup and Load EHR Data (MIMIC-III) # Use SQLite to ingest .csv files (Example 4-1). Use Neo4j or TypeDB containers (Docker). Load and explore data with basic queries. üîπ 3. Medication Harmonization Use Case # Focus on heparin, and identify pitfalls with NDC codes. Extract and deduplicate NDCs from prescriptions. Build queries that target therapeutic use only. üîπ 4. Query and Harmonize in Three Paradigms # Task SQL Neo4j TypeDB Load data pandas + sqlite3 pandas + NeoInterface pandas + TypeDB client Query for \u0026ldquo;heparin\u0026rdquo; LIKE on drug names toLower(drug_name) CONTAINS drug has name contains Harmonization Filter with ndc IN (...) Create concept node + edges Insert facts or define rules üîπ 5. Linkage and Reasoning # Create custom drug concepts. Track prescriptions per patient. Link concepts using: SQL joins Neo4j (:Patient)-[:has_prescription]-\u0026gt;(:Drug) TypeDB roles + rules. üîπ 6. Evaluate Tradeoffs and Performance # Review table of pros/cons (Table 4-2). Balance: Query simplicity vs data model reusability. Rule-driven inference vs static mapping. Ecosystem maturity. ‚úÖ End of Chapter Outcome # You should now be able to:\nChoose the right data model (SQL, graph, hypergraph) for your RWD task. Implement and harmonize medication concepts. Balance engineering choices with clinical accuracy and long-term maintainability. Begin thinking about integrating terminologies (UMLS, SNOMED CT) into your models. "},{"id":36,"href":"/healthcare-domain/learning/hands-on-healthcare-data/ch6_graph_ml/","title":"Ch6 ML and Graph Analytics","section":"Hands-On Healthcare Data","content":" Ch6 Machine Learning \u0026amp; Graph-Based Analytics - ConvSummary # Part 1: Q\u0026amp;A Summary # 1. What is the difference between cleaning, harmonization, and feature engineering? # Cleaning: Removing errors or inconsistencies in the raw data. Harmonization: Mapping and aligning data semantically across datasets (e.g., converting NDC to RxNorm). Feature Engineering: Transforming data to fit the needs of specific algorithms or analysis (e.g., PCA, one-hot encoding). 2. Why are graphs more useful for harmonization than feature engineering? # Graphs help link concepts across vocabularies, terminologies, or systems. Feature engineering tends to be model-specific and harder to generalize. 3. What are the downsides of repeating cleaning/harmonization for each project? # Redundancy: Same steps are repeated across projects. Inefficiency: Each team member duplicates similar work. Inconsistency: No central source of truth for processed data. 4. What is a feature store and how does it help? # A feature store centralizes reusable, preprocessed features. Helps reduce redundancy and promotes consistency. 5. How do knowledge graphs improve the pipeline? # Data is cleaned and harmonized once at the graph level. All downstream users can reuse the harmonized view via queries or APIs. 6. What assumptions are made when using a knowledge graph? # Patient-level data and terminology concepts are stored in the same graph. Nodes/edges are tagged with metadata (e.g., timestamps, source). The graph is a supergraph enabling subgraph extraction. 7. What are graph embeddings and why are they useful? # They convert graph structures into vectors usable in ML models. Enable pattern detection, similarity analysis, and deep learning. 8. What is node2vec? # Random walk-based graph embedding technique. Uses return (p) and in-out (q) parameters to tune graph walk. Captures homophily and structural equivalence. 9. What is cui2vec? # Embeds UMLS CUIs based on co-occurrence in various RWD sources. Context-aware (claims, notes, publications). Useful for understanding concept similarity. 10. What is med2vec? # Uses temporal sequence of medical events to create visit-based embeddings. Retains longitudinal context. 11. What is snomed2vec? # Embeds SNOMED CT concepts using hierarchical and network-based methods. Includes alternatives like metapath2vec and Poincar√© embeddings. 12. What are some challenges with pretrained embeddings? # Risk of overfitting to training data domain (e.g., CMS claims). May not generalize well to other populations or use cases. Introduces extra model layer to maintain and tune. Part 2: Curriculum-Style Breakdown with \u0026ldquo;Why\u0026rdquo; # üß≠ Phase 1: Understand the Motivation # Task: Read and distinguish between cleaning, harmonization, and feature engineering. Why: Clarifies each pipeline component and prevents misuse of graphs for tasks like feature engineering. üß± Phase 2: Explore Pipeline Challenges # Task: Analyze Figures 6-6 to 6-9 on pipeline repetition and inefficiency. Why: Understand how lack of standardization leads to duplicated efforts. üß† Phase 3: Learn about Feature Stores # Task: Study how feature stores centralize and reuse engineered features. Why: Saves time, increases reproducibility, and reduces tech debt. üåê Phase 4: Integrate Knowledge Graphs # Task: Understand what goes into a knowledge graph (patient data + ontologies). Why: Enables one-time harmonization per data source, allowing scalable reuse. üß© Phase 5: Explore Graph Embedding Techniques # Task: Implement node2vec on a small graph. Why: Learn homophily vs structural equivalence, key for biomedical graph reasoning. üß¨ Phase 6: Biomedical Concept Embeddings # Task: Compare and contrast cui2vec, med2vec, and snomed2vec. Why: Appreciate how embeddings differ by data type (temporal, co-occurrence, hierarchical). ‚ö†Ô∏è Phase 7: Real-World Concerns with Embeddings # Task: Evaluate pretrained embeddings and consider limitations (overfitting, generalizability). Why: Embeddings may look good on paper but can fail in new domains. üîÅ Phase 8: Apply to Your Use Case # Task: Pick a small real-world use case and simulate a pipeline using a knowledge graph and embedding. Why: Reinforces learning and identifies operational gaps in pipeline design. "},{"id":37,"href":"/healthcare-domain/learning/clinical-data-science/","title":"Clinical Data Science","section":"Learning","content":"Content for the Clinical Data Science Specialization section.\n"},{"id":38,"href":"/healthcare-domain/learning/ai-in-healthcare/c2_clinical_data/nlp_clinical_text/","title":"Clinical Text Feature Extraction Using Dictionary-Based Filtering","section":"C2 Clinical Data","content":" üß¨ Clinical Text Feature Extraction Using Dictionary-Based Filtering # This guide demonstrates a simplified approach for processing clinical text without removing PHI directly. Instead, it extracts only medical terms from a predefined dictionary (simulated knowledge graph), which passively excludes PHI and enables downstream analyses.\n‚úÖ Objective # Extract present, positive mentions of clinical concepts (e.g., diseases, symptoms, drugs). Avoid mentions that are negated or refer to historical/family context. Demonstrate the principle: \u0026ldquo;Keep only medical terms\u0026rdquo; as an alternative to direct PHI removal. üßæ Input Example # Patient complains of chest pain. No signs of pneumonia. History of diabetes mellitus. Prescribed metformin. Mother had breast cancer. üß† Procedure Overview # Define a medical term dictionary (simulating a knowledge graph). Split the clinical note into sentences. Ignore sentences with negation or irrelevant context. Match and extract terms from the dictionary. Output structured features for downstream use. üß™ Code Implementation (Python) # import re # 1. Simulated clinical note clinical_note = \u0026#39;\u0026#39;\u0026#39; Patient complains of chest pain. No signs of pneumonia. History of diabetes mellitus. Prescribed metformin. Mother had breast cancer. \u0026#39;\u0026#39;\u0026#39; # 2. Simulated knowledge graph (medical term dictionary) medical_terms = { \u0026#34;chest pain\u0026#34;: \u0026#34;symptom\u0026#34;, \u0026#34;pneumonia\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;diabetes mellitus\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;metformin\u0026#34;: \u0026#34;drug\u0026#34;, \u0026#34;breast cancer\u0026#34;: \u0026#34;disease\u0026#34; } # 3. Split into sentences sentences = re.split(r\u0026#39;\\.\\s*\u0026#39;, clinical_note.strip()) features = [] # 4. Process each sentence for sentence in sentences: sentence_lower = sentence.lower() # 5. Skip negated or historical context if \u0026#34;no \u0026#34; in sentence_lower or \u0026#34;history of\u0026#34; in sentence_lower or \u0026#34;mother had\u0026#34; in sentence_lower: continue # 6. Match medical terms for term in medical_terms: if term in sentence_lower: features.append({ \u0026#34;term\u0026#34;: term, \u0026#34;type\u0026#34;: medical_terms[term], \u0026#34;sentence\u0026#34;: sentence.strip() }) # 7. Output extracted features for feature in features: print(f\u0026#34;Found {feature[\u0026#39;type\u0026#39;]} ‚Üí \u0026#39;{feature[\u0026#39;term\u0026#39;]}\u0026#39; in: \\\u0026#34;{feature[\u0026#39;sentence\u0026#39;]}\\\u0026#34;\u0026#34;) üì§ Sample Output # Found symptom ‚Üí \u0026#39;chest pain\u0026#39; in: \u0026#34;Patient complains of chest pain\u0026#34; Found drug ‚Üí \u0026#39;metformin\u0026#39; in: \u0026#34;Prescribed metformin\u0026#34; üìå Summary # This method:\nAvoids direct PHI detection Extracts useful clinical concepts only Can be adapted to larger vocabularies and real NLP tools (e.g., spaCy, scispaCy, NegEx) Perfect for research scenarios where structured clinical features are needed but full de-identification is too complex.\n"},{"id":39,"href":"/healthcare-domain/learning/ai-in-healthcare/c2_clinical_data/clinical_text_mining_pipeline/","title":"Clinical Text Mining Pipeline (Steps 1‚Äì5)","section":"C2 Clinical Data","content":" üè• Clinical Text Mining Pipeline (Steps 1‚Äì5) # This document outlines a high-level clinical text mining pipeline using knowledge graphs, NLP, and structured indexing. The goal is to extract, enrich, and analyze clinical concepts from raw EMR text.\nüßæ Step 1: Preprocessing Clinical Documents # Goal: Prepare and normalize clinical notes for processing.\nTools: Text cleaning, sentence segmentation, tokenizer.\n# Example: Clean and split into sentences import re clinical_note = \u0026#34;Pt c/o chest pain. No signs of pneumonia. History of stroke. Prescribed metformin.\u0026#34; sentences = re.split(r\u0026#39;\\.\\s*\u0026#39;, clinical_note.lower()) üß† Step 2: Extract Terms Using Knowledge Graph + NLP # Goal: Identify medical terms using a knowledge graph and remove ambiguous, negated, or contextual mentions.\nTools: Knowledge Graph (e.g., UMLS), NegEx, ConText\n# Simulated medical term dictionary (knowledge graph-based) medical_terms = {\u0026#34;chest pain\u0026#34;: \u0026#34;symptom\u0026#34;, \u0026#34;pneumonia\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;stroke\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;metformin\u0026#34;: \u0026#34;drug\u0026#34;} # Filtered sentences (simulate negation/context removal) filtered_mentions = [] for s in sentences: if \u0026#34;no \u0026#34; in s or \u0026#34;history of\u0026#34; in s: continue for term in medical_terms: if term in s: filtered_mentions.append(term) üóÇÔ∏è Step 3: Index Positive, Present Mentions # Goal: Store structured, filtered term mentions for later search.\nTools: JSON/DB-based indexing, storing patient-term mappings.\nindexed_mentions = [ {\u0026#34;patient_id\u0026#34;: 1, \u0026#34;term\u0026#34;: \u0026#34;chest pain\u0026#34;}, {\u0026#34;patient_id\u0026#34;: 1, \u0026#34;term\u0026#34;: \u0026#34;metformin\u0026#34;}, ] üß≠ Step 4: Query-Time Semantic Expansion # Goal: Expand the user‚Äôs query using KG (synonyms, variants, etc.) and disambiguate based on context.\nTools: Knowledge Graph (UMLS), synonym/semantic type lookup, optional filters\nquery = \u0026#34;stroke\u0026#34; expanded_terms = [\u0026#34;stroke\u0026#34;, \u0026#34;cva\u0026#34;, \u0026#34;cerebrovascular accident\u0026#34;] # Disambiguate (simplified) def is_valid(term, patient_age, season): return not (term == \u0026#34;heatstroke\u0026#34; and patient_age \u0026lt; 18 and season == \u0026#34;summer\u0026#34;) üìä Step 5: Build Patient-Feature Matrix for Analysis # Goal: Aggregate term mentions per patient for cohort selection and modeling.\nTools: Pandas, matrix construction, temporal tagging\nfrom collections import defaultdict feature_matrix = defaultdict(lambda: {\u0026#34;stroke_mention\u0026#34;: 0}) patient_metadata = {1: {\u0026#34;age\u0026#34;: 65, \u0026#34;season\u0026#34;: \u0026#34;spring\u0026#34;}} for mention in indexed_mentions: pid = mention[\u0026#34;patient_id\u0026#34;] term = mention[\u0026#34;term\u0026#34;] if term in expanded_terms and is_valid(term, patient_metadata[pid][\u0026#34;age\u0026#34;], patient_metadata[pid][\u0026#34;season\u0026#34;]): feature_matrix[pid][\u0026#34;stroke_mention\u0026#34;] += 1 print(dict(feature_matrix)) ‚úÖ Summary # Step Goal Tools 1 Clean \u0026amp; tokenize notes Regex, NLP 2 Extract clean medical terms KG, NegEx, filtering 3 Store structured mentions JSON, DB 4 Expand/interpret queries KG, synonyms, disambiguation 5 Analyze for research Patient-feature matrix, Pandas This modular pipeline separates data preparation from query-time flexibility, making it robust and reusable.\n"},{"id":40,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/data_quality_labeling_weak_supervision_qa/","title":"Data Quality, Labeling, and Weak Supervision in Clinical ML","section":"C3 ML Healthcare","content":" Data Quality, Labeling, and Weak Supervision in Clinical ML # Q1: What does \u0026ldquo;Garbage In, Garbage Out\u0026rdquo; mean in machine learning? # It means that no model, no matter how advanced, can compensate for poor-quality data. If your input data is noisy, biased, irrelevant, or mislabeled, your model will reflect those flaws.\n‚úÖ The choice of data and problem matters more than the algorithm itself.\nQ2: Can large, rich datasets still be garbage? # Yes ‚Äî if the data is fundamentally flawed or based on faulty assumptions (like phrenology), more volume just means more noise.\nüìå Data quality ‚â† data quantity.\nQ3: What makes clinical data especially tricky to label accurately? # Lack of standardized label definitions Evolving medical criteria (e.g. changing diabetes thresholds) Some labels (e.g., mortality) are easier to pin down Others (e.g., pneumonia, hypertension) require complex confirmation (labs, imaging, notes) üß† Medical label creation is hard, expensive, and often subjective.\nQ4: How do we deal with label noise in practice? # Label noise is inevitable, but manageable.\nStrategies:\nHave domain experts label a subset for benchmarking Use multiple reviewers to estimate disagreement rate Triangulate labels (e.g., combine ICD codes + meds + clinician notes) üìâ This reduces label noise but often shrinks dataset size.\nQ5: Is it ever okay to use noisy labels? # Surprisingly, yes ‚Äî if you have enough data.\nüìà A Stanford study found:\n90% label accuracy ‚âà baseline (with 50% more data) 85% label accuracy ‚âà baseline (with 100% more data) ‚úÖ Rule of thumb:\n10% noise ‚Üí 1.5√ó more data 15% noise ‚Üí 2√ó more data Q6: What is weak supervision? # Weak supervision refers to learning from labels that are:\nNoisy Incomplete Imperfectly defined This is common in healthcare due to:\nThe cost of expert labeling The complexity of clinical truth üë®‚Äç‚öïÔ∏è That‚Äôs why domain experts + scalable label strategies are a key bottleneck in clinical ML.\nQ7: If training data is noisy, what about the test set? # The test set must be as clean as possible.\nWhy?\nIf test labels are noisy, your evaluation metrics will be inaccurate It may underestimate model performance, leading to incorrect conclusions üìå Training set: can handle some noise.\nüìå Test set: must approximate gold-standard ground truth.\nüîë Final Takeaways # Principle Why It Matters Garbage In, Garbage Out Bad input = bad model, no matter the algorithm Labels ‚â† Truth Always validate how close your labels are to clinical reality More Data ‚â† Better Data Large, outdated, or noisy datasets can harm performance Weak Supervision Works (With Scale) Noisy labels can be offset by higher volume Test Set Must Be Clean Final evaluation must reflect ground truth "},{"id":41,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/diagnostic_metrics_and_curves/","title":"Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations","section":"C3 ML Healthcare","content":" Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations # This guide summarizes the core diagnostic metrics based on anchoring logic (condition vs. prediction), and how these metrics relate to ROC and PR curves ‚Äî especially under balanced vs. imbalanced class distributions.\nüîπ Test-Centric Metrics (Anchored on Actual Condition) # Evaluates test performance, independent of disease prevalence. Anchor: Ground truth label (actual condition). Positive-Focused (Sensitivity) # Fix actual Positive label. Incorrect prediction: False Negative (FN). Pair: (TP, FN) Sensitivity = TP / (TP + FN) Negative-Focused (Specificity) # Fix actual Negative label. Incorrect prediction: False Positive (FP). Pair: (TN, FP) Specificity = TN / (TN + FP) üî∏ Outcome-Centric Metrics (Anchored on Prediction) # Evaluates usefulness of test result, dependent on both test performance and prevalence. Anchor: Test result (prediction output). Positive-Focused (PPV / Precision) # Fix Positive prediction. Incorrect prediction: False Positive (FP). Pair: (TP, FP) Positive Predictive Value (PPV) = TP / (TP + FP) Negative-Focused (NPV) # Fix Negative prediction. Incorrect prediction: False Negative (FN). Pair: (TN, FN) Negative Predictive Value (NPV) = TN / (TN + FN) üìä Extension to ROC and PR Curves # üéØ ROC Curve (Receiver Operating Characteristic) # What it does: # Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) across thresholds. TPR = Sensitivity = TP / (TP + FN) FPR = 1 ‚àí Specificity = FP / (FP + TN) These metrics are calculated by conditioning on the actual class labels, not predictions. Anchoring View: # ‚úÖ Test-Centric / Condition-Anchored Starts from actual condition and evaluates how well the test distinguishes between classes. Independent of class imbalance in its calculation. Use Case: # Suitable when both positive and negative classes are equally important. Can be misleading in highly imbalanced datasets (e.g., rare disease). üìà Precision-Recall (PR) Curve # What it does: # Plots Precision (PPV) vs. Recall (Sensitivity) across thresholds. Precision = TP / (TP + FP) Recall = TP / (TP + FN) Anchoring View: # ‚úÖ Outcome-Centric / Prediction-Anchored Focuses on the model‚Äôs positive predictions and how often they are correct. Particularly useful for evaluating performance on the positive class in imbalanced datasets. Use Case: # Ideal for problems with class imbalance, where the positive class is rare but important (e.g., cancer detection, fraud, anomaly detection). Answers: ‚ÄúWhen the model says positive, can I trust it?‚Äù üß† Summary of Metric Anchors and Curve Use # Curve Type Metrics Used Anchored On Evaluation Focus Best For ROC TPR (Sensitivity), FPR Actual condition Discrimination ability Balanced class settings PR Precision (PPV), Recall Prediction output Precision of predictions Imbalanced settings üí° Takeaway: # ROC Curve is a test-centric (condition-anchored) tool: great for balanced data, focuses on test performance across thresholds. PR Curve is an outcome-centric (prediction-anchored) tool: best for imbalanced data, reflects how reliable positive predictions are. "},{"id":42,"href":"/healthcare-domain/learning/ai-in-healthcare/c2_clinical_data/ethics_in_ai_healthcare_qna/","title":"Ethics in AI for Healthcare","section":"C2 Clinical Data","content":" Ethics in AI for Healthcare: A Guided Q\u0026amp;A Framework # This document presents a structured chain-of-thought (CoT) using guiding questions and answers to understand ethical considerations in the development and deployment of AI in healthcare, based on Module 7 from the Stanford \u0026ldquo;Introduction to Clinical Data\u0026rdquo; course.\n1. Why is ethics important in the context of AI in healthcare? # Answer:\nAI tools impact patients directly or indirectly, whether through their development (research) or their deployment (clinical practice). Each of these domains carries different ethical responsibilities that must be considered and governed carefully.\n‚û°Ô∏è Leads to: Understanding the foundations of research ethics.\n2. How has the field of research ethics developed over time? # Answer:\nThrough responses to unethical practices (e.g., Tuskegee Study, Nazi experiments), a series of ethical frameworks and regulations emerged, including the Nuremberg Code, the Declaration of Helsinki, and most notably, the Belmont Report.\n‚û°Ô∏è Leads to: A deeper look into the Belmont Report and its enduring impact.\n3. What does the Belmont Report contribute to research ethics? # Answer:\nIt introduces three core principles:\nRespect for Persons: Informed consent and autonomy Beneficence: Minimize harm, maximize benefit Justice: Fair distribution of research benefits and burdens ‚û°Ô∏è Leads to: Applying these principles to modern AI data sources.\n4. Where does AI get its data, and what ethical concerns arise? # Answer:\nAI uses data from research repositories, clinical records, and even consumer devices. Ethical concerns include consent validity, privacy, data security, and the risk of underrepresenting vulnerable populations.\n‚û°Ô∏è Leads to: Addressing secondary uses of data and consent workarounds.\n5. How can researchers ethically use data collected for other purposes? # Answer:\nVia:\nQA exemptions Use of de-identified data IRB-approved waiver of consent\nThese methods are sometimes necessary but ethically controversial due to risks of eroding public trust. ‚û°Ô∏è Leads to: The ethical dilemma of returning individual results.\n6. Should researchers return results to participants? # Answer:\nIt depends. Options range from never returning results (to avoid harm/confusion) to always returning them (to respect autonomy). Most agree on a middle ground: only return results that are valid and actionable.\n‚û°Ô∏è Leads to: Examining systems where research and practice are merged‚Äîlike a Learning Health System.\n7. What is a Learning Health System (LHS), and how does it relate to AI? # Answer:\nAn LHS continuously learns from clinical care data to improve outcomes. AI is central to this feedback loop, but it blurs the line between research and care, making traditional ethical boundaries harder to apply.\n‚û°Ô∏è Leads to: Rethinking ethical frameworks for hybrid systems like LHS.\n8. Is there an ethical model better suited for a Learning Health System? # Answer:\nYes. A proposed model includes duties to:\nRespect patients (via transparency, not just consent) Improve care (beneficence) Reduce inequality (justice) Engage both clinicians and patients in the learning process\nHowever, it lacks strict rules for handling trade-offs between these duties. Summary:\nEach principle in the Belmont Report supports the others. Respect enables informed choice, beneficence ensures that choice isn\u0026rsquo;t harmful, and justice guarantees fairness across all participants. As AI transforms healthcare, our ethical thinking must evolve accordingly.\n"},{"id":43,"href":"/ai-workflows/structural-reasoning/graphs/graphrag/","title":"GraphRAG","section":"Graphs","content":"Content for the GraphRAG section.\n"},{"id":44,"href":"/healthcare-domain/learning/hands-on-healthcare-data/","title":"Hands-On Healthcare Data","section":"Learning","content":" Based on \u0026ldquo;Hands-On Healthcare Data: Taming the Complexity of Real-World Data\u0026rdquo;\nCh1 Ch2 Ch3 Ch4 Ch5 Ch6. Machine Learning \u0026amp; Graph-Based Analytics in Healthcare Ch7 "},{"id":45,"href":"/healthcare-domain/data/healthcare_layers/","title":"Healthcare Data Layers","section":"Healthcare Data","content":" Healthcare Data Layers # 1Ô∏è‚É£ Data Sources (Raw Data \u0026amp; Collection Level) These are the foundational data sources used in healthcare analysis, originating from clinical trials, hospitals, insurance claims, and patient records.\nClinical Data (RCTs, EHR, OMOP, CDM) ‚Äì Structured, controlled, and often randomized data used for regulatory and research applications. Real-World Data (RWD: EHR, Claims, Registries) ‚Äì Observational and confounded, requiring advanced causal inference methods to extract meaningful insights.\nüîó Relationship: Clinical Data is typically highly structured and standardized, whereas RWD is heterogeneous, requiring bias correction.\n2Ô∏è‚É£ Data Management \u0026amp; Standardization (Processing \u0026amp; Infrastructure Level) This layer ensures that raw clinical \u0026amp; real-world data are cleaned, structured, and made interoperable for analysis.\nHealthcare Informatics ‚Äì The framework for data integration, ETL processes, standardization (OMOP, FHIR, CDMs), interoperability, and terminology mapping (SNOMED, LOINC, ICD).\nüîó Relationship:\nHealthcare Informatics acts as a bridge between data collection (clinical \u0026amp; RWD) and analytics. Without informatics, AI models and statistical analyses would lack clean, structured, and standardized data. 3Ô∏è‚É£ Data Analytics \u0026amp; Decision Intelligence (AI \u0026amp; Statistical Analysis Level) This layer applies statistical, machine learning (ML), and deep learning (DL) models to structured and unstructured healthcare data for actionable insights.\nTraditional Data Science \u0026amp; Statistical Analysis (Used for both Clinical \u0026amp; RWD)\nBiostatistics, Bayesian Methods, Survival Analysis, Causal Inference (PSM, DAGs, DiD) Used to control bias, estimate treatment effects, and generate regulatory-grade evidence (RWE). AI in Healthcare (Machine Learning \u0026amp; Deep Learning Applications)\nSupervised Learning (Logistic Regression, Decision Trees, Random Forests) Deep Learning (CNNs, Transformers, NLP, Reinforcement Learning) Model Interpretability (SHAP, LIME) and AI Fairness (Bias Mitigation) üîó Relationship:\nAI \u0026amp; ML rely on structured, clean data (from Healthcare Informatics) and leverage Clinical Data \u0026amp; RWD to generate predictions and automate decision-making. Statistical analysis methods (causal inference, survival analysis) are critical for ensuring valid results before AI is applied. "},{"id":46,"href":"/healthcare-domain/data/healthcare_sources/","title":"Healthcare Data Sources","section":"Healthcare Data","content":" Healthcare Data Sources # Phenotype KnowledgeBase (PheKB) # Description:\nA collaborative portal for sharing and validating electronic phenotype definitions used in observational health research.\nTags: phenotyping, EHR, cohort definitions\nUse Cases:\nStandardized phenotype definitions for conditions like diabetes, asthma, etc. Sharing phenotype algorithms across institutions MIMIC-IV (Medical Information Mart for Intensive Care) # Description:\nA large, publicly available critical care database containing de-identified health data from ICU patients at the Beth Israel Deaconess Medical Center.\nTags: ICU, de-identified data, clinical research\nUse Cases:\nPredictive modeling in critical care Benchmarking clinical algorithms Training deep learning models Access Requirements:\nRequires credentialed training and data use agreement via PhysioNet\nOHDSI / OMOP Common Data Model # Description:\nAn open community initiative and standard model for organizing observational health data across institutions and studies.\nTags: standardization, EHR, interoperability, CDM\nUse Cases:\nConverting disparate data sources into a consistent format Enabling federated analysis across healthcare systems Supporting tools like ATLAS for cohort building National COVID Cohort Collaborative (N3C) # Description:\nA centralized, secure platform for analyzing harmonized COVID-19 clinical data from dozens of healthcare providers across the US.\nTags: COVID-19, federated research, clinical data\nUse Cases:\nStudying disease trajectories and treatment effects Multisite analytics using harmonized EHR data Evaluating outcomes for long COVID Access Requirements:\nApplication and institutional affiliation required\nBioPortal # Description:\nA comprehensive repository of biomedical ontologies from the National Center for Biomedical Ontology.\nTags: ontologies, terminology, semantic web, linked data\nUse Cases:\nAccessing ontologies like SNOMED CT, ICD, LOINC, RxNorm Mapping data to standard vocabularies Enabling semantic interoperability Unified Medical Language System (UMLS) # Description:\nIntegrates over 200 biomedical vocabularies to support natural language processing, terminology mapping, and EHR data harmonization.\nTags: NLP, standard vocabularies, concept mapping\nUse Cases:\nLinking clinical terms to standard codes Enhancing search and retrieval in clinical systems Supporting NLP tools like MetaMap and cTAKES Access Requirements:\nFree license from NLM, requires annual agreement\nAphrodite # Description:\nAn R package developed by OHDSI that supports semi-supervised phenotype algorithm development using feature engineering and machine learning methods on OMOP Common Data Model (CDM) datasets.\nTags: phenotyping, machine learning, semi-supervised, OMOP, OHDSI\nUse Cases:\nRapid development of phenotype classifiers using imperfectly labeled data. Applying machine learning models to predict phenotypes based on structured EHR data. Feature extraction from OMOP CDM to support supervised or semi-supervised learning tasks. "},{"id":47,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/healthcare_use_cases_for_non_textual_unstructured_data/","title":"Healthcare Use Cases for Non-textual Unstructured Data","section":"C3 ML Healthcare","content":" Healthcare Use Cases for Non-textual Unstructured Data # 1. Are foundation models limited to text? # No. Although many early applications focus on text, foundation models are inherently multimodal. They can process and learn from images, audio, video, and other forms of unstructured data without modifying the core model.\n2. What kind of unstructured data exists in healthcare beyond text? # Medical images: X-rays, CT, MRI Audio data: Patient speech, clinical voice notes Video data: Endoscopy, movement assessments Digital pathology and genomic data: High-resolution slides and sequence strings 3. How do image-text foundation models work in healthcare? # They learn from paired image and text data (e.g., medical images and associated reports) to understand content holistically. This allows for better diagnostic performance and contextual understanding than single-modality models.\n4. What is the potential impact of image-text models on radiology? # Radiology is AI-friendly due to digital formats and standards. Current AI tools support quantification, detection, classification, etc. Foundation models go beyond‚Äîintegrating prior exams, clinical context, and treatment recommendations. 5. Why is the current model development approach inefficient? # Today‚Äôs approach builds narrow, task-specific models requiring large labeled datasets. Foundation models reduce this burden via few-shot learning, general knowledge transfer, and multimodal reasoning.\n6. What advantages do foundation models bring to medical imaging? # Combine image + text for deeper insight Reduce false positives/negatives via context Extract insights beyond human interpretation Generate full radiology reports or treatment suggestions Accelerate model development with less labeled data 7. How are foundation models used in image-based diagnosis? # They can:\nUnderstand body composition via imaging biomarkers Detect osteoporosis, aortic calcium, visceral fat, etc. Predict adverse events from routine imaging 8. Can foundation models incorporate genomic or pathology data? # Yes. They can process complex biomedical data:\nDigital pathology slides Genomic sequences in FASTA format Gene expression and mutation patterns This enables discovery of clinically meaningful patterns across modalities.\n9. What is the value of multimodal foundation models? # Integrate text, imaging, genomics, clinical history Offer personalized care and richer diagnostics Support communication with patients in any language or literacy level 10. How do foundation models help in image model development? # They support:\nPreprocessing Data augmentation Synthetic data generation This accelerates model iteration and reduces time to deployment.\n11. Can voice-text data be used in healthcare applications? # Yes. Foundation models can:\nTranscribe speech Analyze speech patterns for diagnosis (e.g., Parkinson‚Äôs) Enable voice prosthetics for patients who‚Äôve lost speech Support virtual medical assistants and mental health chatbots 12. What are some patient-facing applications of voice-text models? # Mental health bots using speech/text input Virtual assistants for disabled individuals Real-time transcription and communication aids 13. What cognitive shift do foundation models support? # They help users move from:\nComprehension-based reasoning ‚Üí using known knowledge Fluid reasoning ‚Üí solving unfamiliar problems with abstracted understanding Foundation models act as co-pilots, enhancing human decision-making.\n14. Do foundation models replace human decision-making? # No. They augment, not replace. Final decisions still rest with trained professionals who interpret the model‚Äôs output with judgment and context.\n15. What‚Äôs the catch with all this power? # Even with their vast capabilities, foundation models are subject to the no free lunch theorem. They have trade-offs, biases, and limitations‚Äîtopics discussed in follow-up modules.\n"},{"id":48,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/healthcare_use_cases_for_text_data/","title":"Healthcare Use Cases for Text Data","section":"C3 ML Healthcare","content":" Healthcare Use Cases for Text Data # 1. Can large language models like ChatGPT perform at a physician level? # Yes. ChatGPT has demonstrated performance comparable to expert physicians on tasks like the USMLE medical board exam. This raises important questions about the evolving role of human expertise in healthcare as LLMs continue to advance.\n2. Should LLMs be integrated into medical training or exams? # Possibly. LLMs could enhance the medical licensing exam process by reflecting real-world clinical scenarios. However, it\u0026rsquo;s essential for healthcare professionals to understand their benefits and limitations before full integration.\n3. What are the risks of integrating LLMs into clinical systems? # While useful for tasks like recipe generation or patient education, LLMs can fail unexpectedly, hallucinate references, or output incorrect information. Human oversight and validation remain critical.\n4. What are practical healthcare use cases for LLMs today? # Clerical task automation: Scheduling, patient communication, and triaging. Inbox management: Reducing message overload and provider burnout. Collaborative assistant: Recommending actions based on patient history. Low-code innovation: Empowering clinicians to build apps/tools. 5. How do LLMs support medical data processing? # LLMs streamline key NLP tasks:\nTokenization: Segmenting clinical notes into analyzable units. Named Entity Recognition (NER): Identifying drugs, diseases, etc. Negation Detection: Understanding sentiment/context (e.g., \u0026ldquo;no cancer\u0026rdquo;). Relation Extraction: Mapping relationships between entities. De-identification: Masking PHI for privacy compliance. 6. Why are foundation models better than traditional NLP for medical text? # They handle variation across institutions, formats, and languages with few-shot/zero-shot learning, reducing the need for custom engineering and enabling broader generalization.\n7. Can LLMs handle complex clinical queries without structured data? # Yes. Prompts like:\n‚ÄúFind all named entities related to diabetes management‚Äù ‚ÄúDe-identify this record per HIPAA‚Äù ‚ÄúHow has cancer progressed after Keytruda treatment?‚Äù show how LLMs can perform analytics directly from unstructured text. 8. Can LLMs be further trained on clinical data? # Yes. Training LLMs on patient records, trials, and guidelines can increase domain-specific accuracy. Applications include:\nClinical Decision Support Drug Interaction Warnings Guideline Recommendations 9. How can LLMs help with clinical trial recruitment? # LLMs can evaluate eligibility based on:\nPatient history Medications Lab results They can also explain trials directly to patients, improving enrollment.\n10. What role can LLMs play in patient communication? # They can:\nAnswer health-related questions Translate jargon into plain language Provide reminders and follow-ups Offer multilingual, conversational support 11. How can LLMs assist with billing and coding? # With medical terminology knowledge, LLMs can:\nAssign billing codes Improve record-keeping Reduce administrative burden 12. Can LLMs support public health efforts? # Yes. They can monitor:\nOutbreak detection using EHRs, social media Pattern recognition across data sources This enables faster responses to public health threats. 13. Can LLMs process and learn from genomic data? # Yes. Genomic data (e.g., FASTA format) is text-based. LLMs can:\nIdentify mutations linked to diseases Predict disease risk Integrate with clinical and lifestyle data 14. What is the benefit of multimodal analysis in genomics? # LLMs can combine:\nGenomic sequences EHRs Environmental/lifestyle data This integration enables personalized care and discovery of complex health patterns.\n15. Can LLMs support pharmacogenomics? # Yes. They can identify:\nDrug responses Adverse reactions Genetic factors impacting efficacy This paves the way for precision medicine.\n16. How do LLMs improve drug discovery? # Applications include:\nVirtual screening: Identify promising molecules Lead optimization: Improve safety and effectiveness Toxicity prediction: Flag unsafe compounds early Mechanism of action prediction: Understand how a drug works 17. What is the long-term outlook for LLMs in healthcare? # The future is expansive:\nFrom analytics and operations to clinical care and research Support for providers, patients, researchers Accelerating breakthroughs in drug development and personalized medicine LLMs are set to revolutionize the healthcare landscape, and we are only scratching the surface.\n"},{"id":49,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/how_foundation_models_work_in_healthcare_applications/","title":"How Foundation Models Work","section":"C3 ML Healthcare","content":" How Foundation Models Work (in Healthcare Applications) # 1. What are foundation models and why are large language models (LLMs) a key example? # Foundation models are large-scale machine learning models trained on vast and diverse datasets, enabling them to perform well across multiple tasks. Large Language Models (LLMs) like GPT are a major class of these, powering tools like ChatGPT and being early, widely successful examples.\n2. What architecture do foundation models, especially LLMs, rely on? # They rely heavily on the Transformer architecture, introduced in the paper ‚ÄúAttention Is All You Need‚Äù. This architecture uses self-attention mechanisms to allow models to learn relationships across sequences of data, making them highly effective for tasks involving language and other sequential data.\n3. How does the attention mechanism work in transformers? # The attention mechanism enables the model to focus on different parts of the input when making predictions. Self-attention lets each element of the input sequence attend to all others, allowing the model to integrate contextual information holistically.\n4. How are transformers structured in large language models? # Transformers in LLMs are deep neural networks built from stacks of layers combining attention with traditional neural layers. They process input as sequences‚Äîideal for language data‚Äîand use tokenization to convert text into manageable units (words, subwords, etc.).\n5. Can tokenization be applied to other data types? # Yes. Tokenization can be generalized to image patches, audio frames, or structured data. This generalization enables foundation models to learn from and operate across multiple modalities.\n6. What are transformer encoders and how are they used in models like BERT? # A transformer encoder transforms input sequences into dense vector representations. BERT, for example, is pretrained using self-supervised learning by masking words and predicting them, allowing it to learn language representations without labeled data.\n7. What about transformer decoders and models like GPT? # Transformer decoders, unlike encoders, generate sequences from input vectors or initial prompts. GPT (Generative Pre-trained Transformer) models are based on decoder-only architectures and are trained to predict the next token in a sequence‚Äîa task aligned with language generation.\n8. How do models like GPT-3 perform impressive tasks with little supervision? # Thanks to transfer learning, GPT models demonstrate zero-shot and few-shot learning, performing tasks they weren\u0026rsquo;t explicitly trained for by using prompts and examples within the input.\n9. How does GPT handle complex tasks like medical Q\u0026amp;A or clinical reasoning? # LLMs can respond to medical questions, explain reasoning for clinical answers, and even alter case scenarios to change correct answers. This shows their potential for medical education and diagnostic simulation.\n10. What are the limitations of LLMs trained like GPT-3? # Since they‚Äôre trained on general web and literature data, LLMs may generate plausible but incorrect outputs, due to misalignment with human values or domain knowledge. Their training objective (next-word prediction) does not inherently align with truth or utility.\n11. How does ChatGPT improve upon this with human feedback? # ChatGPT uses Reinforcement Learning from Human Feedback (RLHF):\nStep 1: Supervised Fine-Tuning (SFT) with human-written responses. Step 2: Reward Model trained from human-ranked outputs. Step 3: Reinforcement learning (e.g., PPO) optimizes the model to favor preferred responses. 12. What is a ‚Äúprompt‚Äù and why does it matter? # A prompt is the input that guides the model\u0026rsquo;s output. Its structure and content influence results dramatically, making prompt engineering a critical skill for getting reliable, targeted responses from foundation models.\n13. What are some common prompt engineering styles? # Instruction prompt: Direct task requests (e.g., ‚ÄúHow to manage diabetes?‚Äù) Role-based prompt: Assigning the model a persona (e.g., ‚ÄúYou are a nurse‚Ä¶‚Äù) Few-shot prompt: Providing examples before the actual question Chain-of-thought (CoT): Encouraging the model to reason step-by-step Zero-shot CoT: Adding phrases like ‚ÄúLet‚Äôs think step-by-step‚Äù to guide reasoning Self-consistency prompting: Generating multiple answers and choosing the majority Generative Knowledge prompting: First generating facts, then reasoning over them 14. Why is chain-of-thought prompting effective, and what are its limits? # CoT prompting helps elicit reasoned answers, particularly in clinical contexts. However, it‚Äôs most effective in larger models (100B+ parameters) and less so in smaller ones.\n15. Do foundation models apply beyond language? # Yes. The transformer architecture extends to:\nImages: via patch tokenization (e.g., Vision Transformers) Audio/Speech: like OpenAI‚Äôs Whisper for transcription Multimodal data: like DALL¬∑E 2 for text-to-image generation 16. How does a multimodal model like DALL¬∑E 2 work? # DALL¬∑E 2:\nEncodes a text prompt into a vector. Maps it to visual feature space. Uses a diffusion model decoder to generate images from that representation. 17. What are diffusion models and how do they help? # Diffusion models learn to denoise images progressively, allowing them to generate high-quality, realistic outputs. They‚Äôre widely used in modern generative vision models like DALL¬∑E 2.\n18. What\u0026rsquo;s the future outlook for foundation models in healthcare? # Foundation models, particularly LLMs and multimodal transformers, are rapidly evolving. With human feedback, prompt engineering, and domain-specific fine-tuning, they offer immense potential for clinical decision support, medical education, and personalized care.\n"},{"id":50,"href":"/ipark/","title":"Inhee Park, PhD - Resume","section":"","content":" "},{"id":51,"href":"/ai-workflows/structural-reasoning/graphs/knowledge-graphs/","title":"Knowledge Graphs","section":"Graphs","content":"Content for the Knowledge Graphs section.\n"},{"id":52,"href":"/healthcare-domain/learning/ai-in-healthcare/c2_clinical_data/missing_values/","title":"Missing Data Scenarios in Healthcare Modeling","section":"C2 Clinical Data","content":" Missing Data Scenarios in Healthcare Modeling # 1. Should Be Measured But Wasn‚Äôt # Description: The value is expected but is missing due to random or procedural issues (e.g., lab error, missed test). Technical Term: MCAR: Missing Completely At Random MAR: Missing At Random Example: A routine blood test wasn\u0026rsquo;t recorded because the sample was lost. Strategy: Impute (mean, median, or model-based). Add a missingness indicator variable (e.g., var_missing = 1). Rationale: The missingness is unrelated to the value itself, so estimation is relatively safe. 2. Mostly Zero Due to Rare Occurrence # Description: Not truly missing ‚Äî the value is zero or absent for most patients because the condition/event is rare. Technical Term: Not Missing (No abbreviation needed) Example: HIV diagnosis column is 0 for most patients. Strategy: Do not impute ‚Äî the 0s are meaningful and reflect true absence. Rationale: These are real values, and zeros carry clinical meaning. 3. Deliberately Not Recorded # Description: Clinician or system chooses not to record a value based on context (e.g., patient clearly stable or too ill). Technical Term: MNAR: Missing Not At Random Example: Sodium level not tested because the patient was clearly stable. Strategy: Avoid imputation if possible ‚Äî it may introduce bias. Use models that handle missingness natively (e.g., decision trees, XGBoost, LightGBM). Consider adding a missingness indicator. Rationale: The missingness depends on the unobserved value and may carry predictive signal. Summary Table # Case Description Abbreviation Impute? Extra Notes 1 Should be measured but wasn‚Äôt MCAR / MAR ‚úÖ Yes Add indicator if signal is likely 2 Mostly zero (rare condition) Not Missing üö´ No Keep as is ‚Äî zeros are informative 3 Deliberately not recorded MNAR ‚ö†Ô∏è Caution Use native handling + possible indicator "},{"id":53,"href":"/healthcare-domain/learning/ai-in-healthcare/c2_clinical_data/omop_vs_rlhf_comparison/","title":"OMOP vs. RLHF","section":"C2 Clinical Data","content":" OMOP vs. RLHF: A Side-by-Side Comparison # This document compares OMOP (Observational Medical Outcomes Partnership) in healthcare with RLHF (Reinforcement Learning from Human Feedback) in generative AI, focusing on their structures, purposes, and alignment with Learning Health System (LHS) principles.\nüîç Summary Table # Aspect OMOP (Healthcare) RLHF (GenAI) Domain Clinical/healthcare data Natural language modeling Purpose Standardize and structure real-world patient data for learning, analytics, and AI Align AI model behavior with human preferences and values Core Process ETL (Extract-Transform-Load) clinical data into a common format for analysis Fine-tune a pretrained LLM using human-labeled preferences or rewards Data Source EHRs, claims, labs, devices Human judgments on AI-generated outputs Feedback Type Structured medical events (diagnoses, drugs, labs, etc.) Human preference signals on outputs (better/worse answers) Learning Method Enables observational \u0026amp; causal learning from patient data Reinforcement learning from ranked or scored examples Governance Layer Ethics via IRB, consent, privacy laws Ethics via safety research, alignment goals, red-teaming Use in Feedback Loops LHS uses OMOP to ‚Äúlearn from care to improve care‚Äù RLHF uses feedback to ‚Äúteach the model to behave better‚Äù üîÅ Conceptual Analogy # OMOP + Learning Health System (LHS) is to the health system\nas\nRLHF is to a generative AI model.\nIn both cases:\nData flows through a system Human-derived feedback loops guide improvement The system continuously adapts and aligns with user or patient needs üß† Key Takeaways # Both OMOP and RLHF are feedback-driven learning architectures grounded in human data. OMOP is part of an ecosystem (LHS) that feeds learning back into medical care. RLHF aligns generative models with human preferences through iterative fine-tuning. Each reflects a shift toward real-time, adaptive, ethically grounded learning. Would you like to extend this comparison with diagrams, code examples, or regulatory implications?\n"},{"id":54,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/oap_framework_healthcare/","title":"Output-Action Pairing (OAP) Framework in Healthcare","section":"C3 ML Healthcare","content":" üß† Output-Action Pairing (OAP) Framework in Healthcare # This guide provides real-world examples of the Output-Action Pairing (OAP) framework: aligning machine learning model outputs with concrete clinical actions to improve care.\nüìã OAP Template # Output (Prediction) Action Taken Who Acts Why It Helps What the model predicts The clinical step or decision triggered The role/team responsible How it improves outcomes or safety ‚úÖ Real-World Examples # 1. Sepsis Prediction # Output: High risk of sepsis in next 6 hours Action: Alert care team, initiate fluids/labs/antibiotics Who acts: Rapid response team (nurses + physicians) Why it helps: Early treatment improves survival 2. Readmission Risk Score # Output: 30% chance of readmission within 30 days Action: Extra discharge planning, follow-up calls, medication check Who acts: Care coordinator + pharmacist Why it helps: Reduces avoidable readmissions 3. Pneumothorax Detection on Chest X-ray # Output: Pneumothorax detected Action: Immediate flag to radiologist and ER for review Who acts: Radiologist + ER team Why it helps: Enables life-saving chest tube intervention 4. COVID-19 Triage # Output: High risk of severe COVID progression Action: ICU evaluation, enhanced monitoring, begin treatment Who acts: Hospitalist or ICU triage physician Why it helps: Allocates ICU resources effectively 5. Fall Risk in Hospital # Output: High fall risk during admission Action: Enable fall precautions (alarms, sitter, etc.) Who acts: Nursing team Why it helps: Prevents injury and hospital complications 6. Stroke Detection via CT # Output: Acute stroke suspected on scan Action: Notify neurologist, activate stroke protocol (tPA window) Who acts: Radiologist + Stroke Response Team Why it helps: Reduces time to brain-saving treatment üîÑ Summary # The OAP framework ensures that ML predictions translate to action, improving clinical relevance and patient safety. Every model in healthcare should answer:\nWhat is the output? What is the action? Who will act on it? How does it help the patient? "},{"id":55,"href":"/healthcare-domain/learning/ai-in-healthcare/c2_clinical_data/diabetes_phenotype_pipeline/","title":"Rule-Based Electronic Phenotyping Example: Type 2 Diabetes","section":"C2 Clinical Data","content":" Rule-Based Electronic Phenotyping Example: Type 2 Diabetes # This notebook walks through the process of defining an electronic phenotype using a rule-based approach, with a focus on Type 2 Diabetes. The pipeline includes concept mapping, multi-patient evaluation, and phenotype logic visualization.\nüîπ Step 1: Simulated Vocabulary Lookup (UMLS / OMOP) # We define the clinical concept (Type 2 Diabetes) using relevant ICD-10 and RxNorm codes.\n# Simulated UMLS/OMOP vocab mapping UMLS_LOOKUP = { \u0026#34;type2_diabetes\u0026#34;: { \u0026#34;icd10\u0026#34;: {\u0026#34;E11.9\u0026#34;, \u0026#34;E11.65\u0026#34;, \u0026#34;E11.00\u0026#34;}, \u0026#34;rxnorm\u0026#34;: {\u0026#34;metformin\u0026#34;, \u0026#34;insulin\u0026#34;}, } } üîπ Step 2: Multi-Patient Phenotyping Logic # Each patient is checked for:\nPresence of ‚â•2 relevant ICD-10 codes Any matching diabetes-related medication (RxNorm) Start date of phenotype based on first matching code from typing import List, Dict, Set from datetime import datetime def has_required_codes(patient, valid_codes: Set[str], source_field: str, min_count=1) -\u0026gt; bool: return len([code for code in patient[source_field] if code in valid_codes]) \u0026gt;= min_count def find_start_date(patient, valid_codes: Set[str], code_dates: Dict[str, List[str]]) -\u0026gt; str: dates = [] for code in valid_codes: if code in code_dates: dates.extend(code_dates[code]) return min(dates) if dates else None def apply_phenotype_definition(patient, concept_map) -\u0026gt; Dict: icd_match = has_required_codes(patient, concept_map[\u0026#34;icd10\u0026#34;], \u0026#34;icd10_codes\u0026#34;, min_count=2) med_match = has_required_codes(patient, concept_map[\u0026#34;rxnorm\u0026#34;], \u0026#34;medications\u0026#34;) start_date = find_start_date(patient, concept_map[\u0026#34;icd10\u0026#34;] | concept_map[\u0026#34;rxnorm\u0026#34;], patient[\u0026#34;code_dates\u0026#34;]) return { \u0026#34;phenotype_positive\u0026#34;: icd_match and med_match, \u0026#34;has_diabetes_codes\u0026#34;: icd_match, \u0026#34;has_diabetes_med\u0026#34;: med_match, \u0026#34;start_date\u0026#34;: start_date } # Sample patient data (multiple patients) patients = [ { \u0026#34;id\u0026#34;: \u0026#34;P001\u0026#34;, \u0026#34;icd10_codes\u0026#34;: [\u0026#34;E11.9\u0026#34;, \u0026#34;E11.9\u0026#34;], \u0026#34;medications\u0026#34;: [\u0026#34;metformin\u0026#34;], \u0026#34;code_dates\u0026#34;: { \u0026#34;E11.9\u0026#34;: [\u0026#34;2023-01-01\u0026#34;, \u0026#34;2023-03-01\u0026#34;], \u0026#34;metformin\u0026#34;: [\u0026#34;2023-01-05\u0026#34;] } }, { \u0026#34;id\u0026#34;: \u0026#34;P002\u0026#34;, \u0026#34;icd10_codes\u0026#34;: [\u0026#34;I10\u0026#34;], # Hypertension only \u0026#34;medications\u0026#34;: [\u0026#34;lisinopril\u0026#34;], \u0026#34;code_dates\u0026#34;: { \u0026#34;I10\u0026#34;: [\u0026#34;2023-04-01\u0026#34;], \u0026#34;lisinopril\u0026#34;: [\u0026#34;2023-04-05\u0026#34;] } } ] # Apply phenotype to all results = {} for patient in patients: result = apply_phenotype_definition(patient, UMLS_LOOKUP[\u0026#34;type2_diabetes\u0026#34;]) results[patient[\u0026#34;id\u0026#34;]] = result # Show results for pid, res in results.items(): print(f\u0026#34;{pid}: {res}\u0026#34;) üîπ Step 3: Phenotype Logic Flowchart # Below is a visual flowchart that shows the phenotype logic step-by-step.\nAlt text ‚úÖ Summary # This markdown covers:\nRule-based phenotyping using ICD and RxNorm codes Handling multiple patients Simulated code-date structure Logical combination of conditions (AND logic) A visual diagram of the rule logic This framework can be expanded to:\nInclude real UMLS/OMOP lookups via API Support more complex logic (time gaps, lab thresholds) Incorporate chart-reviewed gold standards "},{"id":56,"href":"/healthcare-domain/learning/ai-in-healthcare/c3_ml_healthcare/precision_vs_recall_in_healthcare/","title":"Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare","section":"C3 ML Healthcare","content":" Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare # This guide summarizes two key scenarios in healthcare where we might prefer:\nHigh Precision but Lower Recall High Recall but Lower Precision (1) High Precision, Lower Recall # ‚úÖ When to Use: # When false positives are costly or harmful When resources are limited In early screening/filtering stages üìå Justification: # You want to be very confident before taking action. Missing some real cases is acceptable if wrongly flagging someone leads to emotional, financial, or clinical harm. üí° Examples: # Genetic Testing for Rare Diseases: Only flag patients when you\u0026rsquo;re very sure. A false positive could cause unnecessary panic or life changes. ICU Bed Allocation: If you only have 5 beds, you‚Äôd want to use them for patients who are most certainly critical. Drug Discovery Pre-Screening: Select molecules that are most likely to work, even if some potential candidates are missed. (2) High Recall, Lower Precision # ‚úÖ When to Use: # When missing a real case is dangerous When early detection can improve outcomes When follow-up tests or actions are safe and cheap üìå Justification: # It\u0026rsquo;s better to catch every possible case, even if you have some false alarms. Especially important in serious or rapidly progressing conditions. üí° Examples: # Cancer Screening: Better to flag more patients for follow-up than miss someone with early-stage cancer. Sepsis Prediction in ER: Alerting the care team early‚Äîeven with some false alarms‚Äîcan save lives. COVID-19 Testing in High-Risk Areas: Broad detection to prevent spread, even if some healthy people test positive. üß† Summary Table # Scenario Priority Justification Example High Precision, Lower Recall Precision üü¢ Avoid harm/cost from false positives Genetic testing, ICU triage High Recall, Lower Precision Recall üü¢ Avoid missing critical or contagious conditions Cancer screening, sepsis alert "},{"id":57,"href":"/ai-workflows/nlp-llm-genai/transformer_attention_concepts/","title":"Transformer Attention: Full Conceptual Breakdown","section":"NLP‚ÜíLLMs‚ÜíGenAI","content":" üß† Transformer Attention: Full Conceptual Breakdown # This document summarizes an in-depth discussion on attention mechanisms in Transformers, with a special focus on vocabulary embeddings, Q/K/V matrices, and multi-head attention.\nüìå 1. Understanding the Self-Attention Image # The image shows a single-head self-attention computation. Each row is a token (element) at a position, with a feature vector (embedding). The attention weights (left column) are used to compute a weighted sum over these vectors. The final output vector is shown at the bottom ‚Äî this is the attention output for one token. üîç 2. Element vs. Position # Element: the actual word or token in the input sequence. Position: the index of the element in the sequence. Though tightly coupled (1:1), they are conceptually different. Transformers rely on positional encoding to retain order, since attention alone is orderless. ü§ñ 3. How Attention Scores Are Computed # Input embeddings X are projected into:\nQueries (Q) Keys (K) Values (V) Attention score between token i and j:\nscore = dot(Q[i], K[j]) / sqrt(d_k) Apply softmax to get weights.\nMultiply each Value by its weight and sum ‚Üí gives the final output vector.\nüß† 4. What Is X in the Diagram? # The large matrix on the right of the image is the input embedding matrix X. Shape: sequence_length √ó embedding_dim It is built by looking up each token‚Äôs vector from the vocabulary embedding matrix. üîÑ 5. What Is Multi-Head Attention? # Single-head attention is shown in the image. Multi-head attention: Splits X into smaller chunks (d_model / n_heads) Computes self-attention in parallel on each chunk (head) Concatenates results from all heads Applies a final linear projection üî° 6. Vocab Embedding Matrix vs. Q/K/V # Vocabulary embedding matrix: Initialized randomly Trained to map each token to a vector Q, K, V: Computed from X using learned matrices W_Q, W_K, W_V Not stored in the vocabulary matrix Are trainable and persistent ‚ôªÔ∏è 7. Lifetime of W_Q, W_K, W_V # These matrices are: Initialized once Trained over time Reused across batches They are not reset per input or per batch. Gradients update them through backpropagation. üì• 8. Is Vocabulary Matrix Also Trainable? # ‚úÖ Yes.\nIt is randomly initialized and trained alongside the rest of the model. Each token lookup retrieves a vector from this matrix. This matrix evolves to encode semantic relationships between words. üì¶ 9. Use Cases After Training # Goal Uses Vocab Matrix Uses W_Q/K/V Inference on new sentence ‚úÖ ‚úÖ Static embedding for a token ‚úÖ ‚ùå Contextual embedding in sentence ‚úÖ ‚úÖ üìê 10. Dimensions of X, Q, K, V, and Attention # Let:\nL = sequence length d_model = embedding dimension (e.g. 512) n_heads = number of attention heads d_k = d_model / n_heads Component Shape Input X (L, d_model) W_Q, W_K, W_V (d_model, d_model) Q, K, V (stacked) (n_heads, L, d_k) Attention output (head) (L, d_k) Concatenated heads (L, d_model) Final output (L, d_model) ‚ùì 11. Why Isn‚Äôt the Final Output a Distribution Over Vocabulary? # This is a great question that highlights a common confusion.\nThe output of multi-head attention (and the full Transformer stack) is:\n(L, d_model) But the vocabulary distribution comes after applying a final linear layer:\nW_vocab ‚àà ‚Ñù^(d_model √ó vocab_size) logits = output √ó W_vocab ‚Üí (L, vocab_size) Then softmax gives:\nprobability distribution over vocabulary for each token position Stage Output Shape Multi-head Attention (L, d_model) Final Linear Projection (L, vocab_size) Softmax (L, vocab_size) So the discrepancy is resolved when we remember that attention is only a component ‚Äî the final vocabulary distribution is computed later in the model pipeline.\nPrepared as a study summary by ChatGPT based on a thread of detailed conceptual questions.\n"},{"id":58,"href":"/ai-workflows/nlp-llm-genai/bert_cls_classification_summary/","title":"Understanding How to Use BERT's CLS Token for Classification","section":"NLP‚ÜíLLMs‚ÜíGenAI","content":"Date: 2025-03-31\n‚ùì Question # How can we use the [CLS] token (i.e., h_cls) from the last layer of BERT for classification tasks? Given that the BERT output has shape [batch_size, sequence_length, hidden_size], how is it valid to pass only [batch_size, hidden_size] to a nn.Linear(hidden_size, num_classes) without flattening the sequence? And why don\u0026rsquo;t we flatten the whole sequence ‚Äî wouldn\u0026rsquo;t that destroy order?\n‚úÖ Answer # üîπ BERT Output and the [CLS] Token # BERT outputs a tensor of shape:\n[batch_size, sequence_length, hidden_size] But for classification tasks, we typically use only the [CLS] token, which is located at position 0 in the sequence:\nh_cls = outputs.last_hidden_state[:, 0, :] # Shape: [batch_size, hidden_size] This token is designed to act as a summary representation of the entire sequence, and this output shape matches exactly what a nn.Linear(hidden_size, num_classes) expects ‚Äî no flattening needed.\nüîπ Why Not Flatten? # Flattening the whole sequence (e.g., [batch_size, sequence_length * hidden_size]) loses:\nToken order Positional embeddings Sequence structure In NLP, this breaks the semantic and syntactic structure of the input. Instead, use:\nüî∏ Recommended Pooling Strategies # Strategy Description [CLS] Token Use outputs[:, 0, :]; trained as a sequence summary Mean Pooling outputs.mean(dim=1); averages token embeddings Max Pooling outputs.max(dim=1).values; takes strongest signal Attention Pooling Learns weights to summarize tokens adaptively üìö Sources and Justification # BERT Paper: Devlin et al. (2018) ‚Äî [CLS] token for classification Sentence-BERT: Reimers \u0026amp; Gurevych (2019) ‚Äî Mean pooling often better for embeddings Hugging Face Transformers: Practical implementation patterns NLP Community Practices: Kaggle, blogs, and tutorials üß™ Summary # Use [CLS] or pooling (not flattening) for sequence-level tasks. Flattening destroys sequence information and is rarely appropriate in NLP. The linear layer works on [batch_size, hidden_size] ‚Äî no need to flatten across tokens. "},{"id":59,"href":"/ai-workflows/nlp-llm-genai/self_attention_summary/","title":"Understanding Self-Attention in Transformers: A Visual Breakdown","section":"NLP‚ÜíLLMs‚ÜíGenAI","content":" üîç Understanding Self-Attention in Transformers: A Visual Breakdown # This document summarizes key questions about self-attention, embedding vectors, positions, and the input matrix in Transformers ‚Äî using the image you provided as the foundation.\nüß† What Is Happening in the Diagram? # The figure shows how self-attention computes the output for a specific position (\u0026ldquo;detection\u0026rdquo;) by:\nGenerating attention weights between that position and all other positions. Using those weights to compute a weighted sum of the input feature vectors. üß© Key Concepts Explained # Term Meaning Element A token or word in the input sequence. Each row in the matrix is one. Position The index (0-based) of each element. Used to maintain order. Sequence The full ordered list of elements (e.g. a sentence). Word The natural-language item each element may represent. Feature Values Vector representation of the element (its embedding). While element and position are tightly linked (1:1), they are conceptually distinct: Position = slot/index Element = content in that slot üßÆ How Attention Scores Are Computed # Self-attention uses scaled dot-product attention:\nInput matrix X (from the figure) holds all embeddings. It is projected into Q, K, V using learned weights. Attention scores = dot(Q[i], K[j]) / sqrt(d_k) Softmax turns scores into attention weights. Output vector = weighted sum over all V[j], using those weights. The purple bar on the left in the figure shows these attention weights (e.g., [0.3, 0.2, 0.1, 0.3, 0, ...]).\n‚úÖ What the Image Represents # Part of Image Concept in Transformer Right-side matrix (rows) Input feature matrix X Each row One input element (word/token) Left-side purple weights Attention scores for one position Final row at bottom Output vector (weighted sum of inputs) Prepared with explanations from ChatGPT based on your questions.\n"}]