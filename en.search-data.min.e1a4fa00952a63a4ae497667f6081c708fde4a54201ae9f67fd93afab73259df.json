[{"id":0,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/toc_course2/","title":"[ToC] Course 2","section":"C2 Clinical Data","content":" ToC of Course 2/5: Introduction to Clinical Data # Module 1: Asking and Answering Questions via Clinical Data Mining # Introduction to the data mining workflow Real Life Example Example: Finding similar patients Example: Estimating risk Putting patient data on timeline Revisit the data mining workflow steps Types of research questions Research questions suited for clinical data Example: making decision to treat Properties that make answering a research question useful Module 2: Data Available from Healthcare Systems # Review of the healthcare system Review of key entities and the data they collect Actors with different interests Common data types in Healthcare Strengths and weaknesses of observational data Bias and error from the healthcare system perspective Bias and error of exposures and outcomes How a patient\u0026rsquo;s exposure might be misclassified How a patient\u0026rsquo;s outcome could be misclassified Electronic medical record data Claims data Pharmacy Surveillance datasets and Registries Population health data sets A framework to assess if a data source is useful Module 3: Representing Time, and Timing of Events, for Clinical Data Mining # Introduction Time, timelines, timescales and representations of time Timescale: Choosing the relevant units of time What affects the timescale Representation of time Time series and non-time series data Order of events Implicit representations of time Different ways to put data in bins Timing of exposures and outcomes Clinical processes are non-stationary Module 4: Creating Analysis Ready Datasets from Patient Timelines # Turning clinical data into something you can analyze Defining the unit of analysis Using features and the presence of features How to create features from structured sources Standardizing features Dealing with too many features The origins of missing values Dealing with missing values Summary recommendations for missing values Constructing new features Examples of engineered features When to consider engineered features Main points about creating analysis ready datasets Structured knowledge graphs So what exactly is in a knowledge graph What are important knowledge graphs How to choose which knowledge graph to use Module 5: Handling Unstructured Healthcare Data: Text, Images, Signals # Introduction to unstructured data What is clinical text The value of clinical text What makes clinical text difficult to handle Privacy and de-identification A primer on Natural Language Processing Practical approach to processing clinical text Summary - Clinical text Overview and goals of medical imaging Why are images important? What are images? A typical image management process Summary - Images Overview of biomedical signals Why are signals important? What are signals? What are the major issues with using signals? Summary - Signals Module 6: Putting the Pieces Together: Electronic Phenotyping # Introduction to electronic phenotyping Challenges in electronic phenotyping Specifying an electronic phenotype Two approaches to phenotyping Rule-based electronic phenotyping Examples of rule based electronic phenotype definitions Constructing a rule based phenotype definition Probabilistic phenotyping Approaches for creating a probabilistic phenotype definition Software for probabilistic phenotype definitions Module 7: Ethics # Introduction to Research Ethics and AI The Belmont Report: A Framework for Research Ethics Ethical Issues in Data sources for AI Secondary Uses of Data Return of Results AI and The Learning Health System Ethics Summary "},{"id":1,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/toc_course3/","title":"[ToC] Course 3","section":"C3 ML Healthcare","content":" ToC of Course 3/5: Fundamentals of Machine Learning for Healthcare # Module 3: Concepts and Principles of Machine Learning in Healthcare # Introduction to Deep Learning and Neural Networks Deep Learning and Neural Networks Cross Entropy Loss Gradient Descent Representing Unstructured Image and Text Data Convolutional Neural Networks Natural Language Processing and Recurrent Neural Networks The Transformer Architecture for Sequences Commonly Used and Advanced Neural Network Architectures Advanced Computer Vision Tasks and Wrap-Up Module 4: Evaluation and Metrics for Machine Learning in Healthcare # Introduction to Model Performance Evaluation Overfitting and Underfitting Strategies to Address Overfitting, Underfitting and Introduction to Regularization Statistical Approaches to Model Evaluation Receiver Operator and Precision Recall Curves as Evaluation Metrics Module 5: Strategies and Challenges in Machine Learning in Healthcare # Introduction to Common Clinical Machine Learning Challenges Utility of Causative Model Predictions Context in Clinical Machine Learning Intrinsic Interpretability Medical Data Challenges in Machine Learning Part 1 Medical Data Challenges in Machine Learning Part 2 How Much Data Do We Need? Retrospective Data in Medicine and \u0026ldquo;Shelf Life\u0026rdquo; for Data Medical Data: Quality vs Quantity Module 6: Best Practices, Teams, and Launching Your Machine Learning Journey # Clinical Utility and Output Action Pairing Taking Action - Utilizing the OAP Framework Building Multidisciplinary Teams for Clinical Machine Learning Governance, Ethics, and Best Practices On Being Human in the Era of Clinical Machine Learning Death by GPS and Other Lessons of Automation Bias Module 7: Foundation Models # Introduction to Foundation Models Adapting to Technology General AI and Emergent Behavior How Foundation Models Work Healthcare Use Cases for Text Data Healthcare Use Cases for Non-textual Unstructured Data Challenges and Pitfalls Conclusion "},{"id":2,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/toc_course4/","title":"[ToC] Course 4","section":"C4 AI Evaluations","content":" ToC of Course 4/5: Evaluations of AI Applications in Healthcare # Module 1: AI in Healthcare # Learning Objectives Common Definitions Overview Why AI is needed in Healthcare Examples of AI in Healthcare Growth of AI in Healthcare Questions Answered by AI AI Output Think beyond area under the curve Module 2: Evaluations of AI in Healthcare # Learning Objectives Recap: Framework Stakeholders Clinical Utility Outcome: Action Pairing, An Overview Lead Time Type of Action OAP Examples Number Needed to Treat Net Benefits Decision Curves Feasibility overview Implementation Costs Clinical Evaluation and Uptake Summary Module 3: AI Deployment # Learning Objectives The Problem Practical Questions Prior to Deployment Deployment Pathway Design and Development Stakeholder Involvement Data Type and Sources Settings In Silico Evaluation Net Utility \u0026amp; Work Capacity Statistical Validity Care Integration, Silent Mode Clinical Integration, Considerations Technical Integration Deployment Modalities Continuous Monitoring and Maintenance Challenges of Deployment Sepsis Example Summary Module 4: Downstream Evaluations of AI in Healthcare: Bias and Fairness # Learning Objectives Real World Examples of AI Bias Introduction - Types of Bias Historical Bias Representation Bias Measurement Bias Aggregation Bias Evaluation Bias Deployment Bias What is algorithmic Fairness Anti-classification Parity Classification Calibration Applying Fairness Measures Lack of Transparency Minimal Reporting Standards Opportunities and Challenges Summary Module 5: The Regulatory Environment for AI in Healthcare # Learning Objectives The Problem International Definitions Used for Regulatory Purposes Definition Statement \u0026amp; Risk Framework Valid Clinical Association Analytical Evaluation Clinical Evaluation General Control de novo Notifications Software Modification TPLC Locked vs Adapted AI solutions Examples Non-Regulated Products EU Regulations Chinese Guidelines OMB Guidelines Summary Module 7: AI and Medicine (Optional Content) # Introduction: Navigating the Intersections of AI and Medicine Life Cycle of AI A Deep Dive into Historical and Societal Dimensions Race-Based Medicine and Race-Aware Approach Bias Mitigation Strategies Exploring Potentials and Ethical Quandaries Dismantling Race-Based Medicine Deploying AI into Healthcare Settings Conclusion "},{"id":3,"href":"/ai-workflows/genai-systems/5-day-genai-google/day1_foundational_llm_text_generation/","title":"Day 1 - Foundational LLMs \u0026 Text Generation","section":"5-Day GenAI with Google","content":" Day 1 - Foundational LLMs \u0026amp; Text Generation\u0026quot; # Foundations of LLMs # 1. Why LLMs Matter # Traditional NLP systems were narrow, but Large Language Models (LLMs) offer general-purpose capabilities like translation, Q\u0026amp;A, and summarization—all without explicit task-specific programming.\n→ How do LLMs work under the hood?\n2. What Powers LLMs: The Transformer # The Transformer is the core architecture enabling LLMs. Unlike RNNs that process data sequentially, Transformers handle inputs in parallel using self-attention, allowing them to model long-range dependencies more efficiently and scale training.\n→ But to understand how Transformers process input, we need to examine how input data is prepared.\n3. Input Preparation \u0026amp; Embedding # Before data enters the transformer, it’s tokenized, embedded into high-dimensional vectors, and enhanced with positional encodings to preserve word order. These embeddings become the input that feeds into attention mechanisms.\n→ So, once we have these embeddings—how does the model understand relationships within the input?\n4. Self-Attention and Multi-Head Attention # The self-attention mechanism calculates how each word relates to every other word. Multi-head attention expands on this by letting the model attend to different relationships in parallel (e.g., syntax, co-reference). This enables rich, contextual understanding.\n→ To manage this complexity across layers, the architecture needs stabilization techniques.\n5. Layer Normalization and Residual Connections # To avoid training instability and gradient issues, Transformers use residual connections and layer normalization, ensuring smooth learning across deep layers.\n→ After stabilizing, each layer further transforms the data with an extra module…\n6. Feedforward Layers # Each token\u0026rsquo;s representation is independently refined using position-wise feedforward networks that add depth and non-linearity—enhancing the model’s ability to capture abstract patterns.\n→ With these components, we now have building blocks for the full Transformer structure.\n7. Encoder-Decoder Architecture # In the original Transformer, the encoder turns input text into a contextual representation, and the decoder autoregressively generates output using that context. However, modern LLMs like GPT simplify this by using decoder-only models for direct generation.\n→ As LLMs scale, new architectures emerge to improve efficiency and specialization.\n8. Mixture of Experts (MoE) # MoE architectures use specialized sub-models (experts) activated selectively via a gating mechanism. This allows LLMs to scale massively while using only a portion of the model per input—enabling high performance with lower cost.\n→ But performance isn’t just about architecture—reasoning capabilities are equally vital.\n9. Building Reasoning into LLMs # Reasoning is enabled via multiple strategies:\nChain-of-Thought prompting: Guide the model to generate intermediate steps. Tree-of-Thoughts: Explore reasoning paths via branching. Least-to-Most: Build up from simpler subproblems. Fine-tuning on reasoning datasets and RLHF further optimize for correctness and coherence. → To train these reasoning patterns, we need carefully prepared data and efficient training pipelines.\n10. Training the Transformer # Training involves:\nData preparation: Clean, tokenize, and build vocabulary. Loss calculation: Compare outputs to targets using cross-entropy. Backpropagation: Update weights with optimizers (e.g., Adam). → Depending on the architecture, training objectives differ.\n11. Model-Specific Training Strategies # Decoder-only (e.g., GPT): Predict next token from prior sequence. Encoder-only (e.g., BERT): Mask tokens and reconstruct. Encoder-decoder (e.g., T5): Learn input-to-output mapping for tasks like translation or summarization. Training quality is influenced by context length—longer context allows better modeling of dependencies, but at higher compute cost.\n12. The Evolution Begins: From Attention to Transformers # It started with the 2017 \u0026ldquo;Attention Is All You Need\u0026rdquo; paper—laying the groundwork for all Transformer-based models. This sparked a sequence of breakthroughs in model architecture and training methods.\n13. GPT-1: Unsupervised Pre-training Breakthrough # GPT-1 was a decoder-only model trained on BooksCorpus with a pioneering strategy: pre-train on unlabeled data, fine-tune on supervised tasks. This showed that unsupervised learning scales better than purely supervised models and inspired the unified transformer approach.\n14. BERT: Deep Understanding through Masking # BERT introduced the encoder-only model that trains on masked tokens and sentence relationships. Unlike GPT, it’s not a generator but an understander, excelling in classification, NLU, and inference tasks.\n15. GPT-2: Scaling Up Leads to Zero-Shot Learning # By expanding to 1.5B parameters and using a diverse dataset (WebText), GPT-2 revealed that larger models can generalize better, even to unseen tasks—zero-shot prompting emerged as a surprising capability.\n16. GPT-3 to GPT-4: Generalist Reasoners with Instruction-Tuning # GPT-3 scaled to 175B parameters and needed no fine-tuning for many tasks. Later versions (GPT-3.5, GPT-4) added coding, longer context windows, multimodal inputs, and improved instruction following via InstructGPT and RLHF.\n17. LaMDA: Dialogue-Focused Language Modeling # Google’s LaMDA was purpose-built for open-ended conversations, emphasizing turn-based flow and topic diversity—unlike GPT, which handled more general tasks.\n18. Gopher: Bigger is Smarter (Sometimes) # DeepMind’s Gopher used high-quality MassiveText data. It scaled to 280B parameters and showed that size improves knowledge-intensive tasks, but not always reasoning—hinting at the importance of data quality and task balance.\n19. GLaM: Efficient Scaling with Mixture-of-Experts # GLaM pioneered sparse activation, using only parts of a trillion-parameter network per input. It demonstrated that MoE architectures can outperform dense ones using far less compute.\n20. Chinchilla: The Scaling Laws Revolution # Chinchilla showed that previous scaling laws (Kaplan et al.) were suboptimal. DeepMind proved that data-to-parameter ratio matters—a smaller model trained on more data can outperform much larger ones.\n21. PaLM and PaLM 2: Distributed and Smarter # PaLM (540B) used Google\u0026rsquo;s TPU Pathways for efficient large-scale training. PaLM 2 reduced parameters but improved performance via architectural tweaks, showcasing that smarter design beats brute force.\n22. Gemini Family: Multimodal, Efficient, and Scalable # Gemini models support text, images, audio, and video inputs. Key innovations include:\nMixture-of-experts backbone Context windows up to 10M tokens (Gemini 1.5 Pro) Versions for cloud (Pro), mobile (Nano), and ultra-scale inference (Ultra) Gemini 2.0 Flash enables fast, explainable reasoning for science/math tasks. 23. Gemma: Open-Sourced and Lightweight # Built on Gemini tech, Gemma models are optimized for accessibility. The 2B and 27B variants balance performance and efficiency, with Gemma 3 offering 128K token windows and 140-language support.\n24. LLaMA Series: Meta’s Open Challenger # Meta’s LLaMA models evolved with increased context length and safety. LLaMA 2 introduced chat-optimized variants; LLaMA 3.2 added multilingual and visual capabilities with quantization for on-device use.\n25. Mixtral: Sparse Experts and Open Access # Mistral AI’s Mixtral 8x7B uses sparse MoE with only 13B active params per token, excelling in code and long-context tasks. Instruction-tuned variants rival closed-source models.\n26. OpenAI O1: Internal Chain-of-Thought # OpenAI’s “o1” models use deliberate internal CoT reasoning to excel in programming, science, and Olympiad-level tasks, aiming for thoughtful, high-accuracy outputs.\n27. DeepSeek: RL Without Labels # DeepSeek-R1 uses pure reinforcement learning without labeled data. Their GRPO method enables self-supervised reasoning with rejection sampling and multi-stage fine-tuning, matching “o1” performance.\n28. The Open Frontier # Multiple open models are pushing the boundaries:\nQwen 1.5 (Alibaba): up to 72B params, strong multilingual support. Yi (01.AI): 3.1T token dataset, 200k context length, vision support. Grok 3 (xAI): 1M context tokens, trained with RL for strategic reasoning. 29. Comparing the Giants # Transformer models have scaled in size, context, and capability. From 117M to 1T+ parameters, from 512-token limits to 10M-token contexts. Key insights:\nBigger is not always better—efficiency, data quality, and training methods matter more. Reasoning and instruction-following are now central. Multimodality and retrieval-augmented generation are shaping next-gen LLMs. Fine-Tuning and Using LLMs # 30. From Pretraining to Specialization: Why Fine-Tune? # LLMs are pretrained on broad data to learn general language patterns. But for real-world use, we often need them to follow specific instructions, engage in safe dialogues, or behave reliably. This is where fine-tuning comes in.\n31. Supervised Fine-Tuning (SFT): The First Specialization Step # SFT improves LLM behavior using high-quality labeled datasets. Typical goals:\nBetter instruction-following Multi-turn dialogue (chat) Safer, less toxic outputs Example formats: Q\u0026amp;A, summarization, translations—each with clear input-output training pairs.\n32. Reinforcement Learning from Human Feedback (RLHF) # SFT gives positive examples. But what about discouraging bad outputs? RLHF introduces a reward model trained on human preferences, which then helps guide the LLM via reinforcement learning to:\nPrefer helpful, safe, and fair responses Avoid toxic or misleading completions Advanced variants include RLAIF (AI feedback) and DPO (direct preference optimization) to reduce reliance on human labels.\n33. Parameter Efficient Fine-Tuning (PEFT): Adapting Without Full Retraining # Full fine-tuning is costly. PEFT methods train small, targeted modules instead:\nAdapters: Mini-modules injected into LLM layers, trained separately LoRA: Low-rank matrices update original weights efficiently QLoRA: Quantized LoRA for even lower memory Soft Prompting: Trainable vectors (not full prompts) condition the frozen model PEFT enables plug-and-play modules across tasks, saving memory and time.\n34. Fine-Tuning in Practice (Code Example) # Google Cloud\u0026rsquo;s Vertex AI supports SFT using Gemini models with JSONL datasets and APIs. A few lines of code initialize the model, start fine-tuning, and use the new endpoint—all on cloud infrastructure.\n35. Using LLMs Effectively: Prompt Engineering # LLMs respond differently based on how you ask:\nZero-shot: Just the instruction Few-shot: Add 2–5 examples Chain-of-thought: Show step-by-step reasoning Effective prompting is key to controlling tone, factuality, or creativity.\n36. Sampling Techniques: Controlling Output Style # After generating probabilities, sampling chooses the next token:\nGreedy: Always highest prob (safe but repetitive) Random/Temperature: More creativity Top-K / Top-P: Add diversity while maintaining focus Best-of-N: Generate multiple candidates, choose best Choose based on your goal: safety, creativity, or logic.\n37. Task-Based Evaluation: Beyond Accuracy # As LLMs become foundational platforms, reliable evaluation is critical:\nCustom datasets: Reflect real production use System-level context: Include RAG and workflows, not just model Multi-dimensional “good”: Not just matching ground truth but business outcomes 38. Evaluation Methods # Traditional metrics: Fast but rigid Human evaluation: Gold standard, but costly LLM-powered autoraters: Scalable evaluations with rubrics, rationales, and subtasks Meta-evaluation calibrates autoraters to human preferences—essential for trust.\nConclusion # This section links training, fine-tuning, and usage of LLMs in a production-ready loop:\nTrain generally → fine-tune specifically Prompt smartly → sample selectively Evaluate robustly Together, these techniques ensure LLMs are accurate, safe, helpful, and aligned with real-world needs.\nAccelerating Inference in LLMs # 39. Scaling vs Efficiency: Why Speed Matters Now # LLMs have grown 1000x in parameter count. While quality has improved, cost and latency of inference have also skyrocketed. Developers now face an essential tradeoff: balancing performance with resource efficiency for real-world deployments.\n40. The Big Tradeoffs # a. Quality vs Latency/Cost # Sacrifice a bit of quality for big speed gains (e.g., smaller models, quantization). Works well for simpler tasks where top-tier quality isn\u0026rsquo;t needed. b. Latency vs Cost (Throughput) # Trade speed for bulk efficiency (or vice versa). Useful in scenarios like chatbots (low latency) vs offline processing (high throughput). 41. Output-Approximating Methods # These techniques may slightly affect output quality, but yield major gains in performance.\n🔹 Quantization # Reduce weight/activation precision (e.g., 32-bit → 8-bit). Saves memory and accelerates math operations. Some quality loss, but often negligible with tuning. 🔹 Distillation # Use a smaller student model trained to mimic a larger teacher model. Techniques: Data distillation: Generate synthetic data with teacher. Knowledge distillation: Match student output distributions. On-policy distillation: Reinforcement learning feedback per token. 42. Output-Preserving Methods # These do not degrade quality and should be prioritized.\n🔹 Flash Attention # Optimizes memory movement during attention. 2–4x latency improvement with exact same output. 🔹 Prefix Caching # Cache attention computations (KV Cache) for unchanged inputs. Ideal for chat histories or uploaded documents across multiple queries. 🔹 Speculative Decoding # A small \u0026ldquo;drafter\u0026rdquo; model predicts tokens ahead. Main model verifies in parallel. Huge speed-up with no quality loss, if drafter is well aligned. 43. Batching and Parallelization # Beyond ML-specific tricks, use general system-level methods:\nBatching: Handle multiple decode requests at once. Parallelization: Distribute heavy compute ops across TPUs/GPUs. Decode is memory-bound and can benefit from parallel batching as long as memory limits aren’t exceeded.\nSummary # Inference optimization is about smarter engineering, not just faster chips. You can:\nTrade off quality when it’s safe. Preserve output via caching and algorithmic improvements. Use hybrid setups like speculative decoding + batching. Choose methods based on your task: low-latency chat, high-volume pipelines, or edge deployment. Speed and cost matter—especially at scale.\nApplications and Outlook # 44. LLMs in Action: Real-World Applications # After mastering training, inference, and prompting, the final step is applying LLMs to real tasks. These models have transformed how we interact with information across modalities—text, code, images, audio, and video.\n43. Core Text-Based Applications # 🔹 Code and Mathematics # LLMs support:\nCode generation, completion, debugging, refactoring Test case and documentation generation Language translation between programming languages Tools like AlphaCode 2, FunSearch, and AlphaGeometry push competitive coding and theorem solving to new heights. 🔹 Machine Translation # LLMs understand idioms and context:\nChat translations in apps Culturally-aware e-commerce descriptions Voice translations in travel apps 🔹 Text Summarization # Use cases:\nSummarizing news with tone Creating abstracts for scientific research Thread summaries in chat apps 🔹 Question-Answering # LLMs reason through queries with:\nPersonalization (e.g. in customer support) Depth (e.g. in academic platforms) RAG-enhanced factuality and improved prompts 🔹 Chatbots # Unlike rule-based bots, LLMs handle:\nFashion + support on retail sites Sentiment-aware entertainment moderation 🔹 Content Generation # Ads, marketing, blogs, scriptwriting Use creativity-vs-correctness sampling tuning 🔹 Natural Language Inference # Legal analysis, diagnosis, sentiment detection LLMs bridge subtle context to derive conclusions 🔹 Text Classification # Spam detection, news topic tagging Feedback triage, model scoring as \u0026ldquo;autoraters\u0026rdquo; 🔹 Text Analysis # Market trends from social media Thematic and character analysis in literature 44. Multimodal Applications # Beyond text, multimodal LLMs analyze and generate across data types:\nCreative: Narrate stories from images or video Educational: Personalized visual+audio content Business: Chatbots using both image+text inputs Medical: Scans + notes = richer diagnostics Research: Drug discovery using cross-data fusion Multimodal systems build on unimodal strengths, scaling to more sensory and intelligent interactions.\nSummary # Transformer is the backbone of modern LLMs. Model performance depends on size and training data diversity. Fine-tuning strategies like SFT, RLHF, and safety tuning personalize models for real-world needs. Inference optimization is critical—use PEFT, Flash Attention, prefix caching, and speculative decoding. Prompt engineering and sampling tuning matter for precision or creativity. Applications are exploding—text, code, chat, multimodal interfaces. LLMs are not just tools—they\u0026rsquo;re platforms. They’re reshaping how we search, chat, learn, create, and discover.\n"},{"id":4,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/summary_m1/","title":"[Summary] Module 1: Asking Answering Questions via Clinical DataMining","section":"C2 Clinical Data","content":" Module 1: Asking Answering Questions via Clinical DataMining # 1 Introduction to the data mining workflow # Q: What is the main goal of this course? # A: To explain how clinical data can be used to answer research questions that improve patient and population health.\nQ: What is the structure of the course? # A: It begins with choosing meaningful research questions, followed by understanding the healthcare system, exploring data types, reviewing processing and analysis methods, and addressing bias and error.\nQ: What is the data mining workflow in this context? # A: It consists of four key steps:\nPose a research question Identify suitable data sources Extract and transform data Conduct analysis using appropriate methods Q: Why is posing the research question considered the most important step? # A: Because all subsequent steps rely on having a clear and meaningful question to guide data selection and analysis methods.\n➡️ What makes a research question useful to answer?\n2 Real Life Example # Q: What clinical scenario is used to illustrate the data mining process? # A: A teenager named Laura with systemic lupus erythematosus (SLE) develops proteinuria, pancreatitis, and antiphospholipid antibodies — raising the question of whether she should receive anticoagulant medication.\nQ: What makes this case clinically significant? # A: The complexity and severity of symptoms, especially in a teenager, highlight a rare and serious condition that lacks straightforward treatment guidelines.\nQ: What is the key research question raised? # A: Should teenagers with SLE, proteinuria, and antiphospholipid antibodies be treated with anticoagulants?\nQ: What is the traditional method for answering such questions? # A: Reviewing medical literature or relying on clinical experience.\nQ: Why is data mining proposed instead? # A: Because traditional evidence might be limited or absent for such specific populations, making retrospective data mining valuable for generating insights.\n➡️ How can we find similar patients to inform treatment decisions?\n3 Example: Finding similar patients # Q: What is the next step in answering the clinical question? # A: Extract and transform data from the electronic medical record (EMR) to find patients who match the research criteria.\nQ: What challenge does EMR data present? # A: EMR data are not organized for easy searching and often require clinical expertise to map to useful criteria, like diagnosis codes or lab values.\nQ: How would one identify relevant pediatric patients? # A: By querying based on age to find those under 18, and then filtering for those with a diagnosis code indicating systemic lupus erythematosus (SLE).\nQ: How is proteinuria identified? # A: Through lab values in urine tests.\nQ: How about antiphospholipid antibodies? # A: These are ideally found via numeric lab data, but often reside in unstructured text, requiring natural language processing (NLP) or manual review.\n➡️ How do we estimate risk in patients like Laura using this data?\n4 Example: Estimating risk # Q: What is the objective of the analysis in this case? # A: To estimate the risk of clotting in teenagers with SLE, proteinuria, and antiphospholipid antibodies compared to the baseline risk in teenagers with just SLE.\nQ: How is the patient cohort defined? # A: By selecting patients who meet the specified criteria and identifying which of them had a blood clot event.\nQ: What is a major practical challenge in this step? # A: Extracting usable patient data from the EMR, which is often the bottleneck due to unstructured or fragmented data.\nQ: What assumption is made for the purposes of this example? # A: That the data extraction step is bypassed, allowing focus directly on the risk analysis.\n➡️ How can patient data be visualized over time using timelines?\n5 Putting patient data on timeline # Q: What is the purpose of using a patient timeline? # A: To visually arrange and analyze a patient\u0026rsquo;s clinical data chronologically to understand event sequences and compute outcomes like risk.\nQ: What types of data are placed on the timeline? # A: Diagnosis codes, lab results, medication orders, and clinician notes from the EMR.\nQ: How are relevant patients identified and flagged? # A: Pediatric patients are filtered first, followed by those with SLE, and then those developing proteinuria and antiphospholipid antibodies.\nQ: What does the timeline help determine in this context? # A: Whether a patient developed a blood clot after the onset of each condition, enabling computation of relative risk based on timing.\n➡️ How do we revisit and refine the steps of the data mining workflow?\n6 Revisit the data mining workflow steps # Q: What was the clinical question in the example revisited here? # A: Whether the risk of clotting in teenagers with SLE, proteinuria, and antiphospholipid antibodies justifies treatment with anticoagulants.\nQ: What was the data source? # A: The Electronic Medical Record (EMR).\nQ: What steps were involved in extracting and transforming the data? # A: Identifying teenagers with SLE, forming subgroups based on clinical criteria, using diagnosis codes, crafting search terms, and occasionally applying proxy terms with follow-up confirmation.\nQ: How was analysis conducted? # A: By comparing clotting risk in subgroups to the general risk in teenagers with SLE, guiding the decision to treat.\nQ: What is the broader implication of this example? # A: It introduces the core elements of the data mining workflow, which will be elaborated in the course—especially regarding accurate execution and bias mitigation.\n➡️ What types of research questions can be asked using clinical data?\n7 Types of research questions # Q: Why is it important to understand types of research questions? # A: It helps in formulating one\u0026rsquo;s own questions and critically evaluating the validity of others’ findings using clinical data.\nQ: What is the simplest type of research question? # A: Descriptive questions — they summarize data using counts or proportions, e.g., \u0026ldquo;What proportion of the population has familial hypercholesterolemia?\u0026rdquo;\nQ: What comes after descriptive questions? # A: Exploratory questions — they aim to find patterns in data, such as identifying subtypes of a disease like autism.\nQ: What types of methods are used in exploratory questions? # A: Techniques like clustering or statistical modeling to identify patterns without predefined hypotheses.\n➡️ Which research questions are best suited for clinical data?\n8 Research questions suited for clinical data # Q: What types of questions are clinical data best suited to answer? # A: Descriptive, exploratory, inferential, and predictive questions.\nQ: What types of questions are harder to answer with clinical data? # A: Causal and mechanistic questions, which often require carefully designed experiments and new data collection.\nQ: What are the two primary goals of asking research questions in medicine? # A:\nRisk stratification — determining whether to treat a patient. Data-driven treatment selection — deciding how best to treat a patient. Q: Why is it important to match question type with purpose? # A: To ensure both the research question and analysis methods are appropriate and meaningful for clinical decision-making.\n➡️ How does data mining help in making treatment decisions?\n9 Example: making decision to treat # Q: What distinction is made between the question asked and the question answered? # A: The original question was whether to treat Laura, a specific patient. The analysis, however, answered a descriptive question — what proportion of similar patients developed clots.\nQ: What kind of analysis was conducted? # A: A descriptive risk stratification, grouping patients based on risk of clotting using historical data.\nQ: What assumption underlies the application of this analysis to Laura? # A: That Laura’s outcome is likely to mirror those of similar past patients.\nQ: How does this analysis support decision-making? # A: It helps determine whether Laura belongs in a high-risk group, supporting a treatment recommendation such as anticoagulation.\nQ: What additional considerations are necessary in real life? # A: We must also consider the risks of adverse events from the proposed treatment before making a final clinical decision.\n10 Properties that make answering a research question useful # Q: What determines whether answering a question is useful in clinical data mining? # A: Usefulness is assessed via a checklist of aspects, not a strict formula. Key considerations include impact, actionability, and downstream effects.\nQ: What is the first major factor? # A: The number of lives affected — including the disease burden and scope of patient populations influenced by the answer.\nQ: What is the second factor? # A: The probability that the results will lead to beneficial changes for clinicians or patients, improving clinical care or outcomes.\nQ: What is the third key aspect? # A: The real-world consequences: does the answer help reduce mortality/morbidity, lower healthcare costs, increase care access, or guide medical decisions?\nQ: What is an important implication of this approach? # A: Slight rephrasing of the question can often make it significantly more useful or relevant.\n➡️ How is clinical data mining used in a real-world example?\n"},{"id":5,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/","title":"C2 Clinical Data","section":"AI in Healthcare","content":" 📘 Course 2: Clinical Data # [ToC] Course 2 [Summary] Module 1: Asking Answering Questions via Clinical DataMining [Summary] Module2: Data Available From Healthcare Systems [Summary] Module3: Representing Time Timing Events For Clinical Data Mining [Summary] Module4 : Creating Analysis Ready Dataset from Patient Timelines Clinical Text Feature Extraction Using Dictionary-Based Filtering Clinical Text Mining Pipeline (Steps 1–5) Ethics in AI for Healthcare Missing Data Scenarios in Healthcare Modeling OMOP vs. RLHF Rule-Based Electronic Phenotyping Example: Type 2 Diabetes 🧭 Module 1: Asking and Answering Questions via Clinical Data Mining # 1. What\u0026rsquo;s the Problem?\nClinicians and researchers have important questions but lack a structured approach to answering them using clinical data.\n2. Why Does It Matter?\nWithout a systematic workflow, decisions may rely on anecdotal evidence or outdated knowledge, leading to suboptimal care.\n3. What\u0026rsquo;s the Core Idea?\nThe 4-step clinical data mining workflow: (1) Ask the right question → (2) Find suitable data → (3) Extract/transform data → (4) Analyze and iterate.\n4. How Does It Work?\nStart with a real clinical scenario, define inclusion/exclusion criteria, search EMRs using codes/tests, and compute outcomes. Use a timeline and patient-feature matrix to support decisions.\n5. What\u0026rsquo;s Next?\nThis foundation enables accurate data selection (Module 2), temporal modeling (Module 3), and building datasets (Module 4).\n🏥 Module 2: Data Available from Healthcare Systems # 1. What\u0026rsquo;s the Problem?\nHealthcare data is fragmented, inconsistently coded, and filled with biases and errors.\n2. Why Does It Matter?\nUsing flawed or incomplete data without understanding its origin can lead to misleading conclusions or unsafe decisions.\n3. What\u0026rsquo;s the Core Idea?\nCategorize and understand different healthcare data types, sources, and their limitations, including EMR, claims, registries, and patient-generated data.\n4. How Does It Work?\nStudy the roles of key actors (patients, providers, payers), structured vs. unstructured data types, and typical biases (selection, misclassification, incentives).\n5. What\u0026rsquo;s Next?\nProvides the context for building timelines (Module 3) and feature matrices (Module 4) while recognizing biases that need correction.\n🕰️ Module 3: Representing Time in Clinical Data # 1. What\u0026rsquo;s the Problem?\nMost databases don\u0026rsquo;t represent or reason well about time, yet clinical reasoning depends heavily on event timing.\n2. Why Does It Matter?\nIncorrect ordering or missing timestamps can invalidate exposure-outcome relationships and confuse chronic vs. acute processes.\n3. What\u0026rsquo;s the Core Idea?\nUse patient timelines and time-aware logic to represent, bin, and reason about clinical events over time.\n4. How Does It Work?\nDefine index times, use bins to aggregate events, calculate time-to-event, handle censoring, and test for non-stationarity.\n5. What\u0026rsquo;s Next?\nEstablishes the temporal framework needed for building structured datasets (Module 4) and modeling disease progression (Module 6).\n🧱 Module 4: Creating Analysis-Ready Datasets # 1. What\u0026rsquo;s the Problem?\nRaw timelines are complex and inconsistent — they can\u0026rsquo;t be directly used in analysis or machine learning.\n2. Why Does It Matter?\nPoor feature engineering or ignoring missingness leads to weak, biased, or uninterpretable models.\n3. What\u0026rsquo;s the Core Idea?\nBuild a patient-feature matrix by selecting, cleaning, imputing, and engineering features from structured/unstructured data.\n4. How Does It Work?\nStandardize features, reduce dimensionality, handle missingness with imputation or removal, and use domain knowledge or PCA to create meaningful features.\n5. What\u0026rsquo;s Next?\nFeeds directly into downstream modeling, classification (Module 6), and cohort identification with better interpretability.\n📄 Module 5: Handling Unstructured Data # 1. What\u0026rsquo;s the Problem?\nValuable clinical information is trapped in unstructured formats like notes, images, and signals.\n2. Why Does It Matter?\nFailing to extract this information limits your ability to detect key conditions, traits, or outcomes that are not coded elsewhere.\n3. What\u0026rsquo;s the Core Idea?\nUse text mining, signal processing, and image interpretation to turn unstructured data into usable features.\n4. How Does It Work?\nApply NLP (e.g., negation/context detection), use knowledge graphs for term recognition, and process signals/images with appropriate tools.\n5. What\u0026rsquo;s Next?\nEnhances the patient-feature matrix (Module 4) and improves phenotyping accuracy and completeness (Module 6).\n🧬 Module 6: Electronic Phenotyping # 1. What\u0026rsquo;s the Problem?\nIdentifying who truly has a disease or condition is challenging using only raw or coded data.\n2. Why Does It Matter?\nMisclassified patients lead to invalid cohorts, incorrect inferences, and flawed clinical decisions or model training.\n3. What\u0026rsquo;s the Core Idea?\nDefine phenotypes using rule-based or probabilistic methods to accurately identify conditions of interest.\n4. How Does It Work?\nUse inclusion/exclusion criteria (rule-based) or train classifiers (probabilistic) with anchors, weak labels, and features from Modules 4–5.\n5. What\u0026rsquo;s Next?\nEnables reliable cohort creation for clinical trials, observational studies, and AI/ML applications.\n⚖️ Module 7: Clinical Data Ethics # 1. What\u0026rsquo;s the Problem?\nUsing patient data without safeguards risks violating privacy, losing trust, and causing harm.\n2. Why Does It Matter?\nUnethical data use can lead to legal issues, exclusion of vulnerable groups, and poor public perception of healthcare AI.\n3. What\u0026rsquo;s the Core Idea?\nApply ethical frameworks like the Belmont Report and Learning Health System to govern data use, consent, and fairness.\n4. How Does It Work?\nEnsure de-identification, obtain proper consent (or waiver), handle return of results thoughtfully, and consider justice in access and outcomes.\n5. What\u0026rsquo;s Next?\nProvides ethical boundaries and practices for applying all previous modules responsibly in real-world systems.\n"},{"id":6,"href":"/ai-workflows/genai-systems/5-day-genai-google/day1_prompt_engineering/","title":"Day 1 – Prompt Engineering","section":"5-Day GenAI with Google","content":" Day 1 – Prompt Engineering # 1. Why Prompt Engineering Matters # We start with the need for controlling LLM behavior. Although everyone can write prompts, crafting high-quality prompts is complex. The model, structure, tone, and context all affect the outcome. Prompt engineering is an iterative process requiring optimization and experimentation.\n→ how do we guide LLMs effectively without retraining them?\n2. How LLMs Predict Text # LLMs are token prediction machines. They predict the next likely token based on previous tokens and training data. Prompt engineering means designing inputs that lead the model toward the desired outputs using this prediction mechanism.\n→ how do prompt structure and context impact token prediction?\n3. Controlling Output Length # Setting the number of output tokens affects cost, latency, and completeness. Shorter outputs don’t make the model more concise—they just truncate the output. Prompts must be adapted accordingly.\n→ how do we engineer prompts to work well with shorter output limits?\n4. Temperature – Controlling Randomness # Temperature tunes the creativity vs. determinism of responses. Lower = more deterministic. Higher = more diverse. Temperature = 0 means always selecting the highest-probability token.\n→ what randomness level best matches your use case: precision or creativity?\n5. Top-K vs. Top-P # Top-K selects from the K most likely tokens. Top-P includes tokens whose cumulative probability is under P. Together with temperature, they control diversity. Improper values can lead to repetition loops or incoherence.\n→ what is the optimal balance between relevance and novelty?\n6. Putting Sampling Together # The sampling settings interact:\nTemp=0 overrides others (most probable token only) Top-K=1 behaves similarly (greedy decoding) At extremes, sampling settings may cancel out or be ignored → how do we experiment with sampling settings to avoid repetition and improve quality?\n7. Zero-shot Prompting # The simplest form—just a task or question without examples. Effective when LLMs are pre-trained well. Clarity in phrasing is key.\n→ how do we design zero-shot prompts that still get structured, accurate answers?\n8. One-shot and Few-shot Prompting # One-shot: One example is provided before the prompt. Few-shot: Multiple examples guide the model’s pattern recognition. Ideal for steering structure and increasing precision. → how many examples are enough for complex or high-variance tasks?\n9. System Prompting # Defines the LLM’s role and constraints at a high level—such as format, safety rules, or output requirements. It’s useful to enforce style, format, or structure like JSON outputs.\n→ how can we use system prompts to enforce safety and structured outputs?\n10. Role Prompting # Assigns a persona (e.g., travel guide, teacher). This helps shape tone, depth, and relevance of the response. Adding style (humorous, formal) further guides model behavior.\n→ how do personas influence LLM outputs in nuanced ways?\n11. Contextual Prompting # Injects situational context into the prompt to make responses more accurate. Effective for dynamic environments (e.g., blogs, customer support).\n→ how can contextual prompts adapt to real-time or user-specific tasks?\n12. Step-back Prompting # Starts with a general question to activate background knowledge before solving a specific task. Encourages critical thinking and can reduce bias.\n→ how do we leverage LLMs\u0026rsquo; latent knowledge more strategically?\n13. Chain of Thought (CoT) # LLMs struggle with math and logic unless they break problems into steps. Chain of Thought (CoT) prompting makes the model reason step by step. This adds interpretability, reduces drift across models, and improves answer accuracy.\n→ how can we make reasoning visible to both developers and users?\n14. Self-Consistency # Instead of relying on a single reasoning path, Self-Consistency samples multiple responses (with higher temperature), then picks the majority answer. It increases reliability—especially for ambiguous or hard-to-evaluate tasks.\n→ how can we trade off cost for improved reliability in decision-critical tasks?\n15. Tree of Thoughts (ToT) # Generalizes CoT by enabling multiple reasoning paths at once. Like a decision tree, the model explores and evaluates different intermediate steps before selecting the best route. Powerful for complex planning and exploration.\n→ what’s the best way to structure exploration and backtracking in LLM reasoning?\n16. ReAct (Reason + Act) # ReAct prompts mix reasoning with external tool calls (e.g., Google Search). It creates a loop: Think → Act → Observe → Rethink. This enables LLMs to handle multi-step problems using real-world data or APIs.\n→ how can we design LLMs that interactively use tools and adapt in real time?\n17. Automatic Prompt Engineering (APE) # Prompts are hard to write. APE uses LLMs to generate and refine their own prompts. You ask the model to create N variations of a task prompt, then rank and select the best one based on performance metrics (e.g., BLEU, ROUGE).\nFinal insight: what happens when LLMs become their own prompt engineers—and how can we guide that process safely?\n18. Prompts for Writing Code # LLMs like Gemini can generate well-documented scripts, e.g., renaming files with Bash. It reduces developer overhead for common tasks. Prompts should include goal, language, and behavior clearly.\n→ how do we craft prompts that result in reusable, safe, and tested code?\n19. Prompts for Explaining Code # LLMs can reverse-engineer logic from code. Useful in team settings or code reviews. Helps onboard new developers or document legacy scripts.\n→ how do we evaluate explanation correctness—especially for critical systems?\n20. Prompts for Translating Code # Language models can convert code between languages (e.g., Bash → Python). This helps modernize or modularize projects while preserving logic.\n→ what risks emerge in translation—syntax, dependencies, or behavior drift?\n21. Prompts for Debugging and Reviewing Code # Prompting LLMs to diagnose bugs or suggest improvements enhances development speed. Common mistakes like undefined functions can be spotted easily.\n→ how do we ensure debugging prompts scale with complex codebases?\n22. Multimodal Prompting # Combines inputs like text, images, and audio. Enables more flexible, human-like interaction. Useful in complex workflows or accessibility tasks.\n→ how do we design for alignment across different input modalities?\n23. Best Practice – Provide Examples # Incorporating examples (one-shot or few-shot) within prompts is the most reliable way to guide output. Acts as a template and sets style/tone expectations.\nFinal reflection: prompt engineering is more than writing—it\u0026rsquo;s designing a user interface for the LLM.\n24. Design with Simplicity # Clear, concise prompts yield better responses. Avoid overloading with context or ambiguous language. Use active verbs and break down complex requests.\n→ how do we optimize for both human and machine comprehension in prompt design?\n25. Be Specific About Output # Specify the desired format, length, tone, and structure to reduce ambiguity and improve relevance. Instructions guide better than vague constraints.\n→ how can we use instructions to improve precision without overconstraining the model?\n26. Use Variables # Abstract prompts using variables (e.g., {city}) to enhance reusability in apps or RAG systems. Helps modularize and scale prompt templates.\n→ how can prompt modularity boost automation and maintainability?\n27. Experiment with Formats and Styles # Vary phrasing—questions, statements, or instructions. Try structured outputs (e.g., JSON) and track results for consistency and quality.\n→ how do output formats affect hallucination, readability, and parse-ability?\n28. Work With Schemas # Use structured input/output formats like JSON Schema to guide the model’s understanding. This enables field-level alignment and supports reasoning over attributes.\n→ how do schemas improve accuracy in structured reasoning tasks like RAG or product gen?\n29. Best Practices for Chain-of-Thought (CoT) # Always place the final answer after reasoning Use temperature=0 for deterministic outputs Separate reasoning from the final output for evals → how do we reliably extract and score CoT answers in evaluation pipelines?\n30. Document Prompt Experiments # Track prompt versions, models, sampling settings, and outputs using a table or Google Sheet. Log RAG details and system changes to trace variation.\nFinal principle: prompting is experimental—track what you try, and improve iteratively.\nFinal Best Practices and Wrap-Up # 24. Design with Simplicity # Use concise language and clear goals. Overly complex or vague prompts confuse both the user and the model.\n→ how do we turn messy input into structured guidance for LLMs?\n25. Be Specific About Output # Specificity in instructions (e.g., format, style, length) yields more relevant, focused outputs.\n→ how do we align prompts with precise user needs and formats?\n26. Prefer Instructions Over Constraints # Positive instructions are more intuitive than a list of “don’ts.” Use constraints when safety or exact formatting is critical.\n→ how do we encourage creativity while staying on target?\nVariables, Versions, and Formats # 27. Use Variables in Prompts # Dynamic placeholders (e.g., {city}) improve reusability and maintainability—especially in production pipelines.\n→ how do we modularize prompt logic for reuse across systems?\n28. Experiment with Input and Output Formats # Try different styles—question, statement, instruction—and output formats like JSON. JSON helps structure data and reduce hallucinations.\n→ how do we balance human readability with system parsing needs?\nPrompt Engineering Discipline # 29. Use Schemas for Input and Output # JSON schemas define structure and types—great for grounding LLM understanding and making output usable in applications.\n→ how can we give LLMs structured \u0026ldquo;expectations\u0026rdquo; to reduce drift?\n30. Document Prompt Versions # Track all attempts, model versions, and outcomes in structured logs. Helps with reproducibility, debugging, and future upgrades.\nFinal insight: prompt engineering is iterative design—every prompt is a versioned artifact.\nSummary # Prompt types: zero/few-shot, role, system, contextual Reasoning: CoT, ToT, ReAct, Self-consistency, Step-back Automation: APE Code prompting: generate, translate, debug Multimodal and schema-guided prompting Best practices: Evaluation, formatting, variables, and documentation The journey from text to structured reasoning begins with a well-crafted prompt.\n"},{"id":7,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/summary_m2/","title":"[Summary] Module2: Data Available From Healthcare Systems","section":"C2 Clinical Data","content":" Module2: Data Available From Healthcare Systems # 1 Review of the healthcare system # Q: What is the focus of this module\u0026rsquo;s introduction? # A: To demonstrate how clinical data from the healthcare system can be used to ask and answer meaningful research questions.\nQ: What key topics are introduced in this session? # A:\nCommon data sources in healthcare Types of data generated Systematic inaccuracies in the data Strategies for working with imperfect data Q: How does this connect to the earlier module? # A: It builds on the discussion of research question formulation and data mining workflows by diving into real-world data availability and limitations.\n➡️ What entities exist in the healthcare system and what data do they collect?\n2 Review of key entities and the data they collect # Q: Who are the key entities in the healthcare system that generate data? # A:\nPatients : Generate little data when healthy, but more when they seek treatment. Search Engines : Capture data from patients researching health online. Healthcare Providers : Generate EMR data including diagnoses, test orders, and prescriptions. Pharmacies : Record dispensing data. Pharmaceutical Companies : Design and manufacture drugs. Insurers/Payers : Capture claims and billing data. Q: What kind of data is produced at each step of care? # A: Data are produced at nearly every step — from search behavior before treatment, to EMR entries during care, to dispensing records from pharmacies, and payment data from insurers.\n➡️ Who are the actors in the healthcare system and what interests do they have?\n3 Actors with different interests # Q: Who are the main actors in the healthcare system? # A:\nPatients and their families Healthcare professionals Payers (e.g. insurance companies) Government and regulatory agencies Q: What are the interests of these different actors? # A:\nPatients want to stay healthy and recover quickly. Clinicians seek best practices to maximize benefit and minimize harm. Payers and regulators aim for societal cost-effectiveness, fairness, and justice. Q: How do differing interests create conflict? # A: A patient may want to pursue a high-risk treatment for hope, while a clinician may focus on safety, and insurers may prioritize cost-effectiveness — leading to tension in care decisions.\n➡️ What are the common data types used in healthcare?\n4 Common data types in Healthcare # Q: What are the major types of healthcare data? # A: Structured and unstructured data.\nQ: What is structured data? # A: Data organized in consistent formats like tables with rows (patients) and columns (attributes such as age, DOB). Missing values are often marked with placeholders like \u0026ldquo;NA\u0026rdquo;.\nQ: What is unstructured data? # A: Data without a uniform format, including clinical notes, images, biomedical signals, and free text.\nQ: Why is this distinction important? # A: Structured data is easier to analyze computationally, while unstructured data requires specialized methods (e.g. NLP, image processing) to extract insights.\n➡️ What are the strengths and weaknesses of observational healthcare data?\n5 Strengths and weaknesses of observational data # Q: What are observational data in healthcare? # A: Data collected during routine clinical care for the primary purpose of delivering care — not for research. They\u0026rsquo;re used secondarily for analysis.\nQ: What are the main strengths of observational data? # A:\nLarge scale : Covers millions to billions of records, capturing rare events. Real-world relevance : Reflects actual clinical practices and patient behaviors. Efficiency : Already collected, so no additional data gathering is needed. Q: What are the weaknesses of observational data? # A:\nBias and incompleteness : Not collected with research goals in mind. Lack of standardization : Varies across sites and systems. Potential confounding : Causal inference is difficult without randomized control. ➡️ What are the biases and errors introduced by the healthcare system itself?\n6 Bias and error from the healthcare system perspective # Q: How can data from the healthcare system be biased or inaccurate? # A: Each entity in the healthcare system contributes potential sources of bias, especially due to who seeks care and how care is recorded.\nQ: What is selection bias in this context? # A: It occurs when only certain patients (e.g., those who seek care) are recorded, leaving out healthy individuals or those managing illness at home or outside the system.\nQ: What patient-level factors influence bias? # A: Health literacy, financial situation, insurance coverage, and cultural beliefs can all affect whether and how patients engage with the healthcare system.\n➡️ How do biases and errors affect recorded exposures and outcomes?\n7 Bias and error of exposures and outcomes # Q: What are exposures and outcomes in clinical data analysis? # A:\nExposures are events or conditions that occur to a patient (e.g., diseases, procedures, medications). Outcomes are events or conditions assessed after the exposure (e.g., complications, lab results, costs). Q: Why is distinguishing exposures and outcomes important? # A: It provides a framework for analyzing how prior events influence later results and helps in identifying where bias or error may occur.\nQ: What type of biases can arise in this context? # A: Misclassification, timing errors, or missing data related to either exposures or outcomes can distort analysis and conclusions.\n➡️ How can patient exposure be misclassified in clinical data?\n8 How a patient exposure might be misclassified # Q: How can exposure to medication be misclassified in clinical data? # A: When prescription records don’t align with actual medication use, such as delays in filling, use of free samples, or interruptions in adherence.\nQ: What example illustrates this misclassification? # A: A doctor gives a 15-day free sample before a prescription is filled. The patient begins treatment immediately, but data may only show the pharmacy fill date, not the true start date.\nQ: Why does this matter? # A: Misclassification of the timing or existence of exposure can distort analyses linking exposures to outcomes, especially for time-sensitive effects.\n➡️ How might a patient outcome be misclassified in clinical records?\n9 How a patient outcome could be misclassified # Q: How can a patient’s outcome be misclassified in the medical record? # A: Sometimes a diagnosis code is added based on suspicion (e.g., diabetes) before a condition is confirmed, and it may remain even if the diagnosis is later ruled out.\nQ: What strategies help reduce outcome misclassification? # A:\nRequire multiple instances of a diagnosis code. Pair diagnosis codes with procedure codes specific to the condition. Look for treatment or intervention evidence supporting the diagnosis. Q: Why are procedure codes more reliable? # A: Procedures are typically documented only if they were actually performed, adding confirmatory weight to a diagnosis.\n➡️ What are the key sources of electronic medical record data?\n10 Electronic medical record data # Q: What are electronic medical records (EMRs)? # A: EMRs are digital versions of the traditional paper patient charts. They store detailed information collected in clinical settings.\nQ: What types of data are typically stored in EMRs? # A:\nPatient demographics Diagnosis and procedure codes Clinical notes Medication records Imaging and lab test results (Increasingly) genetic test results and wearable device data Q: How are EMRs generated? # A: As a byproduct of routine clinical care and documentation processes in hospitals and clinics.\nQ: Are EMRs and EHRs the same? # A: The terms are often used interchangeably, though technically EHR may imply a broader, longitudinal view across providers.\n➡️ What can we learn from claims data in healthcare systems?\n11 Claims data # Q: What are claims data in healthcare? # A: Claims data are records generated when healthcare providers submit bills to insurers for services rendered.\nQ: What information do claims typically include? # A:\nPatient identifiers Insurance status Diagnosis and procedure codes Requested charges and actual payments Q: How is coding related to billing? # A: Specific codes are used to categorize services for billing. These may differ between clinician entries, hospital submissions, and what insurers reimburse.\nQ: Why are claims data valuable? # A: They include detailed information on costs, utilization, and provider-payer interactions — data often missing from EMRs.\n➡️ What data do pharmacies provide, and how is it used?\n12 Pharmacy # Q: What kind of data do pharmacies provide? # A: They document when prescriptions are written, filled, and paid for — offering insight into medication access and fulfillment.\nQ: Why is pharmacy data valuable? # A: It goes beyond prescription intent (EMR) to show that a patient actually obtained the medication, which is a step closer to actual use.\nQ: What are limitations of pharmacy data? # A: It doesn\u0026rsquo;t guarantee the patient took the medication — only that it was picked up.\nQ: How can pharmacy records be fragmented? # A: Patients may use multiple sources: retail chains (e.g., CVS), mail-order services, and online pharmacies, dispersing data across datasets.\n➡️ What are surveillance datasets and registries, and how are they used?\n13 Surveillance datasets and Registries # Q: What are surveillance datasets and why are they important? # A: They monitor adverse events and side effects of drugs or devices after approval (post-marketing surveillance) to catch safety issues early.\nQ: What are examples of surveillance systems in the U.S.? # A:\nFAERS : FDA Adverse Event Reporting System MAUDE : Manufacturer and User Facility Device Experience database Q: Who uses these datasets and for what purpose? # A: Government agencies like the FDA and CDC use them to track disease outbreaks (e.g. flu, Ebola) and monitor product safety. Local and state agencies often assist.\nQ: What are registries? # A: Registries are organized systems maintained by agencies or societies to collect consistent clinical data on specific conditions, devices, or populations.\n➡️ What are population health datasets and how are they used?\n14 Population health data sets # Q: How do population health datasets differ from patient-centric data? # A: They focus on aggregated health trends, costs, and resource use across populations — not on individual patient records.\nQ: What are key examples of U.S. population health datasets? # A:\nNational Inpatient Sample (NIS) : Tracks hospital resource utilization, costs, and outcomes. Medical Expenditure Panel Survey (MEPS) : Surveys patients, providers, and employers on healthcare usage and spending. NHANES : Measures demographic, nutritional, and health variables through national surveys conducted by the CDC. Q: Why are these datasets useful? # A: They offer broad insights into national health patterns, disparities, and costs — helping guide public policy and resource planning.\n➡️ How do we assess if a healthcare data source is useful?\n15 A framework to assess if a data source is useful # Q: What questions should you ask to assess a healthcare dataset\u0026rsquo;s usefulness? # A:\nIs there a well-documented data model? Poor or missing documentation makes the data hard to use. What is the data provenance? Understand how and where the data were collected. Are the data accessible and in what form? Consider legal restrictions and costs. What known errors or missingness exist? Evaluate data quality and be prepared to address gaps. Are data standards used (e.g., vocabularies or formats)? Standardization affects interoperability and analysis readiness. Q: Why is this framework important? # A: It helps researchers avoid costly or infeasible data efforts and ensures they can trust and interpret results from the dataset effectively.\n"},{"id":8,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/","title":"C3 ML Healthcare","section":"AI in Healthcare","content":" 📘 Course 3: Fundamentals of ML for Healthcare # [ToC] Course 3 [Summary] Module 3: Concepts and Principles of ML in Healthcare [Summary] Module 4: Evaluation and Metrics for ML in Healthcare [Summary] Module 5: Strategies and Challenges in ML for Healthcare [Summary] Module 6: Best Practices, Terms, and Launching Your ML Journey [Summary] Module 7: Foundation Models Case Study: The Hidden Danger of Correlation in Healthcare AI Categories of Machine Learning Applications in Healthcare Data Quality, Labeling, and Weak Supervision in Clinical ML Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations Foundation Models for Healthcare Healthcare Use Cases for Non-textual Unstructured Data Healthcare Use Cases for Text Data How Foundation Models Work Output-Action Pairing (OAP) Framework in Healthcare Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare 🤖 Module 1: Why Machine Learning in Healthcare? # 1. What’s the Problem?\nUnderstanding how ML fits into the healthcare ecosystem and why traditional models are insufficient.\n2. Why Does It Matter?\nML has the potential to improve diagnosis, patient care, and reduce costs, but it also raises ethical and technical concerns.\n3. What’s the Core Idea?\nMachine learning leverages data to improve healthcare predictions, enabled by access to digital health data and modern computing.\n4. How Does It Work?\nML systems learn mappings between inputs and outputs from data, using statistical methods rather than hard-coded rules.\n5. What’s Next?\nExplore the basic types and terminology of ML, including supervised and unsupervised learning.\n📘 Module 2: Concepts and Principles of ML in Healthcare Part 1 # 1. What’s the Problem?\nClarifying foundational ML terms and how ML differs from traditional programming.\n2. Why Does It Matter?\nFoundational understanding is necessary for applying ML models correctly in clinical settings.\n3. What’s the Core Idea?\nML learns functions that map inputs to outputs using labeled (or unlabeled) data. Supervised learning uses labels; unsupervised does not.\n4. How Does It Work?\nML involves preprocessing data, training models, validating predictions, and testing accuracy with structured data splits.\n5. What’s Next?\nDelve into deep learning, neural network architectures, and their applications in healthcare.\n🧠 Module 3: Concepts and Principles of ML in Healthcare Part 2 # 1. What’s the Problem?\nApplying and interpreting complex ML models like deep neural networks in healthcare.\n2. Why Does It Matter?\nNeural networks enable breakthroughs in imaging and text analysis but require understanding to avoid misuse.\n3. What’s the Core Idea?\nDeep learning uses layers of neurons to learn complex mappings. CNNs are suited for images; RNNs and Transformers for sequences.\n4. How Does It Work?\nTraining uses backpropagation and loss optimization. Model performance is evaluated with metrics like AUROC, accuracy, and precision.\n5. What’s Next?\nEvaluate ML models using statistical metrics, assess overfitting, and explore model generalizability.\n📊 Module 4: Evaluation and Metrics for Machine Learning in Healthcare # 1. What’s the Problem?\nEnsuring ML models are reliable and generalizable before clinical deployment.\n2. Why Does It Matter?\nPoorly evaluated models may be unsafe or ineffective in high-stakes healthcare settings.\n3. What’s the Core Idea?\nEvaluation includes accuracy, AUROC, precision-recall, and more. Proper data splits and validation strategies are critical.\n4. How Does It Work?\nUse learning curves, loss plots, cross-validation, and hyperparameter tuning to evaluate models. Choose appropriate thresholds.\n5. What’s Next?\nUnderstand practical barriers and strategies for deploying ML in real clinical environments.\n🛠️ Module 5: Strategies and Challenges in Machine Learning in Healthcare # 1. What’s the Problem?\nDealing with real-world limitations like data bias, label noise, interpretability, and clinical relevance.\n2. Why Does It Matter?\nModels that lack robustness or interpretability may fail in practice or perpetuate health disparities.\n3. What’s the Core Idea?\nTactics include regularization, domain-specific feature engineering, human-centered design, and sensitivity to healthcare context.\n4. How Does It Work?\nApply dropout, saliency mapping, ensemble learning, and transparent reporting. Collaborate with clinicians for contextual validation.\n5. What’s Next?\nBuild multidisciplinary teams and prepare for real-world deployment including ethical review and monitoring.\n🚀 Module 6: Best Practices, Teams, and Launching Your ML Journey # 1. What’s the Problem?\nBridging the gap between ML research and real-world clinical implementation.\n2. Why Does It Matter?\nSuccess depends on team composition, ethics, data stewardship, and designing for human-AI interaction.\n3. What’s the Core Idea?\nUse frameworks like Output-Action Pairing to define measurable goals. Involve stakeholders throughout the development cycle.\n4. How Does It Work?\nForm teams with technical, clinical, ethical, and operational expertise. Start small, iterate, and evaluate continuously.\n5. What’s Next?\nIdentify your project focus, assemble collaborators, and begin experimenting responsibly.\n"},{"id":9,"href":"/ai-workflows/genai-systems/5-day-genai-google/day2_embeddings_vectordb/","title":"Day 2 – Embeddings \u0026 Vector Databases","section":"5-Day GenAI with Google","content":" Day 2 – Embeddings \u0026amp; Vector Databases # 1. Why Embeddings? # We begin with the core problem of representing diverse data types. Images, text, audio, and structured data all need to be compared, retrieved, and clustered. Embeddings map these into a shared vector space where similarity can be computed numerically.\n→ how can we measure and preserve semantic meaning across different data types?\n2. Mapping Data to Vector Space # Embeddings reduce dimensionality while preserving meaning. For example, just like latitude and longitude embed Earth’s surface into 2D coordinates, BERT embeds text into 768D space. Distances represent semantic similarity.\n→ how do different embedding models impact representation fidelity and downstream performance?\n3. Key Applications # Embeddings power:\nSearch (e.g., RAG, internet-scale) Recommendations Fraud detection Multimodal integration (e.g., text + image) → how do we design joint embeddings for multi-modal tasks?\n4. Quality Metrics # Evaluation focuses on how well embeddings retrieve similar items:\nPrecision@k: Are top results relevant? Recall@k: Do we get all relevant items? nDCG: Are the most relevant ranked highest? → how can evaluation help us improve and select embedding models for specific applications?\n5. RAG and Semantic Search # A standard setup involves:\nEmbedding documents and queries via a dual encoder Storing doc embeddings in a vector DB (e.g., Faiss) At query time, embedding the question and retrieving nearest neighbors Feeding results into an LLM for synthesis → how does the embedding model choice impact the quality of LLM-augmented answers?\n6. Operational Considerations # Embedding models keep improving (e.g., BEIR from 10.6 to 55.7). Choose platforms that:\nAbstract away model versioning Enable easy re-evaluation Provide upgrade paths (e.g., Vertex AI APIs) → how do we future-proof embedding systems in production?\n7. Text Embedding Lifecycle # From raw strings to embedded vectors:\nTokenization → Token IDs → Optional one-hot encoding → Dense embeddings Traditional one-hot lacks semantics, embeddings retain contextual meaning → how does token context influence the quality of embeddings?\n8. Word Embeddings: GloVe, Word2Vec, SWIVEL # Early methods:\nWord2Vec (CBOW, Skip-Gram): Context windows define meaning GloVe: Combines global + local word statistics using matrix factorization SWIVEL: Fast training, handles rare terms, parallelizable → are static embeddings enough, or do we need context-aware representations?\n9. Shallow and Deep Models # Two major paradigms:\nBoW Models (TF-IDF, LSA, LDA): Sparse, easy to compute but lack context Doc2Vec: Introduces a learned paragraph vector → how do we encode long-range relationships and context in documents?\n10. BERT and Beyond # BERT revolutionized document embedding with:\nDeep bi-directional transformers Pretraining on masked tokens Next-sentence prediction It powers models like Sentence-BERT, SimCSE, E5, and now Gemini-based embeddings. → what’s the trade-off between compute cost and performance in deep embeddings?\n11. Images and Multimodal Representations # Image embeddings from CNNs or ViTs (e.g., EfficientNet) Multimodal models (e.g., ColPali) map text + image into a shared space Enables querying images via text without OCR Closing this section: **what are the infrastructure needs to support scalable multimodal embedding workflows?\n12. Embeddings for Structured Data # Use dimensionality reduction (e.g., PCA) or learned embeddings Enable anomaly detection or classification with fewer labeled examples Especially useful when labeled data is scarce → how can we compress structured data while retaining signal?\n13. User-Item \u0026amp; Graph Embeddings # Embed users and items into the same space for recommender systems Graph embeddings (e.g., Node2Vec, DeepWalk) capture node relationships Useful for classification, clustering, and link prediction → how do we preserve both entity and relational meaning in embeddings?\n14. Dual Encoder \u0026amp; Contrastive Loss # Most embeddings today use dual encoders (e.g., query/doc or text/image towers) Trained with contrastive loss to pull positives close, push negatives away Often initialized from large foundation models (e.g., BERT, Gemini) → how do we balance generalization vs. task-specific fine-tuning?\n15. Vector vs. Keyword Search # Keyword search fails for synonyms and semantic variants Vector search embeds documents and queries, enabling “meaning-based” retrieval Similarity measured via cosine similarity, dot product, or Euclidean distance → what metric and database architecture optimize for your use case?\n16. Efficient Nearest Neighbor Techniques # Brute force is O(N)—not viable at scale LSH hashes similar vectors into the same bucket Tree-based methods (KD-tree, Ball-tree) work for low dimensions HNSW and ScaNN handle large-scale, high-dimensional spaces efficiently → how can we trade off speed vs. accuracy using ANN techniques?\n17. What Are Vector Databases? # Built specifically to index and search embeddings Combine ANN search (e.g., ScaNN, HNSW) with metadata filtering Support hybrid search (semantic + keyword) with pre- and post-filtering → how do we ensure low-latency, high-recall vector search at scale?\n18. Operational Considerations # Embeddings evolve over time—updates may be costly Combine vector + keyword search for literal queries (e.g., IDs) Choose vector DBs based on workload (e.g., AlloyDB for OLTP, BigQuery for OLAP) → how do we manage model/version drift and storage efficiency?\n19. Core Use Cases # Embeddings power:\nSearch \u0026amp; retrieval Semantic similarity \u0026amp; deduplication Recommendations Clustering \u0026amp; anomaly detection Few-shot classification Retrieval Augmented Generation (RAG) → how do embeddings improve relevance and trust in LLM outputs?\n20. Retrieval-Augmented Generation (RAG) # RAG improves factual grounding and reduces hallucinations Retrieves documents → augments prompt → generates answer Return sources for transparency and human/LLM coherence check → how do we design RAG workflows for auditability and safety?\n21. Key Takeaways # Choose models and vector stores based on data, latency, cost, and security needs Use ScaNN or HNSW for scalable ANN Use hybrid filtering to improve search accuracy RAG is critical for grounded LLMs Closing insight: Embeddings + ANN + RAG form the foundation of trustworthy, scalable, semantic applications.\n"},{"id":10,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m3/","title":"[Summary] Module 3: Concepts and Principles of ML in Healthcare","section":"C3 ML Healthcare","content":" Module 3: Concepts and Principles of ML in Healthcare # 1 Introduction to Deep Learning and Neural Networks # Q1: Why are neural networks considered a turning point in machine learning? # Neural networks mark a major departure from traditional ML models because they enable much deeper interactions between features and parameters. Unlike models like logistic regression or decision trees, neural networks—especially deep ones—organize parameters in layers, allowing complex feature transformations.\nTraditional ML: Parameters interact directly with input features. Neural networks: Parameters are arranged in layers; outputs of one layer become inputs to the next. Result: Increased expressive power and ability to model complex patterns. ➡️ To understand what truly sets deep learning apart, we need to explore the unique features of deep neural networks themselves.\nQ2: What makes deep learning different from traditional models? # Deep learning models, also called deep neural networks, often involve millions or even billions of parameters across multiple layers. This structure allows:\nHierarchical representation of data (low-level to high-level features). Repeated multiplication and addition of feature weights across layers. The final output is shaped by a sequence of transformations rather than a direct mapping. ➡️ These layered transformations contribute to the overall power of deep models—so how exactly do these layers work to increase complexity?\nQ3: How do layers in a neural network contribute to model complexity? # Each layer in a neural network captures increasingly abstract representations of the data:\nEarly layers might detect edges or basic patterns in an image. Middle layers combine these into shapes or motifs. Deeper layers can represent complex concepts like organs or specific pathologies in medical images. This layering increases the non-linearity and representational power of the model.\n➡️ To better grasp the idea behind these computational units, it helps to look at their biological inspiration.\nQ4: What are some biological inspirations behind neural networks? # The structure of neural networks was inspired by the human brain:\nEach neuron in a network is a mathematical function mimicking a brain cell. Neurons process inputs and produce outputs that can be fed into other neurons. This setup enables distributed computation, similar to how the brain processes information. ➡️ With such a powerful computational design, what makes neural networks particularly valuable in healthcare?\nQ5: Why are neural networks especially promising for healthcare applications? # Healthcare data is complex and high-dimensional—from imaging to text (EHRs), genomics, and time-series data. Deep learning shines in this space because:\nIt doesn\u0026rsquo;t need hand-engineered features. It can learn representations directly from raw data. It\u0026rsquo;s particularly powerful in image analysis, speech recognition, and clinical text processing. 2 Deep Learning and Neural Networks # Q1: What does the typical training loop of a neural network look like? # The training loop consists of iterative steps to improve model performance:\nStep 1: Pass each training sample through the model to generate predictions. Step 2: Compute a loss value to quantify prediction error. Step 3: Update model parameters using optimization to reduce the loss. This process is repeated over multiple epochs, with each full pass through the training data called an epoch. ➡️ Once training is underway, how do we ensure the model isn\u0026rsquo;t just memorizing the data?\nQ2: How do we validate whether the model is generalizing well? # Model evaluation is typically done on a validation dataset, which the model hasn\u0026rsquo;t seen during training.\nAfter each epoch (or a few), we evaluate the model’s performance on this set. This helps detect overfitting, where a model performs well on training data but poorly on unseen data. Generalization is key in healthcare to ensure predictions work on real-world, diverse patient populations. ➡️ Speaking of overfitting, one factor that contributes to this is the sheer number of parameters in neural networks.\nQ3: Why are deep learning models prone to overfitting? # Deep neural networks often contain millions of parameters, which gives them immense capacity to:\nMemorize training data rather than learning general patterns. Fit even random noise if not properly regularized. This is why data quantity and quality, as well as regularization strategies, are critical. ➡️ But what\u0026rsquo;s the actual structure of these models, and how do they transform data layer by layer?\nQ4: What happens within each layer of a neural network during computation? # Each layer performs a mathematical transformation:\nTakes input (either original features or previous layer output), Applies weighted summation and non-linear activation functions, Passes output to the next layer. This sequence of operations allows the model to build up increasingly abstract representations of the data.\n➡️ To build a solid foundation in understanding training, we need to connect this to how data and loss flow through the model.\nQ5: What is backpropagation and how does it optimize the model? # Backpropagation is a core algorithm for training neural networks:\nIt calculates how the loss changes with respect to each model parameter. These gradients are then used to update the parameters using an optimizer like Stochastic Gradient Descent (SGD). This iterative process enables the model to gradually learn better predictions. 3 Cross Entropy Loss # Q1: What is the purpose of a loss function in machine learning? # A loss function quantifies how far off a model\u0026rsquo;s predictions are from the actual labels. It\u0026rsquo;s a numerical signal used to update model parameters during training:\nLower loss = better prediction performance. The loss guides the optimization process. Without it, the model has no sense of how to improve. ➡️ For classification tasks, what specific loss function is widely used and why?\nQ2: What is cross-entropy loss and why is it used in classification? # Cross-entropy loss measures the dissimilarity between the predicted probability distribution and the true distribution (the one-hot encoded label).\nEspecially useful in multi-class classification. It penalizes wrong, confident predictions more heavily than uncertain ones. Helps push the model to make confident and correct predictions. ➡️ What does this loss look like mathematically and how is it interpreted?\nQ3: How is cross-entropy loss computed mathematically? # For a single class, the loss is defined as:\n\\[ L = -\\log(p) \\] Where p is the predicted probability for the true class. In general:\n\\[ L = -\\sum y_i \\log(p_i) \\] Where:\n( y_i ) is 1 for the correct class, 0 otherwise. ( p_i ) is the predicted probability for class ( i ). ➡️ Since the model outputs raw scores, how are these converted into probabilities?\nQ4: How does the softmax function turn logits into probabilities? # The softmax function transforms the model’s output scores (logits) into a probability distribution across classes:\n\\[ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\] Ensures the outputs are positive and sum to 1. Prepares predictions for comparison with actual labels using cross-entropy. ➡️ How does cross-entropy loss impact model training?\nQ5: How does cross-entropy guide parameter updates in training? # During backpropagation, gradients of the cross-entropy loss with respect to parameters are computed. These gradients are used to update weights so predictions align better with true labels. As training progresses, cross-entropy loss typically decreases, signaling improved classification. 4 Gradient Descent # Q1: Why is optimization necessary in training neural networks? # Optimization is the process that adjusts model parameters to minimize prediction error:\nNeural networks are trained by minimizing a loss function. The model learns by iteratively updating parameters to reduce this loss. Effective optimization is crucial for learning accurate patterns from data. ➡️ What specific algorithm is most commonly used to perform this optimization?\nQ2: What is gradient descent and how does it work? # Gradient descent is an algorithm used to minimize a function by moving in the direction of the steepest descent:\nIt calculates the gradient (slope) of the loss function with respect to each parameter. Parameters are updated by subtracting a portion (the learning rate) of the gradient. This process continues until the model reaches a local minimum. ➡️ How do we determine how big of a step to take in each update?\nQ3: What role does the learning rate play in gradient descent? # The learning rate controls how much the model updates its parameters in response to the calculated gradient:\nToo high: can overshoot and destabilize training. Too low: training can be very slow or get stuck in a poor local minimum. It\u0026rsquo;s often treated as a hyperparameter that must be tuned carefully. ➡️ Are there variations of gradient descent that address real-world training challenges?\nQ4: What are the common variants of gradient descent? # There are three main types:\nBatch Gradient Descent: Uses the entire training set for each update—slow but stable. Stochastic Gradient Descent (SGD): Updates using a single data point—faster but noisier. Mini-batch Gradient Descent: A compromise that uses a small batch of data—efficient and commonly used in practice. ➡️ Beyond variants, can the algorithm adapt during training for better performance?\nQ5: What are some adaptive optimization methods beyond basic gradient descent? # Advanced optimizers improve training by adjusting learning rates automatically:\nMomentum: Adds a fraction of the previous update to the current one to smooth progress. RMSProp: Scales updates by a moving average of recent gradients. Adam: Combines momentum and RMSProp for adaptive, robust performance—popular in practice. 9 Commonly Used and Advanced Neural Network Architectures # Q1: Why explore multiple neural network architectures in healthcare? # Different architectures are designed to solve specific problems:\nStandard models may not perform well on complex or domain-specific tasks. Specialized architectures improve performance, interpretability, and training speed. Understanding these models is key to designing solutions for clinical settings.\n➡️ What are some commonly used image-based architectures?\nQ2: What are ResNet and DenseNet, and what problems do they solve? # ResNet: Introduces residual connections to skip layers and prevent vanishing gradients. DenseNet: Connects each layer to every other layer to encourage feature reuse. Both architectures improve training of very deep networks, commonly used in radiology and pathology.\n➡️ Are there architectures tailored for medical image segmentation?\nQ3: What is U-Net and why is it useful in healthcare? # U-Net is designed for semantic segmentation, particularly in biomedical imaging:\nEncoder-decoder structure with skip connections. Captures both local and global context. Used for tasks like tumor segmentation, organ delineation, and cell counting. ➡️ Beyond visual data, how do we model structured or complex input spaces?\nQ4: What are Autoencoders and what role do they play? # Autoencoders are unsupervised neural networks that learn to reconstruct their input:\nUseful for dimensionality reduction, denoising, and anomaly detection. In healthcare: Identify rare diseases or compress high-dimensional patient data. Latent representations can be used as features in other models. ➡️ What about generating new data or simulating clinical scenarios?\nQ5: What are GANs and how are they applied in clinical ML? # Generative Adversarial Networks (GANs) consist of a generator and discriminator:\nUsed to generate synthetic but realistic data (e.g., images, waveforms). Helps with data augmentation, especially in rare disease settings. Also used in privacy-preserving machine learning and image translation (e.g., CT to MRI). "},{"id":11,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/summary_m3/","title":"[Summary] Module3: Representing Time Timing Events For Clinical Data Mining","section":"C2 Clinical Data","content":" Module3: Representing Time Timing Events For Clinical Data Mining # 1 Time, timelines, timescales and representations of time # Q: Why is it useful to place patient events on a timeline? # A: Timelines integrate diverse patient data sources, helping visualize when each event occurred, enabling analysis of sequence and duration.\nQ: What are two key reasons time matters in healthcare data? # A:\nPatient age: Impacts diagnosis, treatment decisions, metabolism, and insurance access. Event order: Helps infer causality — exposures should precede outcomes. Q: How do timescales vary in medical questions? # A: Medical events can span milliseconds (e.g., EKG signals), days (e.g., symptom onset), or decades (e.g., chronic disease progression), requiring careful scale selection.\n➡️ How do we choose relevant units of time in clinical analysis?\n2 Timescale: Choosing the relevant units of time # Q: Why is timescale selection important in healthcare data? # A: Because relevant time units in clinical contexts can range from milliseconds to decades, depending on the nature of disease, technology, and healthcare system organization.\nQ: What factors influence the appropriate timescale? # A:\nThe biological process (e.g., acute vs chronic disease) Measurement resolution of available technology Clinical workflow and timing of interventions Q: How can timescales vary in practice? # A: Heart rate variability may be tracked in milliseconds, while cancer progression may be observed over years or decades.\n➡️ What influences the choice of timescale in a clinical study?\n3 What affects the timescale # Q: What determines the appropriate timescale in clinical analysis? # A: Two main factors:\nThe research question being asked The type of data being analyzed Q: How do data types influence timescale? # A:\nLab tests may be relevant over days to weeks. Diagnoses may span days to a lifetime. Procedures could have immediate or long-term effects. Q: How do disease types affect timescale? # A:\nAcute diseases (e.g., flu) involve short timelines. Chronic diseases (e.g., diabetes) require long-term tracking. Q: Why is this interaction important? # A: It informs feature selection, granularity, and the scope of data needed for analysis.\n➡️ How is time represented in clinical datasets?\n4 Representation of time # Q: Why is time representation important in healthcare data? # A: Because clinical timelines must eventually be converted into a structured format — typically a patient-feature matrix — for analysis.\nQ: What is a patient-feature matrix? # A: A table where each row represents a patient, and each column captures a feature (e.g., diagnosis, lab result, blood pressure).\nQ: Where does time representation come into play? # A: It influences how temporal data from a patient\u0026rsquo;s timeline are encoded into the matrix — requiring different formats and strategies depending on the use case.\n➡️ How do time series differ from non-time series data in clinical contexts?\n5 Time series and non-time series data # Q: What is a time series in healthcare data? # A: A set of measurements sampled at regular intervals, usually of the same type (e.g., continuous EKG signals).\nQ: Where are time series especially common in clinical settings? # A: In intensive care units (ICUs), where patient vitals are continuously monitored via sensors.\nQ: What kind of methods are used to analyze time series? # A: Techniques from signal processing and electrical engineering.\nQ: How is non-time series data different? # A: Most clinical data are sampled irregularly, based on clinical need (e.g., labs, vitals), not clock time — requiring different representation methods.\n➡️ How is the order of events captured and why does it matter?\n6 Order of events # Q: Why is the order of clinical events important? # A: It allows researchers to reason about relationships between conditions, treatments, and outcomes (e.g., \u0026ldquo;Did condition A precede condition B?\u0026rdquo;).\nQ: What makes reasoning about event order complex? # A: When events span time intervals (e.g., chronic illnesses), we must consider overlaps and relative start/end times — not just simple time points.\nQ: What are different ways to interpret \u0026lsquo;A before B\u0026rsquo;? # A:\nA ends before B starts (no overlap) A starts before B starts (may overlap) A and B occur simultaneously (partial or full overlap) ➡️ How is time represented implicitly in healthcare data?\n7 Implicit representations of time # Q: What is an implicit representation of time in clinical data? # A: It involves ignoring exact timestamps and instead summarizing events over defined intervals (e.g., event counts in time bins).\nQ: What is binning in this context? # A: Dividing the patient timeline into intervals (bins) and counting occurrences of events within each bin. These counts become features in the analysis matrix.\nQ: What are key decisions in binning? # A:\nNumber and size of bins Interval granularity based on the clinical question’s timescale Whether to treat each bin as a separate feature or summarize them Q: What is the benefit of implicit time representation? # A: It simplifies complex event timelines into structured, analyzable data while preserving temporal context.\n➡️ What are the different ways to place data into time bins?\n8 Different ways to put data in bins # Q: What are common ways to summarize data within time bins? # A:\nCount of events Binary indicators (e.g., presence vs. absence) Aggregates like average, maximum, or most recent value Q: How do you choose the binning method? # A: It depends on the clinical item being measured and the nature of the research question.\nQ: Can you give a practical example? # A: For monitoring diabetes, the HBA1C lab test reflects average glucose levels over months. Thus, a single value may suffice instead of multiple timestamped entries.\n➡️ How do we consider the timing of exposures and outcomes in analysis?\n9 Timing of exposures and outcomes # Q: What is a cohort in clinical data analysis? # A: A group of patients meeting certain inclusion criteria, typically based on a shared exposure (e.g., condition, drug, or procedure).\nQ: What qualifies as an exposure? # A: Any condition or event that might affect the patient — such as diseases, medications, procedures, or behaviors like drinking coffee.\nQ: What qualifies as an outcome? # A: Any event that happens after the exposure and is of interest — such as complications, recovery, cost, or survival time.\nQ: Why is timing crucial in analyzing exposures and outcomes? # A: To establish a meaningful temporal relationship and investigate associations (or causality), it’s essential to ensure exposures precede outcomes and to define observation windows accurately.\n➡️ Why are clinical processes considered non-stationary over time?\n10 Clinical processes are non-stationary # Q: What does it mean for clinical data to be non-stationary? # A: It means that the distributions of data — and the associations within them — change over time due to evolving clinical practices, medications, coding systems, and care standards.\nQ: What is a stationary vs. non-stationary process? # A:\nStationary: Data distributions remain consistent over time. Non-stationary: Data distributions (and thus patterns and associations) change over time. Q: Why does non-stationarity matter in clinical data mining? # A: Because models trained on past data may become invalid as treatments, diagnostics, and data collection methods evolve. Analysts must carefully consider data timeframes and changes in system behavior.\n"},{"id":12,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/summary_m4/","title":"[Summary] Module4 : Creating Analysis Ready Dataset from Patient Timelines","section":"C2 Clinical Data","content":" Module4 : Creating Analysis Ready Dataset from Patient Timelines # 1 Turning clinical data into something you can analyze # Q: What is the main objective of this module? # A: To explain how to convert raw clinical data into a patient-feature matrix suitable for analysis and answering research questions.\nQ: What is a patient-feature matrix? # A: A structured table where each row represents a patient and each column represents a clinical feature or measurement.\nQ: What topics will this module cover? # A:\nChoosing and extracting features Managing too many or missing features Understanding how data transformations affect analysis outcomes ➡️ How do we define the unit of analysis in clinical datasets?\n2 Defining the unit of analysis # Q: What is the typical unit of analysis in clinical data studies? # A: The patient — with each row in the data table representing one patient and each column a feature about them.\nQ: Are there other valid units of analysis? # A: Yes. Depending on the question, units can be drug-disease pairs, visits, procedures, or other clinical events.\nQ: Can you give an example of an alternative unit? # A: For studying off-label drug use, the unit might be a drug-disease pair, with features describing co-mentions, sequencing, and usage frequency.\n➡️ How do we use features and track their presence in patient data?\n3 Using features and the presence of features # Q: Should we use all possible features extracted from clinical data? # A: Generally yes — start with a full feature set, unless constrained by computational limits or privacy considerations.\nQ: What are some reasons to remove features? # A:\nResource limitations Low predictive value Sensitivity concerns (e.g., HIV status) Q: How can presence or absence of data itself become a feature? # A: Metadata such as the act of ordering a test (regardless of result) can indicate clinical concern and may be predictive on its own.\nQ: What role does metadata play in feature design? # A: It can be highly informative, capturing clinical behavior that indirectly reflects patient status.\n➡️ How do we create features from structured clinical sources?\n4 How to create features from structured sources # Q: What are structured vs. unstructured healthcare data? # A:\nStructured: Organized in tables with rows/columns (e.g., lab results, diagnosis codes) Unstructured: Free text, images, or signals Q: Why are structured sources important? # A: They are the most readily available and easiest to transform into features for analysis.\nQ: What are the main steps in using structured data? # A:\nAccessing the database Querying with SQL Joining tables using patient IDs Standardizing and reshaping features Handling missing or excessive features Optionally constructing new ones ➡️ Why is standardizing features important and how is it done?\n5 Standardizing features # Q: What does it mean to standardize features? # A: Transforming feature values into a common numerical scale, often called normalization.\nQ: Why is standardization important? # A: It prevents features with large ranges from dominating distance-based or scale-sensitive algorithms in downstream analysis.\nQ: What are common standardization techniques? # A:\nMin-max scaling: Rescales values to a range (e.g., 0 to 1) Z-score normalization: Transforms values to have mean 0 and standard deviation 1 ➡️ How do we deal with having too many features?\n6 Dealing with too many features # Q: Why might you avoid using all available features in clinical data? # A: While comprehensive data is ideal, practical concerns can necessitate reducing features.\nQ: What are reasons to reduce the number of features? # A:\nIrrelevance: Some features offer no useful signal (e.g., record access frequency) Missingness: Some are missing in most patients Sparsity: Too many features missing in a given patient Redundancy: Highly correlated features interfere with some analyses Computational cost: More features = slower analysis Privacy: More features increase re-identification risk Q: How can this affect patient privacy? # A: A rich set of features can unintentionally reveal a patient’s identity.\n➡️ Where do missing values in clinical data originate?\n7 The origins of missing values # Q: Why do missing values occur in clinical datasets? # A: Missing values often result from converting complex patient timelines into a simplified matrix format where not all features are applicable or expected.\nQ: How is missingness different in routine care vs. prospective studies? # A:\nIn prospective studies, missing data usually implies an error or oversight. In routine care, absence might mean the data was never meant to be recorded. Q: What are three possible reasons a value is missing? # A:\nIt should have existed, but wasn’t recorded. It\u0026rsquo;s absent due to matrix transformation — not clinically expected. Its absence has meaning — like no diagnosis implies a negative finding. ➡️ How do we handle missing values in practice?\n8 Dealing with missing values # Q: What is imputation in clinical data analysis? # A: Imputation is a technique used to fill in missing data using predictions based on other available information.\nQ: What is column-mean imputation? # A: A simple method where missing values are replaced with the average of non-missing values in the same column.\nQ: Why might column-mean imputation be unsuitable in medicine? # A: Because one patient\u0026rsquo;s lab value isn\u0026rsquo;t necessarily informative for another — clinical data often lack this kind of inter-patient correlation.\nQ: What is a better alternative in clinical contexts? # A: Use within-patient imputation: infer a missing value from other known values of that same patient using correlated features.\n➡️ What are the recommended practices for handling missing values?\n9 Summary recommendations for missing values # Q: Should you always impute missing values in clinical datasets? # A: Not necessarily — it depends on the extent and distribution of missingness. Expert guidance is recommended.\nQ: When is imputation appropriate? # A: If most values are present and only a few are missing, imputation is usually a good choice.\nQ: When should you drop a variable? # A: If most patients are missing that variable, it\u0026rsquo;s better to exclude it rather than impute unreliable values.\nQ: What about cases in between? # A: There is no universal rule. Some propose using indicator variables to flag imputed values, but this is debated.\n➡️ How are new features constructed from existing data?\n10 Constructing new features # Q: What is feature engineering in clinical data mining? # A: The process of creating new features from existing ones, often by transformation or combination (e.g., computing BMI from height and weight).\nQ: Why is feature engineering valuable? # A: Well-designed features can significantly improve the performance of models — even more than using advanced algorithms with raw data.\nQ: What are some simple examples of constructed features? # A:\nBinary indicators (e.g., converting a count to 1 if \u0026gt; 0) Aggregations or ratios Clinical scores derived from combinations of measurements ➡️ What are some practical examples of feature engineering in healthcare?\n11 Examples of engineered features # Q: What are clinical scoring systems in feature engineering? # A: Formulas that combine multiple clinical values to estimate disease severity or health status — commonly used in risk adjustment.\nQ: What is an example of a simple scoring system? # A: Body Mass Index (BMI) — calculated from height and weight to assess obesity.\nQ: What are examples of comorbidity scoring systems? # A:\nCharlson Comorbidity Index Elixhauser Comorbidity Index Q: Can non-clinical features also be engineered? # A: Yes — proxy features like zip code (socioeconomic status) and record frequency (healthcare utilization) can be derived.\n➡️ When should you consider engineering new features?\n12 When to consider engineered features # Q: When should you engineer new features in clinical datasets? # A: When important concepts are not directly captured in the raw data — consider proxies or derived variables to fill the gap.\nQ: What strategies can guide feature creation? # A:\nUse clinical intuition Include counts, changes over time, or ratios Repurpose validated scoring systems Q: What trade-offs should be considered? # A: Balance the benefit of a new feature against the effort required to create and validate it.\nQ: Can models ever learn features automatically? # A: Yes — deep learning methods can learn features directly from raw data, reducing the need for manual feature engineering.\n➡️ What are the main takeaways about creating analysis-ready datasets?\n13 Main points about creating analysis ready datasets # Q: What is an analysis-ready dataset in clinical research? # A: A clean, structured patient-feature matrix derived from raw clinical data and suitable for analysis.\nQ: What tools are commonly used to construct it? # A: Standard programming tools and database queries (e.g., S### QL + Python).\nQ: How can the number of features be reduced? # A:\nDomain knowledge to combine or drop variables Mathematical methods like Principal Component Analysis (PCA) Q: How are missing values handled? # A: Either removed or imputed, using varying levels of complexity depending on the case.\nQ: What boosts success in dataset creation? # A: Learning the clinical context of the question — deeper medical understanding improves data transformations and feature design.\n➡️ What are structured knowledge graphs, and how do they relate to datasets?\n14 Structured knowledge graphs # Q: What are the two key topics in this part of the module? # A:\nConstructing a patient feature matrix Using curated biomedical knowledge (via knowledge graphs) Q: What is a knowledge graph in healthcare? # A: A structured digital representation of biomedical entities (e.g., diseases, drugs, lab tests) and their relationships — also known as an ontology.\nQ: Why are knowledge graphs useful? # A: They encode expert knowledge in a machine-readable way, enabling smarter feature construction, search, and data linkage.\nQ: What are examples of implicit prior knowledge use? # A: Using test order counts related to glucose as a proxy for diabetes — derived from domain knowledge.\n➡️ What exactly is contained in a biomedical knowledge graph?\n15 So what exactly is in a knowledge graph # Q: What are the core components of a biomedical knowledge graph? # A:\nEntities: e.g., symptoms, diseases, drugs, body parts Synonyms: mappings of equivalent terms (e.g., \u0026ldquo;heart attack\u0026rdquo; = \u0026ldquo;acute myocardial infarction\u0026rdquo;) Relationships: logical links between entities (e.g., \u0026ldquo;is a kind of\u0026rdquo;) Q: What is the most important type of relationship in medical knowledge graphs? # A: The \u0026ldquo;is a kind of\u0026rdquo; relationship — it defines hierarchies (e.g., Lipitor is a kind of lipid-lowering drug).\nQ: What benefit does this hierarchical structure provide? # A: Entities inherit properties from broader categories, enabling powerful reasoning and search (e.g., querying all \u0026ldquo;lipid-lowering drugs\u0026rdquo;).\n➡️ What are the most important knowledge graphs in biomedicine?\n16 What are important knowledge graphs # Q: Where can you find biomedical knowledge graphs? # A: One central repository is BioPortal from the National Center for Biomedical Ontology at Stanford.\nQ: What are some widely used biomedical ontologies? # A:\nICD (International Classification of Diseases): Maintained by WHO; used globally for diagnosis coding (ICD-9, ICD-10). CPT (Current Procedural Terminology): Created by the AMA to categorize procedures and services, mainly for billing. Q: Why are ICD-9 and ICD-10 both relevant today? # A: Many clinical datasets still use ICD-9 codes even though ICD-10 is current, especially in older patient records.\n➡️ How do we choose which knowledge graph to use in practice?\n17 How to choose which knowledge graph to use # Q: What should you consider when selecting a knowledge graph? # A:\nEntity types and how they are classified Relationship meanings between entities Terminology: presence of synonyms and spelling variants Interoperability: how well the graph maps to other knowledge graphs Q: What practical method can help assess utility? # A: Count how many terms from the knowledge graph appear in EMR text or data — this indicates how relevant and compatible it is with your dataset.\n"},{"id":13,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/","title":"C4 AI Evaluations","section":"AI in Healthcare","content":" 📘 Course 4: Evaluations of AI Applications in Healthcare # [ToC] Course 4 🤖 Module 1: AI in Healthcare # 1. What’s the Problem?\nUnderstanding the growing role of AI in healthcare and what meaningful problems it can solve.\n2. Why Does It Matter?\nAI is being rapidly adopted, yet many applications focus on accuracy without considering clinical utility or impact.\n3. What’s the Core Idea?\nAI spans biomedical research, translational science, and medical practice. It excels at data synthesis and task automation, but needs alignment with care goals.\n4. How Does It Work?\nAI systems analyze large-scale structured and unstructured data to assist in diagnostics, treatment recommendations, and patient monitoring. Evaluating AI must go beyond accuracy to include actionability.\n5. What’s Next?\nLearn how to evaluate AI solutions based on clinical utility, outcome-action pairing, and feasibility of implementation.\n🧪 Module 2: Evaluations of AI in Healthcare # 1. What’s the Problem?\nMost AI models are evaluated only by accuracy, without considering whether they actually improve clinical care.\n2. Why Does It Matter?\nBecause real-world deployment requires understanding how an AI model leads to meaningful actions and patient benefit.\n3. What’s the Core Idea?\nUse the Outcome-Action Pairing (OAP) framework: pair predictions with feasible actions that impact care. Evaluate utility, feasibility, and clinical impact.\n4. How Does It Work?\nStart with the clinical problem, define output and required action, assess lead time, type of action (medical, operational), and stakeholder involvement.\n5. What’s Next?\nExamine the four phases of deploying AI in clinical environments, from design to monitoring.\n🚀 Module 3: AI Deployment # 1. What’s the Problem?\nEven well-performing AI models often fail to reach or impact clinical care due to poor integration and lack of support.\n2. Why Does It Matter?\nHealthcare settings require safety, validation, stakeholder buy-in, and long-term monitoring for AI success.\n3. What’s the Core Idea?\nDeployment includes four stages: design and development, evaluation and validation, diffusion and scaling, and continuous monitoring.\n4. How Does It Work?\nEvaluate utility and economic value, run \u0026lsquo;silent mode\u0026rsquo; trials, ensure human-machine interaction, and plan for infrastructure and updates.\n5. What’s Next?\nUnderstand how fairness, transparency, and bias affect AI performance across diverse populations.\n⚖️ Module 4: Downstream Evaluations: Bias and Fairness # 1. What’s the Problem?\nAI models may perpetuate bias or perform poorly on underrepresented populations.\n2. Why Does It Matter?\nBias in AI can lead to inequities in care and further marginalize already vulnerable groups.\n3. What’s the Core Idea?\nBias can occur at any stage—from data collection to deployment. Fairness requires proactive evaluation using tools like MINIMAR.\n4. How Does It Work?\nIdentify and mitigate types of bias (e.g., representation, measurement, aggregation). Use fairness definitions like anti-classification and calibration.\n5. What’s Next?\nExplore how AI regulation addresses these concerns through risk frameworks and transparency standards.\n📜 Module 5: Regulatory Environment for AI in Healthcare # 1. What’s the Problem?\nAI products often lack regulatory approval due to unclear pathways and concerns over safety and accountability.\n2. Why Does It Matter?\nRegulation ensures AI tools are safe, effective, and beneficial in real-world healthcare settings.\n3. What’s the Core Idea?\nThe FDA and IMDRF provide frameworks based on risk classification and clinical evaluation (valid association, analytical \u0026amp; clinical validation).\n4. How Does It Work?\nFollow the SaMD lifecycle: define intended use, evaluate risk, seek approval via 510(k), De Novo, or PMA, and ensure continuous monitoring.\n5. What’s Next?\nApply ethical practices in problem formulation, data choice, stakeholder transparency, and conflict of interest management.\n🧭 Module 6: Best Ethical Practices in AI for Healthcare # 1. What’s the Problem?\nConflicts of interest and unclear problem framing can undermine trust and effectiveness in AI systems.\n2. Why Does It Matter?\nEthical lapses can lead to harm, inequity, or misuse of AI tools in sensitive healthcare decisions.\n3. What’s the Core Idea?\nEthical AI requires clear goals, clinician input, transparent data use, and conflict-of-interest management.\n4. How Does It Work?\nAsk ethical questions early, assess biases in data and design, disclose secondary interests, and implement oversight.\n5. What’s Next?\nAdopt frameworks like MINIMAR, audit for fairness, and ensure that systems are explainable, justifiable, and trustworthy.\n"},{"id":14,"href":"/ai-workflows/genai-systems/5-day-genai-google/day3_generative_agents/","title":"Day 3 – Generative Agents","section":"5-Day GenAI with Google","content":" Day 3 – Generative Agents # 1. What Are Generative Agents? # We start with the definition of agents—AI systems designed to achieve goals by perceiving their environment and taking actions using tools. Unlike static LLMs, generative agents combine models, tools, and orchestration to interact with the world dynamically.\n→ what components make these agents truly autonomous and intelligent?\n2. Agent Architecture Breakdown # An agent’s architecture includes:\nA language model for decision-making. Tools to interface with the outside world (APIs, functions, data). An orchestration layer to manage memory, state, and reasoning techniques like CoT, ReAct, and ToT. → how do we structure agents to be effective in real-world applications?\n3. From MLOps to AgentOps # AgentOps is introduced as a specialized branch of GenAIOps focused on the deployment and reliability of agents. It inherits from DevOps, MLOps, and FMOps, and introduces concepts like prompt orchestration, memory handling, and task decomposition.\n→ how do organizations build scalable, production-grade agent systems?\n4. The Role of Observability and Metrics # Agents must be measured at every level—goal success rates, user interactions, latency, errors, and human feedback. These form the KPIs for agents and inform ongoing improvements.\n→ how do we move beyond proof-of-concept to reliable agent deployment?\n5. Evaluating Agents Effectively # Agent evaluation involves more than just checking output correctness. It requires tracing decision-making, assessing reasoning, evaluating intermediate steps, and gathering structured human feedback.\n→ how do we evaluate agents on logic, tool use, and usefulness holistically?\n6. Instrumentation and Traceability # Traces provide fine-grained visibility into what the agent did and why. This supports debugging and performance tuning, enabling trust and iterative refinement.\n→ what observability tools are best for multi-step agent workflows?\n7. Assessing Core Capabilities # Before deployment, it\u0026rsquo;s vital to evaluate agent capabilities like tool use and planning. Benchmarks such as BFCL and PlanBench test these abilities, but should be supplemented with task-specific tests that reflect real use cases.\n→ what public benchmarks best reflect your agent\u0026rsquo;s core capabilities?\n8. Trajectory Evaluation # Agents often follow multi-step trajectories. Evaluation should compare expected vs. actual tool use paths using metrics like exact match, in-order, any-order, precision, recall, and single-tool usage.\n→ is the agent taking optimal steps—or just getting lucky in the final answer?\n9. Evaluating Final Responses # The agent\u0026rsquo;s final output must be evaluated for correctness, relevance, and tone. LLM-based autoraters are useful, but need precisely defined criteria. Human evaluators still offer the gold standard for nuanced feedback.\n→ can automated evaluation alone guarantee real-world readiness?\n10. Human-in-the-Loop (HITL) # Subjectivity, nuance, and real-world implications often require human review. Direct scoring, comparative evaluations, and user studies are powerful tools to validate and calibrate automated metrics.\n→ when should humans intervene in the agent evaluation loop?\n11. Challenges and Future Directions # Agent evaluation is still maturing. Current challenges include limited evaluation datasets, gaps in process reasoning metrics, difficulty with multimodal outputs, and handling dynamic environments.\nThe forward-looking insight: agent evaluation is shifting toward process-based, explainable, and real-world-grounded methods.\n12. From Single to Multi-Agent Evaluation # Multi-agent systems are the next evolution in generative AI—multiple specialized agents collaborate like a team. Evaluation must now address not just individual outputs, but also cooperation, delegation, and plan adherence.\n→ how do we measure coordination, not just correctness?\n13. Architecture of Multi-Agent Systems # Agents are modular and play distinct roles—planner, retriever, executor, evaluator. Communication, routing, tool integration, memory, and feedback loops form the backbone. These components support dynamic and resilient reasoning.\n→ what enables agents to act as a system, not just individuals?\n14. Multi-Agent Design Patterns # Patterns like sequential, hierarchical, collaborative, and competitive enable scalable, adaptive, and parallel agent behavior. These patterns reduce bottlenecks and improve automation for complex workflows.\n→ which pattern suits your domain—assembly line, team, tournament, or council?\n15. Evaluation at Scale # Evaluating multi-agent systems includes trajectory traceability, agent coordination, agent-tool selection, and system-wide goal success. Instrumenting each step and agent ensures deeper insights.\nThe closing reflection: multi-agent systems multiply both the potential and complexity of generative agents—evaluation must evolve accordingly.\n16. From RAG to Agentic RAG # Traditional RAG pipelines retrieve static chunks of knowledge for LLMs. Agentic RAG innovates by embedding retrieval agents that:\nExpand queries contextually Plan multi-step retrieval Choose data sources adaptively Validate results via evaluator agents → how can agents actively reason during retrieval to boost response quality?\n17. Engineering Better RAG # To improve any RAG implementation:\nParse and chunk documents semantically Enrich chunks with metadata Tune embeddings or adapt search space Use fast vector search + rankers Implement grounding checks → is your RAG problem about generation—or poor search to begin with?\n18. The Rise of Enterprise Agents # 2025 marks the rise of two agent types:\nAssistants: Interactive, task-oriented agents like schedulers or sales aides. Automators: Background agents that observe events and autonomously act. → how do organizations orchestrate fleets of agents across roles and workflows?\n19. Agentspace and Agent Management # Google Agentspace provides enterprise-grade infrastructure for creating, deploying, and managing secure, multimodal AI agents. With features like:\nRBAC, SSO, and data governance Blended RAG and semantic search Scalable agent orchestration and monitoring → what’s needed to manage AI agents as a virtual team at scale?\n20. NotebookLM Enterprise # NotebookLM allows users to upload documents, ask questions, and synthesize insights. Enterprise features include:\nAudio summaries via TTS Semantic linking across documents Role-based access and policy integration Final insight: intelligent notebooks + agents will redefine enterprise knowledge discovery and interaction.\n21. Contract-Adhering Agents # Prototypical agent interfaces are too vague for real-world, high-stakes environments. Introducing contractor agents enables:\nClear outcome definitions Negotiation and refinement Self-validation of deliverables Structured decomposition into subcontracts → how can formalized contracts make agents production-ready and trustworthy?\n22. Contract Lifecycle and Execution # Contractor agents follow a lifecycle: define → negotiate → execute → validate. Execution may involve multiple LLM-generated solutions and iterative self-correction until the contract is fulfilled, optimizing for quality over latency.\n→ what runtime capabilities are needed for contract-based agents?\n23. Co-Scientist: A Real-World Case Study # Google’s AI co-scientist system uses multi-agent collaboration to accelerate hypothesis generation and validation in scientific research. Roles include:\nData processors Hypothesis generators Validators Cross-team communicators Final reflection: multi-agent systems, when built as collaborative contractors, can extend the scientific method itself.\n24. Specialized Agents in the Car # The automotive domain is a natural fit for multi-agent AI. Here are key agent roles:\nNavigation Agent: Plans routes, ranks POIs, and handles traffic awareness Media Agent: Plays contextually relevant music or podcasts Messaging Agent: Drafts, edits, and sends messages hands-free Car Manual Agent: Uses RAG to answer questions about car features General Knowledge Agent: Answers follow-up queries to enhance user experience → how do you design agent roles that align with contextual user needs?\n25. Hierarchical and Diamond Patterns # Hierarchical: A central Orchestrator routes user input to the right agent Diamond: Adds a Rephraser agent for tone/style before speaking responses aloud → when does orchestration alone fall short—requiring tone-sensitive agents?\n26. Peer-to-Peer and Collaborative Patterns # Peer-to-Peer: Agents hand off queries among themselves for better routing resilience Collaborative: Multiple agents contribute partial answers; a Mixer Agent synthesizes the final response → can agents collaborate without central control to produce superior outputs?\n27. Response Mixer and Safety-Critical Use # The Response Mixer evaluates and combines outputs from several agents (e.g., knowledge + tips + manual) to form a cohesive answer, especially for safety-critical queries like aquaplaning.\n→ how do we ensure safety-critical information is prioritized in generative settings?\n28. Adaptive Loop Pattern # Agents refine queries iteratively to meet vague or underspecified user needs—e.g., finding a vegan Italian restaurant with fallback strategies.\nClosing insight: multi-agent architectures thrive where adaptability, refinement, and specialization are essential.\n29. Real-Time Performance and Resilience # Multi-agent systems in cars prioritize on-device responsiveness for safety (e.g., climate control), while using cloud-based agents for tasks like dining suggestions. This hybrid model balances latency, capability, and robustness.\n→ how do agents coordinate local vs. remote processing for safety and personalization?\n30. Vertex AI Agent Builder # Google’s Agent Builder platform integrates secure cloud services, open-source libraries, evals, and managed runtimes for enterprise-grade agent development. Features include:\nRetrieval via Vertex AI Search or RAG Engine Secure APIs via Apigee Gemini and Model Garden access Evaluation pipelines via Vertex AI Eval Service → what developer tools are needed to build, scale, and evaluate enterprise-ready agents?\n31. Key Developer Principles # AgentOps matters: memory, tools, trace, orchestration Automate evals, but combine with HITL Design multi-agent architectures for complexity and scale Improve search before Agentic RAG Use agent/tool registries to reduce chaos Prioritize security, flexibility, and developer cycles 32. Future Directions # Research will focus on:\nProcess-based and AI-assisted evaluation Agent collaboration and communication protocols Memory, adaptivity, explainability, and contracting models Final insight: the future is agentic—developers must blend engineering, ops, UX, and domain logic to build next-gen intelligent systems.\n"},{"id":15,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m4/","title":"[Summary] Module 4: Evaluation and Metrics for ML in Healthcare","section":"C3 ML Healthcare","content":" Module 4: Evaluation and Metrics for ML in Healthcare # 1 Introduction to Model Performance Evaluation # Q1: Why is model evaluation critical in healthcare machine learning? # In healthcare, decisions informed by ML models can have life-altering consequences:\nIt’s not enough for a model to perform well on training data. We need to ensure that the model performs well on unseen patients and real-world conditions. Rigorous evaluation is essential to trust and validate clinical usefulness. ➡️ What does it mean for a model to generalize?\nQ2: What is generalization and how is it assessed? # Generalization refers to the model\u0026rsquo;s ability to perform well on new, unseen data:\nIndicates how well the model has learned the underlying patterns. We measure this using validation and test sets. High training accuracy but poor test accuracy implies overfitting. ➡️ What techniques help ensure fair and reliable evaluation?\nQ3: What is data splitting and why is it important? # Common splits:\nTraining set: For model learning. Validation set: For tuning hyperparameters and model selection. Test set: For final performance estimation. These splits help avoid data leakage and optimism bias in evaluation.\n➡️ Are there methods more robust than simple train/test splits?\nQ4: What is cross-validation and when is it useful? # Cross-validation is a strategy to make evaluation more robust:\nData is divided into k folds. Model trains on k-1 folds and validates on the remaining one. Repeated multiple times to average out variability. It’s particularly useful in small datasets, common in healthcare studies.\n2 Overfitting and Underfitting # Q1: What are overfitting and underfitting in machine learning? # These are two common problems that reduce model effectiveness:\nOverfitting: The model learns noise and specifics of the training set, performing poorly on new data. Underfitting: The model fails to capture underlying trends, resulting in poor performance on both training and test sets. ➡️ What are the visual signs of overfitting and underfitting during training?\nQ2: How can we detect overfitting and underfitting? # By plotting training and validation accuracy/loss:\nOverfitting: Training accuracy increases while validation accuracy drops or plateaus. Underfitting: Both training and validation accuracies remain low. Regular monitoring helps identify these trends early. ➡️ What causes models to overfit?\nQ3: What factors contribute to overfitting in ML models? # High model complexity (deep networks, too many parameters). Small training dataset or lack of representative diversity. Too many epochs without early stopping. Overfitting is especially risky in healthcare due to variability in real-world patient populations.\n➡️ Conversely, what might cause underfitting?\nQ4: Why might a model underfit the data? # The model is too simple (e.g., linear models for nonlinear problems). Insufficient training time or suboptimal hyperparameters. Poor feature engineering or missing important data signals. Underfitting leads to missed patterns—dangerous in diagnostic or predictive tools.\n3 Strategies to Address Overfitting, Underfitting and Introduction to Regularization # Q1: How can we address overfitting in ML models? # Strategies include:\nReducing model complexity: Use fewer layers or parameters. Early stopping: Halt training when validation performance stops improving. Data augmentation: Increase data diversity artificially (especially for images). Regularization: Penalize model complexity. ➡️ What types of regularization techniques are commonly used?\nQ2: What is L1 and L2 regularization? # These techniques add penalty terms to the loss function:\nL1 (Lasso): Adds absolute value of weights → encourages sparsity (some weights become zero). L2 (Ridge): Adds square of weights → discourages large weights. They help control overfitting by shrinking parameter magnitudes.\n➡️ Besides weight penalties, are there other techniques to improve generalization?\nQ3: What is dropout and how does it help prevent overfitting? # Dropout randomly disables neurons during training:\nPrevents co-adaptation of features. Encourages the network to learn redundant, distributed representations. Typically used in deep networks. ➡️ Can underfitting also be addressed through specific strategies?\nQ4: How can we fix underfitting in a model? # Underfitting can be resolved by:\nIncreasing model capacity (deeper or more complex models). Training longer or using better optimization. Improving data quality or features. Adjusting learning rate and other hyperparameters. 4 Statistical Approaches to Model Evaluation # Q1: Why are statistical methods important in evaluating ML models? # Statistical tools help us quantify confidence and variability in model performance:\nAvoid over-interpreting single-point metrics. Make decisions grounded in significance and uncertainty. Essential when models may be deployed in clinical practice. ➡️ What’s a simple way to estimate uncertainty in metrics?\nQ2: What is bootstrapping and how is it used in model evaluation? # Bootstrapping is a resampling technique:\nRepeatedly sample with replacement from the test set. Evaluate the model on each sample to get a distribution of performance metrics. Helps compute confidence intervals for metrics like accuracy or AUC. ➡️ Are there other methods for statistical comparison of models?\nQ3: How do permutation tests assess model significance? # Permutation testing involves:\nRandomly shuffling labels on test data. Evaluating model performance on this randomized data. Repeating multiple times to build a null distribution. If the true model significantly outperforms the null, it\u0026rsquo;s statistically meaningful.\n➡️ How do we assess reliability when comparing two models?\nQ4: What is the paired t-test and when should it be used? # Used to compare two models’ predictions on the same test samples:\nMeasures whether the difference in performance is statistically significant. Assumes approximately normal distribution of differences. Can be useful in evaluating if model A is better than model B.\n5 Receiver Operator and Precision Recall Curves as Evaluation Metrics # Q1: Why do we need different evaluation metrics beyond accuracy? # Accuracy alone can be misleading, especially in imbalanced datasets:\nIn healthcare, positive cases (e.g., disease presence) may be rare. A model that predicts only the majority class can appear highly accurate. Metrics like precision, recall, and AUC provide better insight into real performance. ➡️ What is the ROC curve and how is it interpreted?\nQ2: What is the ROC curve and AUC? # ROC (Receiver Operating Characteristic) curve plots True Positive Rate (TPR) vs. False Positive Rate (FPR). AUC (Area Under the Curve) summarizes this curve into a single number (0.5 = random, 1.0 = perfect). AUC reflects the model’s ranking ability—how well it separates classes. ➡️ Are ROC curves always the best choice?\nQ3: When should we prefer Precision-Recall (PR) curves over ROC? # PR curves focus on positive class performance, plotting Precision vs. Recall. More informative than ROC when dealing with imbalanced data. Helpful for screening tools or rare disease prediction. ➡️ How are these metrics computed and interpreted?\nQ4: What are precision, recall, and F1 score? # Precision: TP / (TP + FP) → Of predicted positives, how many were correct? Recall: TP / (TP + FN) → Of actual positives, how many did we find? F1 Score: Harmonic mean of precision and recall. These metrics give a fuller picture of model trade-offs, especially in clinical use cases.\n"},{"id":16,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c5_capstone/","title":"C5 Capstone Projects","section":"AI in Healthcare","content":" 📘 Course 5: Capston Projects \u0026ndash; COVID-19 AI # 📷 Project 1: CXR-Based COVID-19 Detector # Phase 1: Data Collection # Objective: Build a deep learning model to predict COVID-19 status using chest x-ray (CXR) images. Input: 3000x3000 px uncompressed DICOM images from 30,000 exams (10% COVID-positive). Concern: Class imbalance (90:10) and high-resolution image processing needs. Phase 2: Model Training (Part 1) # Used ResNet-50 on resized 224x224 images. Data split randomly (not by patient). Augmentation: 50% zoom-in on random region. Issue: Training loss did not improve → possible underfitting or flawed preprocessing. Phase 3: Model Training (Part 2) # Improvements made: Patient-level data split to prevent leakage. Image size increased to 512x512 px; model adjusted accordingly. Simplified augmentation: horizontal flip + light zoom. COVID-positive oversampling added. New issue: Overfitting (training loss much lower than validation loss). Metric discrepancy: High accuracy but relatively low AUROC on validation set. Phase 4: Model Evaluation # Applied dropout (p=0.5) and random rotation augmentation. Two early-stopped models: Model A: Best validation AUROC. Model B: Best validation loss. Deployment consideration: Choose model based on worklist prioritization use case. 📈 Project 2: EHR-Based Intubation Predictor # Phase 1: Data Collection # Objective: Predict likelihood of intubation from electronic health records (EHR). Input: COVID dataset with 3,000 EHRs (300 positive) + additional 40,000-exam COVID-like dataset. Issue: Misassumption—only 3,000 usable EHRs in COVID dataset. Challenge: Sparse features, strange lab value distributions (e.g., D-DIMER), many missing values. Phase 2: Model Training (Part 1) # Attempted logistic regression but faced data issues (sparsity, outliers, NaNs). Required strategies to deal with missingness and outliers before modeling. Phase 3: Model Training (Part 2) # New strategy: Train on 40,000 “COVID-like” exams; test on COVID dataset (3,000 exams). Split: 70% train, 30% validation (COVID-like dataset). 10-fold cross-validation used for hyperparameter tuning. Models trained: Logistic regression + Random Forests. Phase 4: Model Evaluation # Performance improved using COVID-like training data. Now selecting operating threshold using precision-recall curve. Deployment consideration: Choose threshold optimized for triage decision-making. 🔗 Cross-Project Learnings # Both projects improved significantly from Phase 2 to 4 through better data practices: Patient-level splits Cross-validation Oversampling Threshold tuning Key divergence: Project 1 is image-based, focuses on COVID diagnosis. Project 2 is EHR-based, focuses on intervention prediction (intubation). Both must align their evaluation strategy with real-world clinical use (triage vs. prioritization). "},{"id":17,"href":"/ai-workflows/genai-systems/5-day-genai-google/day4_domainspecific_llms/","title":"Day 4 – Domain-Specific LLMs","section":"5-Day GenAI with Google","content":" Day 4 – Domain-Specific LLMs # 1. The Rise of Specialized LLMs # We start with the evolution of LLMs from general-purpose to domain-specific tools. This shift was driven by challenges in fields like cybersecurity and medicine, where technical language and sensitive use cases demand more than general knowledge.\n→ why do general-purpose LLMs struggle in specialized domains?\n2. The Challenges in Cybersecurity # Cybersecurity experts face three main issues: rapidly evolving threats, repetitive manual work (toil), and a shortage of skilled talent. These bottlenecks make it hard to keep up with modern security needs.\n→ how can AI reduce toil, bridge talent gaps, and counter fast-evolving threats?\n3. The Role of GenAI in Security # GenAI can assist various security personas—from analysts to CISOs—by translating queries, reverse-engineering code, planning remediation, and summarizing threats. This enables both automation and augmentation of expertise.\n→ what kind of architecture supports this AI augmentation effectively?\n4. Multi-layered System Design # SecLM combines tools (top layer), a reasoning API (middle layer), and secure data sources (bottom layer). This allows for contextual responses using live data and tailored planning.\n→ how do we ensure accuracy and freshness without retraining LLMs constantly?\n5. Domain-Specific Model Training # SecLM trains on open-source and licensed security content, fine-tuned for tasks like alert summarization and command analysis. It uses parameter-efficient tuning and RAG for freshness.\n→ how does the system generalize to unseen tasks or environments?\n6. Flexible Planning and Execution # SecLM decomposes broad questions (e.g., about an APT group) into steps like retrieving intel, translating to SIEM queries, and synthesizing responses—showcasing multi-agent, tool-augmented reasoning.\n→ how does this compare with general-purpose LLMs?\n7. Performance and Evaluation # Through expert evaluations and automated metrics, SecLM outperforms general models on security-specific tasks, demonstrating the need for full-stack, domain-focused platforms.\n→ can this platform be generalized to other domains like health tech?\n8. Medical Q\u0026amp;A as a Grand Challenge # Medical question-answering (QA) demands deep reasoning, evolving knowledge, and accurate synthesis. LLMs like Med-PaLM show potential by answering USMLE-style questions and sourcing info from varied medical content.\n→ how can we ensure these answers are trustworthy and context-aware?\n9. Opportunities for GenAI in Medicine # Use cases range from contextual Q\u0026amp;A on patient history to triaging clinician messages and real-time patient-clinician dialogue support. GenAI systems can enhance decision-making, patient engagement, and clinician efficiency.\n→ what safeguards ensure these capabilities are safe, equitable, and accurate?\n10. Human-Centric and Conversational AI # Med-PaLM aims to support flexible interaction—combining structured clinical expertise with empathetic, human-centric dialogue. It was built to scale reasoning and bring compassion into AI-assisted medicine.\n→ how do we evaluate such models beyond technical accuracy?\n11. Evaluation Frameworks for Medical LLMs # Med-PaLM uses USMLE-style exams and qualitative rubrics to assess reasoning, factuality, and harm potential. Human experts assess each dimension, comparing outputs to clinicians\u0026rsquo; answers in blinded evaluations.\n→ how does Med-PaLM compare to human experts in real scenarios?\n12. From Benchmark to Bedside # Rigorous validation is required for real-world use—starting with retrospective studies, then prospective ones, all before interventional deployment. Past learnings (e.g., from diabetic retinopathy screening) stress this need.\n→ how can we responsibly scale these tools into clinical settings?\n13. Task-Specific vs. Domain-General Models # While Med-PaLM 2 shows expert-level performance on QA tasks, its capabilities must be validated across each medical subdomain. Mental health assessments, for example, require specialized evaluation and adaptation.\n→ can a high-performing model generalize across all medical use cases without fine-tuning?\n14. Toward Multimodal Healthcare AI # Medicine is inherently multimodal—spanning text, images, genomics, EHRs, and sensors. MedLM is expanding into multimodal models, which are in early research stages but promise broader clinical utility.\n→ how can AI integrate and reason across multiple data modalities safely and meaningfully?\n15. Training Innovations in Med-PaLM 2 # Med-PaLM 2 leverages instruction tuning on diverse QA datasets and advanced prompting strategies like chain-of-thought, self-consistency, and ensemble refinement. These enhance stepwise reasoning and output reliability.\n→ which techniques most boost reasoning performance in sensitive domains like healthcare?\n16. Key Takeaways # LLMs show tremendous promise in solving domain-specific problems. In cybersecurity, SecLM combines tools, reasoning, and authoritative data to empower practitioners. In healthcare, MedLM and Med-PaLM show how vertical fine-tuning, evaluation, and collaboration with clinicians drive real-world impact.\nThe overarching insight: LLMs require domain-specific architecture, data, tuning, and evaluation to move from general intelligence to real-world application.\n"},{"id":18,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m5/","title":"[Summary] Module 5: Strategies and Challenges in ML for Healthcare","section":"C3 ML Healthcare","content":" Module 5: Strategies and Challenges in ML for Healthcare # 1 Introduction to Common Clinical Machine Learning Challenges # Q1: Why is deploying machine learning in healthcare uniquely challenging? # Healthcare presents complex, high-stakes environments with unique constraints:\nData is heterogeneous, often unstructured and incomplete. Clinical settings are dynamic and contextual, with human-in-the-loop decisions. Errors have real consequences, requiring robustness and explainability. ➡️ What specific areas of ML model development are affected by these clinical challenges?\nQ2: What types of challenges emerge when applying ML in clinical settings? # Challenges include:\nData issues: missing values, coding errors, shift in distribution over time. Labeling: often derived from billing codes or heuristics—not always ground truth. Deployment: clinical workflows require integration, usability, and ethical oversight. ➡️ How does the clinical environment further complicate ML deployment?\nQ3: How does clinical practice shape ML model development? # Clinical workflows affect ML design because:\nModels must adapt to time constraints, decision pathways, and interdisciplinary teams. Interpretability and actionability are more important than raw performance. Stakeholders include not just data scientists, but also clinicians and patients. 2 Utility of Causative Model Predictions # Q1: Why is causality important in clinical machine learning? # Healthcare decisions often hinge on interventions, not just correlations:\nClinicians need to know: “What happens if I prescribe X?” Predicting causal outcomes is more useful than merely identifying associations. ➡️ How are most ML models limited when it comes to causality?\nQ2: What is the difference between predictive and causative models? # Predictive models estimate outcomes based on observed features. Causative models aim to model the effect of interventions or actions. Predictive models may reflect spurious correlations that fail when environments change. ➡️ What are the risks of using predictive models in clinical decisions?\nQ3: How can predictive models be misleading in practice? # Examples:\nPredicting lower mortality for asthma patients with pneumonia (due to ICU treatment). Models may recommend fewer ICU admissions for high-risk patients, leading to harm. These errors occur when models don’t account for treatment effects or confounding.\n➡️ How can ML practitioners improve model utility in healthcare?\nQ4: What approaches can align model outputs with clinical intent? # Incorporate domain expertise to define causal questions. Use causal inference frameworks (e.g., counterfactual analysis, propensity scores). Ensure models reflect the treatment-action relationship, not just outcome prediction. 3 Context in Clinical Machine Learning # Q1: Why is clinical context essential for interpreting ML models? # Machine learning models do not operate in isolation:\nClinical decisions depend on environmental, temporal, and institutional factors. Models trained in one hospital may fail in another due to context shifts. Context determines how predictions are used and trusted. ➡️ What types of context affect ML model performance?\nQ2: What are some examples of clinical context influencing ML predictions? # Differences in lab test ordering between departments. Temporal trends like new treatment guidelines. Resource availability: ICU beds, diagnostic equipment. These can change the meaning of input features and model outputs.\n➡️ How can ignoring context lead to unintended consequences?\nQ3: What are the risks of deploying context-unaware models? # Silent failures: model appears accurate but gives clinically invalid results. Harmful recommendations due to incorrect assumptions (e.g., missing a comorbidity). Equity concerns: unfair performance across hospitals or populations. ➡️ How can ML practitioners incorporate context into model development?\nQ4: What strategies help ensure models are context-aware? # Collaborate with domain experts to understand local workflows. Analyze data provenance and feature semantics. Perform site-specific validation before general deployment. Monitor and update models as context evolves. 4 Intrinsic Interpretability # Q1: What is interpretability and why is it vital in healthcare ML? # Interpretability refers to how easily a human can understand the reasoning behind a model’s prediction:\nClinicians need to justify decisions based on model outputs. Interpretability improves trust, safety, and regulatory compliance. Essential in high-stakes decisions like diagnosis and treatment. ➡️ What are different ways to achieve interpretability in ML?\nQ2: What is the difference between intrinsic and post-hoc interpretability? # Intrinsic interpretability: Models are interpretable by design (e.g., decision trees, linear models). Post-hoc interpretability: Use tools (e.g., SHAP, LIME) to explain black-box model behavior after training. Intrinsic models are simpler and easier to validate but may underperform on complex tasks.\n➡️ What are some examples of intrinsically interpretable models?\nQ3: What models are considered intrinsically interpretable? # Linear regression: Clear feature impact via coefficients. Decision trees: Transparent logic based on feature thresholds. Rule-based systems: Use if-then logic that mimics human reasoning. These models prioritize simplicity and clarity over complexity.\n➡️ How do we balance accuracy and interpretability in clinical settings?\nQ4: What are the trade-offs in choosing interpretable models? # Interpretable models may sacrifice accuracy on complex data. Black-box models may be more powerful but harder to validate and trust. Best practice: balance performance, interpretability, and clinical context. 5 Medical Data Challenges in Machine Learning Part 1 # Q1: What makes healthcare data particularly challenging for ML models? # Healthcare data is often:\nMessy: includes typos, missing values, inconsistent formats. Heterogeneous: comes from many sources—EHRs, images, notes, sensors. Sparse and incomplete: many features are not consistently recorded. ➡️ What is one major source of complexity in healthcare data?\nQ2: Why is data heterogeneity a significant issue? # Different institutions and clinicians record data differently. Coding systems (e.g., ICD, CPT) vary across time and space. Input formats (structured vs. unstructured) require varied preprocessing. This complicates model generalization and reproducibility.\n➡️ Beyond format, what other data issues pose problems?\nQ3: How do missing and inaccurate labels impact ML models? # Labels are often derived from billing codes or heuristics, not confirmed ground truth. Human input can introduce label noise (e.g., misdiagnoses). This affects both training quality and model evaluation. ➡️ How can we start addressing these foundational issues?\nQ4: What practices help mitigate healthcare data challenges? # Collaborate with domain experts to verify labels and clean data. Use robust data preprocessing pipelines. Augment data via external sources or clinical knowledge bases. 6 Medical Data Challenges in Machine Learning Part 2 # Q1: What are additional complexities of working with medical data? # Beyond noise and heterogeneity, medical data also suffers from:\nTemporal issues: patient data spans time and requires sequence modeling. Label latency: outcomes may be delayed, leading to incomplete labels. Data leakage: unintended inclusion of future info during training. ➡️ How does temporality specifically impact ML in healthcare?\nQ2: Why is temporality a challenge in clinical ML modeling? # Events happen in a timeline, not in isolation. Features need to be time-aligned with outcomes. Some features (e.g., lab tests) are triggered by prior events, not independent signals. Incorrect handling can result in reverse causality or misleading models.\n➡️ What is label leakage and how does it affect models?\nQ3: What is label leakage and why is it dangerous? # Leakage occurs when features directly encode the outcome. Example: using post-diagnosis medication as a predictor of diagnosis. Results in inflated performance and useless real-world predictions. ➡️ How can we mitigate these issues during data preparation?\nQ4: What are best practices to reduce data leakage and temporal issues? # Carefully define observation and prediction windows. Exclude features generated after the outcome window. Collaborate with clinicians to spot illogical or circular data flows. 7 How Much Data Do We Need? # Q1: Why is data quantity important in healthcare ML? # More data typically improves model performance by:\nAllowing better generalization and reducing overfitting. Enabling complex models like deep learning to converge. Increasing coverage of rare cases and subpopulations. ➡️ Is there a rule of thumb for how much data is \u0026ldquo;enough\u0026rdquo;?\nQ2: Is there a specific data size needed to build reliable models? # There\u0026rsquo;s no universal threshold—depends on task complexity and model type. Simpler models may perform well with smaller datasets. Deep learning typically requires large, diverse datasets for optimal performance. ➡️ Besides raw size, what else affects data utility?\nQ3: How does data diversity influence model robustness? # Diverse data improves generalization across patient subgroups. Reduces bias and enhances fairness. Captures a variety of clinical settings and disease presentations. ➡️ Are there diminishing returns with more data?\nQ4: Can collecting more data ever be inefficient or harmful? # Yes, when:\nData quality is low or inconsistent. Additional data doesn’t add new variation. Processing large datasets becomes computationally burdensome. Focus should be on quality, diversity, and relevance, not just quantity.\n8 Retrospective Data in Medicine and Shelf Life for Data # Q1: What is retrospective data and why is it commonly used in ML? # Retrospective data is historical clinical data collected during routine care:\nEasier and cheaper to obtain than prospective data. Often available in large volumes through EHRs. Used to develop predictive models and analyze outcomes. ➡️ What are limitations of using retrospective data?\nQ2: What are the risks and limitations of retrospective datasets? # Data reflects past practices, not current standards. Missingness and bias due to non-random documentation. Models may learn patterns that don’t generalize to new settings. ➡️ Can data lose value over time?\nQ3: What is the “shelf life” of clinical data and why does it matter? # Shelf life refers to how long data remains relevant and useful:\nClinical protocols, technologies, and patient populations change. Models trained on outdated data may perform poorly on current cases. Regular model retraining and validation is needed. ➡️ How can we manage these issues when developing models?\nQ4: How should retrospective data be handled for effective modeling? # Understand the temporal context of data. Align modeling goals with clinical relevance and recency. Combine with prospective validation where possible. Plan for model monitoring and updates post-deployment. 9 Medical Data: Quality vs Quantity # Q1: Is more data always better in healthcare ML? # Not necessarily—quality can matter more than raw volume:\nPoor quality data introduces noise, bias, and misleading signals. High-quality, well-labeled data leads to better generalization and clinical utility. Trade-offs exist between collecting more vs. curating better data. ➡️ What does data “quality” mean in practice?\nQ2: What are characteristics of high-quality medical data? # Accurate, clinically verified labels. Consistent formatting and standards (e.g., coding systems). Completeness and representativeness of the target population. Poor quality data may include irrelevant features or misdiagnosed labels.\n➡️ How can teams improve data quality?\nQ3: What practices can enhance data quality for ML? # Work closely with domain experts for data cleaning and labeling. Apply automated quality checks (e.g., missingness patterns, outlier detection). Use standard vocabularies (e.g., SNOMED, LOINC) to improve structure. ➡️ How should teams balance data quality and quantity?\nQ4: How should we approach the quality vs. quantity trade-off? # Prioritize relevant and diverse samples over raw scale. Smaller, higher-quality datasets often outperform large, noisy ones. Aim for balanced improvement across both dimensions where feasible. "},{"id":19,"href":"/ai-workflows/genai-systems/5-day-genai-google/day5_mlops/","title":"Day 5 – MLOps for Generative AI","section":"5-Day GenAI with Google","content":" Day 5 – MLOps for Generative AI # 1. Introduction # The rise of foundation models and generative AI (gen AI) has brought a paradigm shift in how we build and deploy AI systems. From selecting architectures to managing prompts and grounding outputs in real data, traditional MLOps needs adaptation.\nSo how do we evolve MLOps for this new generative world?\n2. What Are DevOps and MLOps? # DevOps: Automation + collaboration for software delivery (CI/CD, testing, reliability) MLOps: Adds ML-specific needs: Data validation Model evaluation Monitoring Experiment tracking These core principles set the stage, but gen AI has unique needs.\n3. Lifecycle of a Gen AI System # The gen AI lifecycle introduces five major moments:\nDiscover – Find suitable foundation models from a rapidly growing model zoo. Develop \u0026amp; Experiment – Iterate on prompts, use few-shot examples, and chains. Train/Tune – Use parameter-efficient fine-tuning. Deploy – Includes chains, prompt templates, databases, retrieval systems. Monitor \u0026amp; Govern – Ensure safety, fairness, drift detection, and lineage. Each stage requires new tooling and processes compared to traditional ML.\n4. Continuous Improvement in Gen AI # Gen AI focuses on adapting pre-trained models via: Prompt tweaks Model swaps Multi-model chaining Still uses fine-tuning and human feedback loops when needed. But not all orgs handle base model training—many just adapt existing FMs.\n5. Discover Phase: Choosing the Right FM # Why it’s hard:\nExplosion of open-source and proprietary FMs Variation in architecture, performance, licensing Model selection is now a critical MLOps task.\n6. Model Discovery Criteria # Choosing a foundation model now involves nuanced trade-offs:\nQuality: Benchmarks, output inspection Latency \u0026amp; Throughput: Real-time chat ≠ batch summarization Maintenance: Hosted vs self-managed models Cost: Compute, serving, data storage Compliance: Licensing, regulation Vertex Model Garden supports structured exploration of these options.\n7. Develop \u0026amp; Experiment # Building gen AI systems is iterative: prompt tweaks → model swap → eval → repeat.\nThis loop mirrors traditional ML but centers around prompts, not raw data.\n8. Foundation Model Paradigm # Unlike predictive models, foundation models are multi-purpose. They show emergent behavior based on prompt structure. Prompts define task type (translation, generation, reasoning). Small changes in wording can completely shift model output.\n9. Prompted Model Component # The key unit of experimentation in gen AI is: Prompt + Model → Prompted Model Component\nThis redefines MLOps: you now track prompt templates as first-class artifacts.\n10. Prompt = Code + Data # Prompts often include:\nCode-like structures (templates, control flow, guardrails) Data-like elements (examples, contexts, user input) MLOps must version prompts, track results, and match to model versions.\n11. Chains \u0026amp; Augmentation # When prompts alone aren’t enough:\nChains: Link multiple prompted models + APIs RAG: Retrieve relevant info before generation Agents: LLMs choose tools dynamically (ReAct) MLOps must manage chains end-to-end, not just components.\n12. Chain MLOps Needs # Evaluation: Run full chains to measure behavior Versioning: Chains need config + history Monitoring: Track outputs + intermediate steps Introspection: Debug chain inputs/outputs Vertex AI + LangChain integration supports these needs.\n13. Tuning \u0026amp; Training # Some tasks require fine-tuning:\nSFT: Teach model to produce specific outputs RLHF: Use human feedback to improve alignment Tune as needed—especially if prompt engineering hits limits.\n14. Continuous Tuning # Static tasks = low frequency. Dynamic tasks (chatbots) = frequent RLHF.\nBalance GPU/TPU cost with improvement needs Consider quantization to lower costs Vertex AI provides tuning infra + registry + pipelines + governance.\n15. Data in Gen AI # Unlike predictive ML, gen AI uses:\nPrompts \u0026amp; examples Grounding sources (APIs, vectors) Human preference data Task-specific tuning sets Synthetic + curated data Each has different MLOps needs: validation, versioning, lineage.\n16. Synthetic Data Use Cases # Generation: Fill in training gaps Correction: Flag label errors Augmentation: Introduce diversity Use large FMs to generate training or eval data when needed.\n17. Evaluation in Gen AI # Evaluation is hard:\nComplex, open-ended outputs Metrics (BLEU, ROUGE) often miss the mark Auto-evals (e.g. AutoSxS) use FMs as judges Align automated metrics with human judgment early on.\n18. Evaluation Best Practices # Stabilize metrics, approaches, datasets early Include adversarial prompts in test set Use synthetic ground truth if needed Evaluation = cornerstone of experimentation in gen AI MLOps\n19. Deployment in Gen AI Systems # Gen AI apps involve multiple components:\nLLMs Chains Prompts Adapters External APIs Two main deployment types:\nFull Gen AI Systems (custom apps) Foundation Model Deployments (standalone models) 20. Version Control # Key assets to version:\nPrompt templates Chain definitions Datasets (e.g. RAG sources) Adapter models Git, BigQuery, AlloyDB, and Vertex Feature Store help manage assets.\n21. Continuous Integration (CI) # CI ensures reliability through:\nUnit + integration tests Automated pipelines Challenges:\nTest generation is hard due to open-ended outputs Reproducibility is limited due to LLM randomness Solutions draw from earlier evaluation methods.\n22. Continuous Delivery (CD) # CD moves tested systems into staging/production.\nTwo flavors:\nBatch delivery: Schedule-driven, test pipeline throughput Online delivery: API-based, test latency, infra, scalability Chains are the new \u0026ldquo;deployment unit\u0026rdquo;—not just models.\n23. Foundation Model Deployment # Heavy resource demands → need:\nGPU/TPU allocation Scalable data stores Optimization (distillation, quantization, pruning) 24. Infrastructure Validation # Check:\nHardware compatibility Serving configuration GPU/TPU availability Tools: TFX infra validation, manual provisioning checks\n25. Compression \u0026amp; Optimization # Strategies:\nQuantization: 32-bit → 8-bit Pruning: Remove unneeded weights Distillation: Train small model from a larger \u0026ldquo;teacher\u0026rdquo; Step-by-step distillation can reduce size and improve performance.\n26. Deployment Checklist # Steps to productionize:\nVersion control Optimize model Containerize Define hardware and endpoints Allocate resources Secure access Monitor, log, and alert Real-time infra: Cloud Functions + Cloud Run 27. Logging \u0026amp; Monitoring # Track both:\nApp-level inputs/outputs Component-level details (chain steps, prompts, models) Needed for tracing bugs, debugging drift, and transparency.\n28. Drift \u0026amp; Skew Detection # Compare:\nEvaluation-time data vs. Production input Topics, vocab, token count, embeddings Techniques:\nMMD, least squares density, learned kernels Signals shift in user behavior or data domains.\n29. Continuous Evaluation # Capture live outputs Evaluate vs. ground truth or human feedback Track metric degradation Alert on failures or decay Production = where real testing happens.\n30. Governance # Governs:\nChains + components Prompts Data Models Evaluation metrics and lineage Full lifecycle governance = essential for compliance and maintainability.\n31. Role of an AI Platform # Vertex AI acts as an end-to-end platform for developing and operationalizing Gen AI. It supports:\nData prep Training/tuning Deployment Evaluation CI/CD Monitoring Governance It enables reuse, scalability, and full-stack observability for Gen AI teams.\n32. Model Discovery: Vertex Model Garden # Model Garden includes:\n150+ models: Google, OSS, third-party (e.g., Gemini, Claude, Llama 3, T5, Imagen) Modalities: Language, Vision, Multimodal, Speech, Video Tasks: Generation, classification, moderation, detection, etc. Each model has a card with use cases and tuning options.\n33. Prototyping: Vertex AI Studio # Vertex AI Studio offers:\nPlayground for trying models (Gemini, Codey, Imagen) UI + SDKs (Python, NodeJS, Java) Prompt testing + management One-click deploy Built-in notebooks (Colab Enterprise, Workbench) Low barrier for users from business analysts to ML engineers.\n34. Training: Full LLM Training on Vertex AI # TPU and GPU infrastructure for fast, large-scale training Vertex AI supports training from scratch and adapting open-weight models 35. Tuning: Five Key Methods # Prompt engineering – no retraining SFT (Supervised Fine-Tuning) – train on labeled examples RLHF – learn from human preferences Distillation – compress knowledge from large to small models Step-by-step distillation – Google-optimized, fewer data needs Each method balances cost, performance, and latency.\n36. Orchestration: Vertex Pipelines # Define pipelines with Kubeflow SDK Automate tuning, evaluation, and deployment Managed pipelines for Vertex foundation models Enables production-readiness and repeatability.\n37. Chain \u0026amp; Augmentation: Grounding + Function Calling # Vertex AI supports:\nRAG systems – real-time document retrieval Agent-based chains – dynamic tool use via ReAct Function calling – LLM picks which API to use, returns JSON Grounding – verifies/model output via search or private corpora Agent Builder – build search/chat agents grounded on any source Simplifies chaining, reasoning, and integrating internal data.\n38. Vector Search # Vertex AI Vector Search enables:\nHigh-scale, low-latency ANN search Billions of embeddings using ScaNN Use with text, images, hybrid metadata search Works with custom embeddings (e.g., textembedding-gecko) Choose this when you need control over chunking, retrieval, or models.\n39. Evaluate: Vertex AI Experiments \u0026amp; TensorBoard # Experimentation is essential for iterating and improving Gen AI models. Tools include:\nVertex AI Experiments: Track model runs, hyperparams, training environments Vertex AI TensorBoard: Visualize loss, accuracy, embeddings, model graphs Supports reproducibility, debugging, and collaboration.\n40. Evaluation Techniques # Ground truth metrics: Automatic Metrics using reference datasets LLM-based eval: Auto Side-by-Side (Auto SxS) with model judges Rapid Evaluation API: Fast SDK-based eval for prototyping Evaluation is deeply integrated into the development lifecycle.\n41. Predict: Vertex Endpoints # Deploy models to Vertex Endpoints for online prediction Features: Autoscaling Access control Monitoring Works with open-source and Google models 42. Safety, Bias, and Moderation # Built-in responsible AI features:\nCitation checkers: Track and quote data sources Safety scores: Detect harmful content and flag sensitive topics Watermarking: Identify AI-generated content (via SynthID) Bias detection: Ensure fairness and appropriateness Moderation: Filter unsafe responses These ensure ethical and trustworthy AI deployments.\n43. Governance Tools # Vertex Feature Store:\nTrack embedding + feature lineage Drift monitoring Feature reuse + formulas Model Registry:\nLifecycle tracking (versioning, evaluation, deployment) One-click deployment Access to evaluation, monitoring, and aliasing Dataplex:\nCross-product lineage (e.g., Vertex + BigQuery) Golden datasets/models Access governance + IAM integration These unify observability, reproducibility, and compliance across Gen AI assets.\n44. Conclusion # MLOps principles—reliability, scalability, repeatability—fully extend into Gen AI.\nGen AI adds prompt chaining, grounding, function calling, etc. Vertex AI unifies the full lifecycle across models, pipelines, and governance It supports both predictive and Gen AI use cases MLOps isn’t replaced—it’s expanded for the age of foundation models.\n"},{"id":20,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m6/","title":"[Summary]  Module 6: Best Practices, Terms, and Launching Your ML Journey","section":"C3 ML Healthcare","content":" Module 6: Best Practices, Terms, and Launching Your ML Journey # 1 Clinical Utility and Output Action Pairing # Q1: What is clinical utility and why is it important in ML? # Clinical utility refers to the real-world usefulness of a model’s predictions:\nA model must enable action that improves outcomes. Predictions that can\u0026rsquo;t lead to interventions or decisions have limited utility. This bridges the gap between technical performance and clinical relevance. ➡️ How can we ensure predictions are actually actionable?\nQ2: What is Output-Action Pairing (OAP) and how does it help? # OAP connects model outputs to a specific, predefined action:\nDefines what should happen when a model gives a certain prediction. Ensures the output aligns with clinical workflows and capabilities. Encourages careful thought about how predictions will be used. ➡️ What are some examples of OAP in clinical practice?\nQ3: What are practical examples of Output-Action Pairing? # Sepsis risk prediction → Early IV antibiotic administration. Fall risk → Increase room monitoring and physical therapy. Readmission risk → Social work referral or discharge planning. Clear linkage between prediction and intervention enhances adoption and trust.\n➡️ How does OAP guide model design and deployment?\nQ4: How does OAP influence model development choices? # Guides feature selection based on actionability. Helps prioritize precision or recall depending on the intervention. Encourages stakeholder involvement early to define clinical utility goals. 2 Taking Action - Utilizing the OAP Framework # Q1: How can the OAP framework be applied systematically? # The OAP (Output-Action Pairing) framework provides a structured approach:\nStart with the desired clinical action or intervention. Work backward to determine the prediction needed to support it. Design the model with this action-prediction link as the anchor. ➡️ What questions help clarify a good OAP strategy?\nQ2: What questions can guide effective Output-Action Pairing? # What clinical decision is this prediction meant to support? Who will take action based on the output? What are the consequences of false positives or negatives? Is there an existing workflow where this model fits? These guide the framing, design, and evaluation of the ML tool.\n➡️ Can the OAP framework prevent wasted effort or misaligned tools?\nQ3: What happens when models are built without OAP thinking? # Outputs may be ambiguous or non-actionable. Teams may build models no one knows how to use. Integration into practice becomes difficult or ineffective. OAP increases the likelihood of real-world impact.\n➡️ How does OAP support multidisciplinary collaboration?\nQ4: How does OAP promote stakeholder alignment? # Encourages communication between clinicians, engineers, and operational teams. Helps align goals, expectations, and implementation details. Everyone shares a clear understanding of what the model is for and how it will be used. 3 Building Multidisciplinary Teams for Clinical Machine Learning # Q1: Why are multidisciplinary teams essential in clinical ML projects? # Healthcare ML requires collaboration across domains:\nCombines technical expertise with clinical knowledge. Ensures models are grounded in real-world workflows. Increases likelihood of successful design, deployment, and adoption. ➡️ What roles are typically involved in such teams?\nQ2: Who are the key stakeholders in a clinical ML team? # Clinicians: define problems, validate utility, assess safety. Data scientists/engineers: model design, feature extraction, validation. IT and informatics staff: EHR integration, data access. Administrators and ethics leaders: compliance, governance, resourcing. Diverse perspectives help balance performance with feasibility and ethics.\n➡️ How do team dynamics influence project success?\nQ3: What practices foster effective collaboration? # Shared language and goals: use tools like OAP to define objectives. Iterative feedback loops with clinicians. Respect for domain boundaries and active listening. Successful teams recognize that technical and clinical inputs are equally critical.\n➡️ What challenges can arise in interdisciplinary settings?\nQ4: What are common barriers and how can they be addressed? # Misaligned incentives or timelines. Communication breakdowns or unclear roles. Resistance to change or model integration. Solution: Foster trust, transparency, and frequent engagement across disciplines.\n4 Governance, Ethics, and Best Practices # Q1: Why is governance important in clinical machine learning? # Governance ensures ML tools are:\nSafe, fair, and transparent. Aligned with legal and institutional standards. Routinely monitored and updated. It defines who is accountable for model design, deployment, and oversight.\n➡️ What are key components of ethical ML in healthcare?\nQ2: What ethical principles guide responsible ML in medicine? # Fairness: equitable performance across patient groups. Transparency: clear communication of model limitations and risks. Accountability: defined roles for decision-making and error handling. Beneficence: focus on patient well-being and do-no-harm principles. ➡️ How do we institutionalize these principles?\nQ3: What governance practices help enforce ethical use of ML? # Establish ML oversight committees with clinical and technical members. Create model review boards for performance and fairness evaluations. Define escalation plans for failures or unexpected behavior. Governance should be proactive, not reactive.\n➡️ What practical best practices support these efforts?\nQ4: What are some operational best practices in clinical ML? # Regular audits and performance monitoring. Document model versioning, data lineage, and deployment status. Ensure interdisciplinary sign-off before going live. Build models with real-world constraints and fail-safes in mind. 5 On Being Human in the Era of Clinical Machine Learning # Q1: What role do humans continue to play in clinical ML systems? # Even with advanced ML, humans remain central:\nClinicians interpret outputs in nuanced, value-laden contexts. Patients bring individual preferences and lived experiences. Human oversight is essential for ethical and compassionate care. ➡️ Why might fully automated decisions be problematic in healthcare?\nQ2: What are risks of excessive automation in clinical ML? # Models may lack empathy or context-specific judgment. Overreliance can lead to de-skilling or clinician disengagement. Errors may go unchallenged if clinicians defer too heavily to automation. Human clinicians provide interpretive judgment and ensure care remains individualized.\n➡️ How can we design systems that support, not replace, human judgment?\nQ3: How do we build ML systems that augment rather than replace clinicians? # Keep clinicians “in the loop”—with tools to override or question model outputs. Design interfaces for transparency and explanation, not just prediction. Support human strengths: empathy, narrative understanding, ethical judgment. ➡️ What values should guide human-machine collaboration in healthcare?\nQ4: What values should ML practitioners center in their design? # Respect for human dignity and individual autonomy. Empowerment, not displacement, of healthcare professionals. Continuous attention to how technology shapes behavior and trust. 6 Death by GPS and Other Lessons of Automation Bias # Q1: What is automation bias and why is it dangerous in healthcare? # Automation bias is the tendency to:\nOvertrust machine-generated suggestions, even when flawed. Ignore or discount human judgment in favor of algorithmic outputs. Lead to harmful or fatal errors, especially in high-stakes domains. ➡️ What are real-world examples of automation bias?\nQ2: What lessons do we learn from non-healthcare automation failures? # Example: “Death by GPS”—drivers blindly following GPS into unsafe areas.\nSimilar dynamics occur in medicine when clinicians follow flawed model predictions. Automation can make errors seem more trustworthy due to perceived objectivity. ➡️ How can we design systems to guard against this?\nQ3: How can ML systems reduce risk of automation bias? # Provide confidence scores, explanations, and alternative scenarios. Train users to critically evaluate model outputs. Design alerts and interfaces that encourage reflective judgment, not blind acceptance. ➡️ What role do institutions and governance play?\nQ4: How should organizations manage automation risks? # Regular audits for model drift and edge-case failures. Create feedback loops so users can flag concerning outputs. Promote a culture where questioning automation is encouraged. "},{"id":21,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m7/","title":"[Summary] Module 7: Foundation Models","section":"C3 ML Healthcare","content":" Module 7: Foundation Models # 1 Introduction to Foundation Models # Q1: What are foundation models and why are they significant? # Foundation models are large-scale models trained on massive datasets:\nThey can be adapted for many downstream tasks with minimal fine-tuning. Examples include models like BERT, GPT, and CLIP. Represent a shift from building task-specific models to training one model for many uses. ➡️ Why have foundation models become so prominent recently?\nQ2: What has enabled the rise of foundation models? # Scale of data and compute: Internet-scale corpora and powerful GPUs/TPUs. Advances in transformer architectures and self-supervised learning. Institutional and commercial support from industry leaders. These advances allow models to generalize better across modalities and domains.\n➡️ How do foundation models differ from traditional ML approaches?\nQ3: What makes foundation models different from conventional models? # Traditional models are narrow in scope—trained for a single task. Foundation models are broad and general-purpose, and adaptable post-training. They transfer knowledge learned from pretraining to new problems. ➡️ What impact might this have on healthcare?\nQ4: What is the potential of foundation models in medicine? # Accelerate ML adoption in health by reducing data and engineering needs. Enable multi-task and multimodal learning (e.g., notes + imaging). Could transform how healthcare systems approach AI tool development. 2 Adapting to Technology # Q1: Why is adaptation critical for deploying foundation models in healthcare? # Even powerful models need to be aligned with local context and goals:\nFoundation models are trained on general data, not tailored clinical settings. Without adaptation, predictions may be irrelevant or unsafe. Local fine-tuning ensures performance reflects specific use cases. ➡️ What are the main ways to adapt foundation models?\nQ2: What are strategies for adapting foundation models to healthcare? # Prompting: crafting task-specific input formats. Fine-tuning: retraining on domain-specific data. Adapter layers: lightweight modules inserted into the model to customize behavior. Adaptation can be computationally efficient and task-specific.\n➡️ What are the clinical implications of adaptation?\nQ3: How does adaptation support clinical relevance and safety? # Tailors outputs to match local patient population characteristics. Ensures alignment with regulations, vocabularies, and care guidelines. Enables deployment in settings with limited training data. ➡️ Are there risks associated with improper adaptation?\nQ4: What are potential pitfalls if adaptation is not done carefully? # Models may produce hallucinated or outdated information. Can reflect biases from pretraining data. Lack of transparency in adaptation may hinder clinical trust and oversight. 3 General AI and Emergent Behavior # Q1: What is meant by emergent behavior in foundation models? # Emergent behavior refers to capabilities that arise unexpectedly as model scale increases:\nSkills not explicitly programmed or observed in smaller models. Includes reasoning, translation, code generation, and more. Reflects how large-scale training enables complex pattern learning. ➡️ Why is this behavior important to understand in healthcare?\nQ2: What risks and opportunities do emergent behaviors present? # Can lead to unexpected breakthroughs in performance. But may also result in unpredictable outputs or failure modes. Raises concerns about controllability, bias, and hallucination. Emergence adds power—and responsibility.\n➡️ What does this imply for clinical AI deployment?\nQ3: How should healthcare practitioners approach emergent AI behavior? # Be aware that model behavior may shift with size or updates. Validate models thoroughly in specific settings. Favor models with transparency, safety checks, and ability to decline uncertain tasks. ➡️ How does this relate to broader conversations about general AI?\nQ4: What is general AI, and are foundation models a step toward it? # General AI (AGI) refers to models that perform well across diverse tasks without retraining. Foundation models exhibit early signs of generalization, but are not AGI. Healthcare must remain cautious and evidence-driven in their application. 4 How Foundation Models Work # Q1: What architectures power most foundation models? # Most foundation models are built using transformers:\nUse self-attention to model relationships between tokens. Enable parallel processing of sequences for scale and speed. Form the backbone of models like GPT, BERT, and T5. ➡️ How are these models trained at scale?\nQ2: What are key aspects of training foundation models? # Pretraining on large corpora using self-supervised tasks. Use of masked language modeling or next-token prediction. Require massive compute resources and careful engineering. ➡️ How do these models adapt to downstream tasks?\nQ3: What methods are used to fine-tune foundation models? # Supervised fine-tuning on labeled datasets. Prompt tuning or instruction tuning to guide behavior. Some use reinforcement learning from human feedback (RLHF). These allow general models to become specialized.\n➡️ What’s important to know about input/output design?\nQ4: How are inputs and outputs structured in foundation models? # Inputs are tokenized text or sequences (can include images or other modalities). Outputs may be text, logits, embeddings, or class labels. Models often respond based on context and prompt structure. 5 Healthcare Use Cases for Text Data # Q1: Why is text data important in healthcare? # Much of healthcare documentation is unstructured text:\nClinical notes, radiology reports, discharge summaries, referrals. Contains nuanced, contextual information not captured in structured fields. Foundation models offer new ways to process and understand this text. ➡️ What can foundation models do with clinical text?\nQ2: What are key use cases for foundation models on text data? # Summarization of long clinical documents. De-identification for research use. Clinical question answering and decision support. ICD code prediction or billing optimization. Models can handle complex reasoning and language generation.\n➡️ How do foundation models compare to traditional NLP in this space?\nQ3: How do foundation models improve upon older NLP approaches? # Handle longer contexts with attention mechanisms. Better generalization and language understanding. Can be prompted or fine-tuned for specific clinical tasks. They reduce the need for hand-crafted features and rules.\n➡️ What considerations are needed for responsible use?\nQ4: What are risks of using foundation models on clinical text? # Hallucination: generating incorrect or fabricated content. Bias from training data may propagate into outputs. Lack of transparency in decision-making logic. Mitigation includes domain-specific fine-tuning, human review, and guardrails.\n6 Healthcare Use Cases for Non-textual Unstructured Data # Q1: What kinds of non-text data are common in healthcare? # Medical imaging: X-rays, MRIs, CT scans. Waveforms: ECG, EEG, vital signs. Genomics and biosignals. These data types are unstructured and require specialized models. ➡️ How can foundation models be applied to these modalities?\nQ2: What are use cases for foundation models beyond text? # Radiology report generation from medical images. Multimodal fusion of images and text (e.g., CLIP-style models). Pattern recognition in long physiological time series (e.g., ICU monitors). Genomic feature embedding for disease prediction or drug discovery. ➡️ What benefits do foundation models bring to these domains?\nQ3: How do foundation models enhance non-text applications? # Enable end-to-end training from raw inputs. Reduce need for handcrafted pipelines and feature engineering. Capture latent structure across modalities (e.g., linking image with diagnosis text). ➡️ Are there challenges unique to unstructured non-text data?\nQ4: What limitations exist when applying foundation models to these data types? # Require large, annotated datasets for effective transfer. Data standardization and privacy are complex for medical images/genomics. Interpretability is often harder than with text-based models. 7 Challenges and Pitfalls # Q1: What challenges come with using foundation models in healthcare? # Despite their potential, foundation models raise several issues:\nBias from large-scale internet training data. Hallucination: generating confident but incorrect outputs. Lack of transparency in decision-making. These issues can have significant consequences in clinical environments.\n➡️ What technical limitations exist in current foundation models?\nQ2: What are some technical pitfalls of foundation models? # Struggle with numerical accuracy and reasoning. Prone to context drift or forgetting task instructions. Outputs can be nonsensical if prompts are ambiguous or poorly structured. ➡️ What risks emerge in deployment and regulation?\nQ3: What deployment risks should healthcare teams be aware of? # Difficult to monitor and validate evolving model behavior. Hard to meet regulatory and documentation standards (e.g., FDA). Models may make hidden decisions that are hard to audit. ➡️ How can we mitigate these challenges?\nQ4: What strategies help address the pitfalls of foundation models? # Fine-tuning with domain data to reduce hallucination and bias. Combine models with clinical guardrails and human oversight. Use evaluation benchmarks aligned with safety and clinical goals. 8 Conclusion # Q1: What are the transformative aspects of foundation models in healthcare? # Enable multi-task and multimodal learning. Reduce barriers to entry for building clinical AI tools. Allow fine-tuning and prompting rather than starting from scratch. Foundation models shift how we think about problem-solving with ML.\n➡️ What is the main caution despite their promise?\nQ2: What precautions should be taken before clinical deployment? # Ensure validation on real-world clinical data. Understand risks of hallucination, bias, and overconfidence. Incorporate governance and human oversight mechanisms. ➡️ What’s the path forward for teams using these models?\nQ3: How can healthcare teams prepare for working with foundation models? # Build multidisciplinary teams (clinicians, engineers, ethicists). Invest in data infrastructure for secure and high-quality inputs. Prioritize clinically relevant problems where models can augment care. "},{"id":22,"href":"/ai-workflows/","title":"AI Reasoning Stack","section":"","content":" AI Reasoning Stack # Component Role Data Modeling Foundation GenAI Systems Application Alignment \u0026amp; Reasoning Governance Eval Methods Validation MLOps Production "},{"id":23,"href":"/ai-workflows/data-modeling/","title":"Data Modeling","section":"AI Reasoning Stack","content":" Data Modeling # Scoping # Focuses on the design and structuring of:\nInputs Labels Outputs\nthat feed AI systems Applies across the AI development lifecycle, including:\nAligning models with human feedback (e.g., RLHF, DPO) Training LLMs on structured or semi-structured data Evaluating model responses using consistent taxonomies Emphasizes that:\nThe quality and structure of your data is as critical as model architecture Well-modeled data improves alignment, interpretability, and generalization Topics # Data Representations\nToken-level, embedding spaces, tabular → prompt transformation\nLabeling Schemas\nInstruction formats, multi-choice templates, scoring guidelines\nOntologies \u0026amp; Vocabularies\nStandard medical, legal, or domain-specific taxonomies\nSchema Evolution\nHow to adapt label schemas over time as tasks shift\nConnected Areas # Feeds alignment strategies like RLHF, DPO, causal learning Supports evaluation through consistent human label designs Powers genAI systems by making inputs task-aware and user-aligned "},{"id":24,"href":"/ai-workflows/genai-systems/","title":"GenAI Systems","section":"AI Reasoning Stack","content":" GenAI Systems # 5-Day GenAI with Google ↳ Day 1 - Foundational LLMs \u0026amp; Text Generation ↳ Day 1 – Prompt Engineering ↳ Day 2 – Embeddings \u0026amp; Vector Databases ↳ Day 3 – Generative Agents ↳ Day 4 – Domain-Specific LLMs ↳ Day 5 – MLOps for Generative AI AI Agents ↳ ChatGPT Prompt-Only vs OpenAI API \u0026#43; Function Calling Prompt Engineering ↳ Step-by-Step CoT Q\u0026amp;A Guide with Generative Agents Transformer Attention: Full Conceptual Breakdown Understanding How to Use BERT\u0026#39;s CLS Token for Classification Understanding Self-Attention in Transformers: A Visual Breakdown "},{"id":25,"href":"/ai-workflows/alignment-reasoning/","title":"Alignment \u0026 Reasoning","section":"AI Reasoning Stack","content":" Alignment \u0026amp; Reasoning # Alignment: Ensuring AI systems behave in ways that reflect human intent — safely, reliably, and ethically. Reasoning: Providing structural tools and methods that guide model thinking, improve transparency, and support meaningful generalization. These aren\u0026rsquo;t separate concerns — in modern AI workflows, alignment depends on structured reasoning, and reasoning is guided by the goal of human alignment.\nRLHF (Reinforcement Learning from Human Feedback) Reward modeling Preference data collection Proximal Policy Optimization (PPO) fine-tuning DPO (Direct Preference Optimization) Simplified alternative to RLHF using pairwise ranking Causality Structural causal models Counterfactuals and interventions Causal inference in ML and AI safety Graph-Based Reasoning GraphRAG (Graph-enhanced Retrieval-Augmented Generation) Knowledge graph-guided retrieval pipelines Interpretable memory structures Knowledge Graphs Ontologies and structured semantic reasoning Context-aware generation in LLMs "},{"id":26,"href":"/ai-workflows/eval-methods/","title":"Eval Methods","section":"AI Reasoning Stack","content":" Eval Methods # Eval_Methods/ ├── AI_Evals/ # Alignment-focused evals (e.g., OpenAI evals) │ ├── OpenAI_Evals/ │ ├── Benchmark_Suites/ │ └── Eval_Metrics/ ├── Human-in-the-Loop/ # Evaluation strategies w/ annotators │ ├── Labeler-Guides/ │ └── HITL-Pipelines.md ├── Eval Frameworks/ # Tools: helm, trl.eval, chat-arena └── Monitoring_vs_Eval.md # Clarify ops-vs-research boundary AI_Evals/ — Evaluation Content Focused on Alignment # Goal: Evaluate how well AI models behave according to human preferences and task goals.\nOpenAI_Evals/\nFor evaluating models with OpenAI’s evals framework — includes preference rankings, math prompts, multi-turn responses, and tool use evals.\nBenchmark_Suites/\nCurated sets of standard benchmark tasks like:\nTruthfulQA (factual alignment) MMLU (multitask understanding) BIG-Bench (general reasoning) MT-Bench / Arena-Hard (comparative LLM evals) Eval_Metrics/\nStandard and emerging metrics to quantify:\nHelpfulness Harmlessness Coherence Factuality Preference alignment Use when: you want to compare models quantitatively or analyze behavioral drift across training versions.\nHuman-in-the-Loop/ — Crowdsourced or Expert Human Judgments # Goal: Structure manual evaluation workflows using human labelers or expert annotators.\nLabeler-Guides/\nGuidelines and templates for human evaluators:\nRating rubrics Examples of “good vs bad” outputs Ethical and fairness considerations HITL-Pipelines.md\nHow to organize:\nPrompt → model response → reviewer feedback Labeling pipelines in tools like Label Studio, Prodigy, Surge AI, etc. Use when: evaluating open-ended generation, dialog quality, or subjective preferences.\nEval Frameworks/ — Tooling to Run Evals at Scale # Goal: Explore libraries and frameworks that let you run, automate, and visualize evaluation workflows.\nExamples:\nhelm (Stanford’s Holistic Eval of Language Models) trl.eval (from HuggingFace’s TRL package) chat-arena (for pairwise comparison tournaments) language-evals (emergent libraries focused on LLM evals) Use when: you want to run evals as code, integrate with CI/CD, or do head-to-head model comparisons.\nMonitoring_vs_Eval — Operational vs Research Evaluation # Goal: Clarify the difference between offline evaluation and live monitoring in production.\nEvaluation ≠ Monitoring:\nEvaluation = Pre-deployment, scenario-specific Monitoring = Post-deployment, continuous observability How feedback loops connect them\nWhy alignment evals don’t end at launch\n"},{"id":27,"href":"/ai-workflows/mlops/","title":"MLOps","section":"AI Reasoning Stack","content":" MLOps # Azure vs AWS for AI/ML/GenAI "},{"id":28,"href":"/healthcare/","title":"Healthcare","section":"","content":" AI in Healthcare Agenda # Healthcare Data – From Clinical Trials to Real-World Data # AI in Healthcare – Clinical Data, Evaluations of AI Applications (Coursera – Stanford) Clinical Data Science – Hands-on training in structured clinical data (EHR, OMOP, CDM), computational phenotyping, predictive modeling, and clinical NLP (Coursera – UColorado) Hands-On Healthcare Data – Healthcare Informatics \u0026amp; Knowledge Graph (Book – Andrew Nguyen) Causal Reasoning for Healthcare Decision # A Crash Course in Causality – Inferring Causal Effects from Observational Data (Coursera – PennMed) Causal AI – Structural Causal Models (SCMs) using DoWhy, enabling automated causal discovery, counterfactual reasoning, and intervention-based AI systems (altdeep.ai) Knowledge-Driven Healthcare Decision # Knowledge Graphs – Enabling structured reasoning for clinical decision support (Neo4j). Knowledge Graph-Enhanced RAG – Combining structured medical knowledge with AI-driven retrieval to deliver context-aware, accurate, and explainable clinical insights (Neo4j with LangChain). Regulatory Drivers of AI in Healthcare # 21st Century Cures Act (2016) FDA RWE Framework (2018) CMS \u0026amp; ONC Interoperability Rule (2020) FDA AI/ML SaMD Framework (2021) FDA AI-Generated Synthetic Control Arms (2021) HIPAA Updates for AI (2023) "},{"id":29,"href":"/healthcare/domain_knowledge/","title":"Domain","section":"Healthcare","content":" Domain – course notes # AI in Healthcare Specialization - Coursera Hands-on Healthcare Data - Book "},{"id":30,"href":"/healthcare/data/","title":"Data","section":"Healthcare","content":" Data - clinical datasets \u0026amp; vocabularies # Healthcare Data Layers Healthcare Data Sources "},{"id":31,"href":"/healthcare/clinical_ai/","title":"AI","section":"Healthcare","content":" AI – applications \u0026amp; modeling workflows # Clinical Data Science Why Clinical NLP \u0026amp; GenAI Are Growing in Healthcare "},{"id":32,"href":"/posts/","title":"Blog","section":"","content":" Blog # Ghibli Art Style Snapshots The AI Engineer Path – Scrimba 5 Steps Learning Template Hugo Setup and Deploy Hugo Source Backup "},{"id":33,"href":"/posts/ghibli_style_snapshots/","title":"Ghibli Art Style Snapshots","section":"Blog","content":" "},{"id":34,"href":"/posts/ai_engineer_path_toc/","title":"The AI Engineer Path – Scrimba","section":"Blog","content":" The AI Engineer Path – Scrimba # https://www.coursera.org/specializations/ai-engineering#courses\nIntro to AI Engineering (104 min) # Welcome to The AI Engineer Path! AI Engineering basics The code so far Polygon API sign-up \u0026amp; key Get an OpenAI API Key Overview of how the API works An API call: OpenAI dependency An API call: Instance and model An API call: The messages array A quick word about models Prompt Engineering and a challenge Adding AI to the App Tokens The OpenAI Playground Temperature The \u0026ldquo;Few Shot\u0026rdquo; Approach Adding Examples Stop Sequence Frequency and Presence Penalties Fine-tuning Creating Images with the DALL·E 3 API Intro to AI Safety Safety Best Practices Solo Project - PollyGlot You made it! Deployment (50 min) # Learn secure \u0026amp; robust deployment strategies Create a Cloudflare worker Connect your worker to OpenAI Update client side data fetching Handle CORS and preflight requests OpenAI API requests \u0026amp; responses Create an AI Gateway Error handling Create \u0026amp; deploy the Polygon API worker Fetch the stock data Download files and push to GitHub Deploy your site with Cloudflare Pages Custom domains with Cloudflare Recap \u0026amp; next steps Open-source Models (33 min) # Open source vs closed source Intro To HuggingFace.js Inference Text To Speech With HuggingFace.js Inference Transforming Images with HuggingFace.js Inference AI Models In The Browser With Transformers.js Download and Run AI Models on Your Computer with Ollama Section Recap Embeddings and Vector Databases (94 min) # Your next big step in AI engineering What are embeddings? Set up environment variables Create an embedding Challenge: Pair text with embedding Vector databases Set up your vector database Store vector embeddings Semantic search Query embeddings using similarity search Create a conversational response using OpenAI Chunking text from documents Challenge: Split text, get vectors, insert into Supabase Error handling Query database and manage multiple matches AI chatbot proof of concept Retrieval-augmented generation (RAG) Solo Project: PopChoice Agents (117 min) # AI Agent Intro Prompt Engineering 101 Control Response Formats Zooming Out Agent Setup Introduction to ReAct prompting Build action functions Write ReAct prompt - part 1 - planning ReAct Agent - part 2 - ReAct prompt ReAct Agent - part 3 - how does the \u0026ldquo;loop\u0026rdquo; work? ReAct Agent - part 4 - code setup ReAct Agent - part 5 - Plan for parsing the response ReAct Agent - part 6 - Parsing the Action ReAct Agent - part 7 - Calling the function ReAct Agent - part 8 - Housekeeping ReAct Agent - part 9 - Finally! The loop! OpenAI Functions Agent - part 1 - Intro OpenAI Functions Agent - part 2 - Demo day OpenAI Functions Agent - part 3 - Tools OpenAI Functions Agent - Part 4 - Loop Logic OpenAI Functions Agent - Part 5 - Setup Challenge OpenAI Functions Agent - Part 6 - Tool Calls OpenAI Functions Agent - Part 7 - Pushing to messages OpenAI Functions Agent - Part 8 - Adding arguments OpenAI Functions Agent - Part 9 - Automatic function calls Adding UI to agent - proof of concept Solo Project - AI Travel Agent Nice work! Multimodality (62 min) # Introduction Generate original images from a text prompt Response formats Prompting for image generation Size, quality and style Editing images Image generation challenge Image generation challenge solution GPT-4 with Vision - Part 1 GPT-4 with Vision - Part 2 Image generation \u0026amp; Vision recap OpenAI\u0026rsquo;s Assistants API (30 min) # Introducing the Assistants API How OpenAI Assistants work Create an Assistant Create a thread and messages Running an Assistant Bring it all together More to explore "},{"id":35,"href":"/posts/5_steps_learning_template/","title":"5 Steps Learning Template","section":"Blog","content":" What\u0026rsquo;s the Problem? What is the issue, gap, or challenge this module/concept is trying to address? → Transition: “So what if this problem exists?”\nWhy Does It Matter? What are the real-world stakes or consequences of not solving this problem? Who or what is affected? → Transition: “Given this urgency, what’s the smart way to tackle it?”\nWhat’s the Core Idea? What is the central concept, structure, or strategy introduced to solve the problem? → Transition: “Okay, so how would I actually apply or build this?”\nHow Does It Work? How is the idea implemented in practice? What are the steps, inputs, mechanics, or workflows? Transition: “Where does this take us next? What does it enable?”\nWhat’s Next? How does this fit into the bigger picture? What future task, analysis, or module does it support or prepare for?\n"},{"id":36,"href":"/posts/hugo-setup/","title":"Hugo Setup and Deploy","section":"Blog","content":" 🚀 Hugo + GitHub Pages Setup (User Site) # Minimal setup using hugo-book theme inside a Conda environment, with GitHub Pages deployment.\n1. Create and Activate Conda Environment # conda create -n hugo-env conda activate hugo-env 2. Install Hugo \u0026amp; Create Hugo Site with hugo-book Theme # # Install Hugo sudo apt install hugo # Or: brew install hugo # Create Hugo site hugo new site hugo-site cd hugo-site # Initialize git and add theme git init git submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book 3. Configure config.toml # baseURL = \u0026#39;https://your-username.github.io/\u0026#39; languageCode = \u0026#39;en-us\u0026#39; title = \u0026#39;My Hugo Site\u0026#39; theme = \u0026#39;hugo-book\u0026#39; [params] BookTheme = \u0026#39;light\u0026#39; BookToC = true BookCollapseSection = true BookFlatSection = false [[menu.sidebar]] name = \u0026#34;Knowledge Graph\u0026#34; url = \u0026#34;/kg/\u0026#34; weight = 1 4. Create Content and _index.md Files # # Create directories and content mkdir -p content/kg/topic1 touch content/_index.md touch content/kg/_index.md touch content/kg/topic1/_index.md hugo new kg/topic1/intro.md Directory Structure # content/ ├── _index.md ├── kg/ │ ├── _index.md │ └── topic1/ │ ├── _index.md │ └── intro.md _index.md contents # content/_index.md\n--- title: \u0026#34;Home\u0026#34; --- content/kg/_index.md\n--- title: \u0026#34;Knowledge Graph\u0026#34; bookFlatSection: false bookCollapseSection: true --- content/kg/topic1/_index.md\n--- title: \u0026#34;Topic 1\u0026#34; --- 5. Create GitHub Repository # Create repo: your-username.github.io\n(Required for GitHub User Pages) 6. GitHub Deployment # a. Generate a Personal Access Token (PAT) # Visit: https://github.com/settings/tokens Create a classic token with repo scope b. Initial Deployment (One-Time) # hugo cd public git init git checkout -b main git remote add origin https://github.com/your-username/your-username.github.io.git git add . git commit -m \u0026#34;Initial deploy\u0026#34; git push -u origin main cd .. c. Create Auto Deploy Script # deploy.sh\n#!/bin/bash hugo -D \u0026amp;\u0026amp; cd public \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -m \u0026#34;Updated site\u0026#34; \u0026amp;\u0026amp; git push origin main \u0026amp;\u0026amp; cd .. echo \u0026#34;✅ Deployment Complete!\u0026#34; Make executable:\nchmod +x deploy.sh Run anytime:\n./deploy.sh 7. Check Deployment # GitHub → Repository → Settings → Pages Source: main Folder: / (root) Live Site: https://your-username.github.io/ 8. Notes # _index.md files define sections and sidebar headings bookFlatSection = false preserves folder hierarchy bookCollapseSection = true enables collapsible sidebar hugo -D includes drafts when building "},{"id":37,"href":"/posts/hugo-source-backup/","title":"Hugo Source Backup","section":"Blog","content":" 🔒 Hugo Source Backup # This guide outlines how to back up your Hugo source files (excluding the public/ folder) to a private GitHub repository.\n📁 Folder Structure # Typical Hugo project structure:\nhugo-site/ ├── archetypes/ ├── content/ ├── layouts/ ├── static/ ├── themes/ ├── config.toml ├── public/ # \u0026lt;- This is ignored for source backup └── backup.sh # Backup script ✅ 1. Create a Private GitHub Repo # Go to https://github.com/new Name it something like hugo-source Set visibility to Private Don’t initialize with README or license ✅ 2. Initialize Git in Your Hugo Site (if not already) # git init git remote add origin https://github.com/\u0026lt;your-username\u0026gt;/hugo-source.git echo \u0026#34;public/\u0026#34; \u0026gt;\u0026gt; .gitignore ✅ 3. Create the Backup Script # Create a file named backup.sh in the root of your Hugo project:\n#!/bin/bash git add . git commit -m \u0026#34;🔒 Backup: $(date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)\u0026#34; git push origin main echo \u0026#34;✅ Backup Complete!\u0026#34; Make it executable:\nchmod +x backup.sh ✅ 4. Use It! # To back up your source files:\n./backup.sh 📝 Notes # Only your source files are backed up. The public/ folder is excluded (it’s where the generated site lives). Combine with deploy.sh for full workflow automation. "},{"id":38,"href":"/ai-workflows/genai-systems/5-day-genai-google/","title":"5-Day GenAI with Google","section":"GenAI Systems","content":" 5-Day Gen AI Intensive Course with Google – Resource Overview # GitHub for Notebooks\nDay Topic Whitepaper Code Labs Case Study 1 Foundational LLMs \u0026amp; Prompt Engineering Foundational LLMs \u0026amp; Text Generation\nPrompt Engineering 1. Prompting Fundamentals Case Study 2 Embeddings \u0026amp; Vector Stores/Databases Embeddings 2. RAG QA System\n3. Text Similarity\n4. Classification with Keras 3 Generative Agents Agents 5. Function Calling\n6. LangGraph Agent Case Study 4 Domain-Specific LLMs Domain-Specific LLMs 7. Google Search Grounding\n8. Custom Fine-Tuning 5 MLOps for Generative AI MLOps No code labs. See: E2E Gen AI Starter Pack FAQ on Large Language Models (LLMs) and Generative AI # 1. What are the fundamental components that enable Large Language Models (LLMs) to process and generate text? # LLMs are primarily powered by the Transformer architecture. This architecture utilizes mechanisms like self-attention and multi-head attention to weigh the importance of different words in the input sequence. Input text is prepared through tokenization and embedding into vector representations. The Transformer often employs encoder and decoder components, along with techniques like layer normalization and residual connections, and in some cases, Mixture of Experts (MoE) for efficient scaling. Training these models involves feeding them vast amounts of text data and employing various strategies to optimize their ability to predict the next word or token in a sequence.\n2. How have LLM architectures evolved over time, and what key breakthroughs characterize this evolution? # The evolution began with the shift towards attention mechanisms and culminated in the Transformer. Key breakthroughs include GPT-1\u0026rsquo;s unsupervised pre-training, BERT\u0026rsquo;s deep contextual understanding through masked language modeling, GPT-2\u0026rsquo;s zero-shot learning capabilities arising from scale, and the emergence of generalist reasoners like GPT-3 and GPT-4 through instruction tuning. Other notable developments include dialogue-focused models (LaMDA), explorations of scaling laws (Chinchilla), efficient scaling with MoE (GLaM, Mixtral), the development of multimodal models (Gemini), and the rise of open-source alternatives (Gemma, LLaMA series). These advancements highlight a trend towards larger, more capable models with improved reasoning, generalization, and multimodal understanding.\n3. What are the primary techniques for adapting pre-trained LLMs for specific tasks or domains? # The main techniques for adapting LLMs include fine-tuning, which involves further training the model on a smaller, task-specific dataset. Supervised Fine-Tuning (SFT) is a common approach. Reinforcement Learning from Human Feedback (RLHF) is used to align models with human preferences. Parameter Efficient Fine-Tuning (PEFT) methods allow for adaptation with fewer trainable parameters. Effective use of LLMs also relies heavily on prompt engineering, which involves crafting specific instructions to guide the model\u0026rsquo;s output, along with selecting appropriate sampling techniques to control the style and randomness of the generated text.\n4. Why is prompt engineering crucial for effectively utilizing LLMs, and what are some key prompting techniques? # Prompt engineering is critical because it directly influences the output and behavior of LLMs. By carefully designing prompts, users can guide the model to perform specific tasks, adopt certain roles, and reason through complex problems. Key techniques include zero-shot prompting (relying solely on the prompt), one-shot and few-shot prompting (providing examples), system prompting (setting the overall context), role prompting (assigning a persona), contextual prompting (providing relevant information), and advanced reasoning techniques like Chain of Thought (CoT), Step-back Prompting, and Tree of Thoughts (ToT).\n5. What are embeddings and vector databases, and how do they facilitate advanced applications of LLMs like Retrieval-Augmented Generation (RAG)? # Embeddings are vector representations of data (text, images, etc.) that capture their semantic meaning, allowing for similarity comparisons. Vector databases are specialized databases designed to efficiently store and search these high-dimensional vector embeddings. In Retrieval-Augmented Generation (RAG), user queries are embedded and used to retrieve relevant information from a knowledge base stored as vector embeddings. This retrieved information is then incorporated into the prompt, allowing the LLM to generate more accurate and contextually grounded responses.\n6. What are generative agents, and what considerations are important when developing and evaluating them, particularly in multi-agent systems? # Generative agents are autonomous entities powered by LLMs that can perceive their environment, make decisions, and take actions. Their architecture typically involves components for planning, memory, and action execution. Operationalizing agents (AgentOps) requires attention to observability and metrics. Evaluation involves assessing core capabilities, the trajectory of agent behavior, and the quality of final responses, often incorporating human feedback. In multi-agent systems, evaluating the interactions and coordination between agents becomes crucial, and specialized architectures and design patterns are employed.\n7. How are domain-specific LLMs being developed and applied in fields like cybersecurity (SecLM) and healthcare (MedLM)? # Domain-specific LLMs are created by training models on large datasets specific to a particular domain, often combined with general pre-training. SecLM for cybersecurity aims to assist with tasks like threat detection and analysis by understanding security-related language and concepts. MedLM in healthcare focuses on medical knowledge and reasoning, with applications in medical Q\u0026amp;A, diagnosis support, and clinical documentation. The development of these models requires careful consideration of domain-specific challenges, such as data privacy and the need for high accuracy, as well as specialized evaluation frameworks and deployment considerations.\n8. What are the key aspects of MLOps for Generative AI systems, and how does it differ from traditional MLOps? # MLOps for Generative AI addresses the lifecycle of these complex systems, including model discovery, development, tuning, deployment, monitoring, and governance. It shares core principles with traditional MLOps but has unique considerations due to the nature of foundation models and prompted systems. This includes managing and versioning prompts, dealing with synthetic data, specialized evaluation techniques, the deployment of large foundation models, and the importance of continuous tuning and monitoring for drift and safety. AI platforms provide tools and infrastructure to support these GenAI-specific MLOps workflows.\n"},{"id":39,"href":"/ai-workflows/genai-systems/ai_agents/","title":"AI Agents","section":"GenAI Systems","content":" AI Agents # "},{"id":40,"href":"/healthcare/domain_knowledge/ai-in-healthcare/","title":"AI in Healthcare","section":"Domain","content":"Content for the AI In Healthcare Specialization section.\nC2 Clinical Data ↳ [ToC] Course 2 ↳ [Summary] Module 1: Asking Answering Questions via Clinical DataMining ↳ [Summary] Module2: Data Available From Healthcare Systems ↳ [Summary] Module3: Representing Time Timing Events For Clinical Data Mining ↳ [Summary] Module4 : Creating Analysis Ready Dataset from Patient Timelines ↳ Clinical Text Feature Extraction Using Dictionary-Based Filtering ↳ Clinical Text Mining Pipeline (Steps 1–5) ↳ Ethics in AI for Healthcare ↳ Missing Data Scenarios in Healthcare Modeling ↳ OMOP vs. RLHF ↳ Rule-Based Electronic Phenotyping Example: Type 2 Diabetes C3 ML Healthcare ↳ [ToC] Course 3 ↳ [Summary] Module 3: Concepts and Principles of ML in Healthcare ↳ [Summary] Module 4: Evaluation and Metrics for ML in Healthcare ↳ [Summary] Module 5: Strategies and Challenges in ML for Healthcare ↳ [Summary] Module 6: Best Practices, Terms, and Launching Your ML Journey ↳ [Summary] Module 7: Foundation Models ↳ Case Study: The Hidden Danger of Correlation in Healthcare AI ↳ Categories of Machine Learning Applications in Healthcare ↳ Data Quality, Labeling, and Weak Supervision in Clinical ML ↳ Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations ↳ Foundation Models for Healthcare ↳ Healthcare Use Cases for Non-textual Unstructured Data ↳ Healthcare Use Cases for Text Data ↳ How Foundation Models Work ↳ Output-Action Pairing (OAP) Framework in Healthcare ↳ Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare C4 AI Evaluations ↳ [ToC] Course 4 C5 Capstone Projects "},{"id":41,"href":"/ai-workflows/mlops/ai_cloud_comparision/","title":"Azure vs AWS for AI/ML/GenAI","section":"MLOps","content":" Azure vs AWS for AI/ML/GenAI # Feature Azure AWS Market Share ~23% ~31% (still largest) Enterprise Adoption Strong in healthcare, finance, gov (esp. with Microsoft 365/Teams/EHR ties) Strong with startups, research, media, big tech AI/ML Tools Azure Machine Learning, OpenAI on Azure, Synapse, Cognitive Services SageMaker, Bedrock, Comprehend, Rekognition GenAI Integration 🔥 Deep OpenAI partnership (GPT, Codex, DALL·E via Azure OpenAI Service) Bedrock (Anthropic, Stability, Cohere), Titan (Amazon’s own) Ease of Use More integrated across MS ecosystem (Power BI, Excel, VS Code) More flexible but often messier to set up Learning Curve Smoother onboarding if familiar with Microsoft tools More customizable, but steeper learning curve Certifications Azure AI Engineer, Data Scientist, OpenAI Engineer (in preview) AWS ML Specialty, Solutions Architect, Bedrock tracks "},{"id":42,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/ehr_model_failure_case_study/","title":"Case Study: The Hidden Danger of Correlation in Healthcare AI","section":"C3 ML Healthcare","content":" Case Study: The Hidden Danger of Correlation in Healthcare AI # 🎯 The Goal # Build a machine learning model to predict which pneumonia patients might die, using historical Electronic Health Record (EHR) data. This would help doctors:\nIdentify high-risk patients who need ICU care Identify low-risk patients who could recover at home ✅ What Actually Happened # The model was trained on real EHR data and found a surprising pattern:\nPneumonia patients with asthma had better outcomes.\nSo the model decided:\n“Asthma patients must be low-risk!”\n🤔 Sounds wrong, right? Asthma usually makes pneumonia worse, not better.\n🧩 The Hidden Problem: A Confounding Variable # Turns out, the hospital had a policy:\nAutomatically admit asthma patients with pneumonia to the ICU for aggressive treatment.\nBecause of this:\nAsthma patients got better care Asthma patients survived more The model assumed asthma caused survival The model didn’t know about the hospital’s policy—it just saw the outcome.\n⚠️ Why This Is a Causality Problem # Concept Explanation Correlation ≠ Causation The model saw a pattern, but misunderstood the cause. Confounder (Lurking Variable) The real reason for better outcomes was ICU treatment, not asthma. Model Generalization Failure In other hospitals (with no asthma policy), this model could suggest unsafe care. False Security The model passed all metrics—accuracy, validation, etc.—but still made a dangerous inference. 🧪 Real-World Impact (If Deployed) # If this model had been used in practice:\nAsthma patients could have been flagged as low-risk Sent home or under-treated Leading to severe illness or death 🔍 Key Takeaways # EHR models can learn patterns from policies, biases, or coincidences. Confounding variables mislead models when context is missing. Medical expertise is essential to catch errors before deployment. Always ask: Is this pattern causal, or just correlative? ✅ What Should Be Done Instead? # Include domain experts when building and validating models Investigate surprising or counterintuitive predictions Test models on external datasets Use methods like causal inference to verify relationships Remember: Hope is not a strategy. "},{"id":43,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/clinical_ml_categories/","title":"Categories of Machine Learning Applications in Healthcare","section":"C3 ML Healthcare","content":" Categories of Machine Learning Applications in Healthcare # In the Stanford course and broader healthcare ML ecosystem, \u0026ldquo;practice of care\u0026rdquo; is just one of several conceptual categories used to frame ML applications. Each category reflects a different purpose and clinical context.\n🧠 1. Diagnosis # Question answered: What is wrong with the patient?\nMachine learning identifies diseases or conditions based on input data.\nExamples:\nDetecting pneumonia from a chest X-ray (image classification) Identifying arrhythmias from ECG waveforms Classifying skin lesions as malignant vs benign 📈 2. Prediction / Prognosis # Question answered: What will happen to the patient?\nML estimates future risk or outcomes to guide decision-making.\nExamples:\nPredicting 30-day readmission for heart failure Estimating sepsis onset 6 hours ahead Predicting 6-month stroke risk ⚙️ 3. Practice of Care # Question answered: How should care be delivered?\nFocuses on workflow, timing, triage, and care operations — not just predictions.\nExamples:\nForecasting ICU nurse staffing needs Scheduling follow-ups based on predicted readmission risk Prioritizing emergency department queues 🧬 4. Discovery # Question answered: What unknown patterns or mechanisms can we find?\nML is used as a research tool to uncover new relationships.\nExamples:\nDiscovering novel genetic associations with disease Identifying new disease subtypes through clustering Finding gene-drug interaction patterns 🧩 Comparison Table # Category Question Answered Data Types Used Output Example Diagnosis What condition does this patient have? Imaging, labs, clinical notes Disease label Prediction What is likely to happen next? Time-series EHR, vitals, labs Risk score Practice of Care How should we act or plan care? EHR, operational data, predictions Workflow alert Discovery What unknown pattern exists? Omics, clustering, NLP Scientific insight "},{"id":44,"href":"/ai-workflows/alignment-reasoning/causality/causal-ai/","title":"Causal AI","section":"Causality","content":" 🧭 Summary: Causal AI in ACM CCS-Style Categories # 🎯 Definition # Causal AI refers to computational systems that model, reason, and learn about cause-effect relationships. This enables systems to simulate interventions (What if\u0026hellip;?), explain outcomes, and support decision-making in uncertain, complex environments.\n📚 ACM CCS-Style Categories Where Causal AI Lives # 1. Computing Methodologies → Artificial Intelligence → Knowledge Representation and Reasoning (KR\u0026amp;R) # Focus: Representing cause-effect relationships using logic, graphs, and symbolic formalisms. Examples: Structural Causal Models (SCMs) Counterfactual and abductive reasoning Causal rules, DAG-based inference 2. Computing Methodologies → Machine Learning # Focus: Learning causal structures or estimating treatment effects from data. Examples: Causal discovery algorithms Uplift modeling Counterfactual prediction 3. Computing Methodologies → Probabilistic Modeling # Focus: Encoding causal dependencies with uncertainty. Examples: Bayesian Networks Structural Equation Models (SEMs) Probabilistic programming for causal reasoning 🌀 Venn Diagram of Causal AI\u0026rsquo;s Interdisciplinary Nature # 🔁 Core Intersections # Overlap Zone Meaning KR\u0026amp;R ∩ ML Causal Representation Learning: Learning latent causal graphs that support symbolic reasoning. KR\u0026amp;R ∩ Probabilistic Probabilistic KR: Encoding causal knowledge with uncertainty, e.g., SCMs with probabilistic inference. ML ∩ Probabilistic Causal Machine Learning: Estimating treatment effects, counterfactual prediction, causal forests. KR\u0026amp;R ∩ ML ∩ Probabilistic 🔁 Causal AI Core: Full integration — systems that can represent, learn, and reason under uncertainty. 🧩 Takeaway # Causal AI is not confined to a single ACM category. It is a cross-cutting area spanning:\nthe representation power of KR\u0026amp;R, the learning ability of ML, and the uncertainty modeling of probabilistic systems. This makes it central to trustworthy, explainable, and decision-supportive AI, especially in high-stakes fields like healthcare, economics, and law.\n"},{"id":45,"href":"/ai-workflows/alignment-reasoning/causality/causal-inference/","title":"Causal Inference","section":"Causality","content":"Content for the Causal Inference section.\n"},{"id":46,"href":"/ai-workflows/alignment-reasoning/causality/","title":"Causality","section":"Alignment \u0026 Reasoning","content":" Causality # Criteria 📈 Causal Inference 🤖 Causal AI 🎯 Core Goal Estimate treatment or policy effects from data Enable AI to reason, simulate, and plan with causality 🌍 Scope Focused on statistical estimation from real-world data Broad, includes CI + causal reasoning in intelligent agents 🛠️ Methods Matching, IVs, DiD, DAGs, do-calculus SCMs, causal discovery, RL, counterfactuals, representation learning 🗂️ Data EHRs, trials, economic panels — structured/tabular Images, text, sensor logs, simulations — multimodal 🧰 Tools DoWhy, EconML, Stata, Stan, CausalML Pyro, CausalBench, Causal Transformers, RL libraries 🧠 Theory Pearl’s SCMs, Rubin’s Potential Outcomes Extends CI with planning, control theory, generative modeling 🧪 Use Cases Drug effects, A/B testing, public health impact Clinical AI agents, counterfactual explainers, planning under uncertainty 🚀 Trends Automated causal discovery, scalable estimation Causal LLMs, structure-aware agents, causal generalization in foundation models 👥 Audience Statisticians, epidemiologists, applied economists ML/AI engineers, decision scientists, generative modeling researchers 🧭 Philosophy \u0026ldquo;Understand causes to intervene wisely\u0026rdquo; \u0026ldquo;Use causality to empower robust, generalizable, explainable intelligence\u0026rdquo; 📚 References Elements of Causal Inference: Foundations and Learning Algorithms Jonas Peters, Dominik Janzing, and Bernhard Schölkopf (2017) Causal AI Robert Osazuwa Ness (2025) 🏥 Healthcare-Focused Examples # Scenario Causal Inference Approach Causal AI Application Does a drug reduce mortality? Use propensity score matching on EHRs to estimate treatment effect Simulate outcomes, explain counterfactuals, and adapt AI decision policy Which patients benefit from a treatment? Estimate HTEs using stratification or causal forests Personalized planning agent using causal graphs and reinforcement learning What if surgery is delayed? Model counterfactuals using SCM or time-series IVs Temporal causal simulation to guide optimal intervention timing "},{"id":47,"href":"/healthcare/domain_knowledge/hands-on-healthcare-data/ch4_ehr/","title":"Ch4 EHR","section":"Hands-On Healthcare Data","content":" Ch4 Deep Dive – Electronic Health Records (EHR) - ConvSummary # 🔍 Q\u0026amp;A-Style Logical Summary # Q1: What is the central focus of Chapter 4? # Chapter 4 focuses on working with electronic health record (EHR) data using the MIMIC-III dataset, and explores medication harmonization using SQL, Neo4j (property graph), and TypeDB (typed hypergraph).\nQ2: What makes working with EHR data complex? # EHRs are highly structured but vary between implementations. Data is often redundant, inconsistent, or missing. Clinical context and domain knowledge are crucial for correct interpretation. Q3: Why was medication harmonization chosen as the use case? # Because medications are objective and widely used in EHRs, but the same drug can appear under multiple names or codes (e.g. NDCs). Harmonization is necessary to:\nNormalize names or codes. Filter out forms like \u0026ldquo;heparin flush\u0026rdquo; vs \u0026ldquo;therapeutic heparin\u0026rdquo;. Q4: How is this implemented using SQL (SQLite)? # CSVs are loaded into SQLite tables. Free-text string search is done across drug name columns (e.g., drug_name_generic LIKE \u0026ldquo;%heparin%\u0026rdquo;). NDC-based harmonization is done by: Extracting distinct NDCs. Filtering to include only valid/therapeutic ones. Rewriting queries using explicit WHERE ndc IN (...) clauses. Q5: What are the challenges of the SQL approach? # Hard to maintain mappings (e.g., repeated NDC lists). Poor separation of concerns (clinical logic leaks into queries). Not reusable across projects or datasets. Q6: How does Neo4j (property graph) improve the workflow? # Drug instances are modeled as nodes, and a new \u0026ldquo;heparin (non-flush)\u0026rdquo; concept is created. Relationships connect drug nodes to patients and prescriptions. Reusability improves because mappings are stored inside the graph. Queries become more semantic, e.g., follow a concept node rather than duplicating NDCs in every query. Q7: How are concepts created in Neo4j? # A node like Drug:Knowledge { drug: \u0026quot;Heparin (non-flush)\u0026quot;, ... } is created. Drug nodes are linked to it via [:for_drug {derived: true}] relationships. This enables querying \u0026ldquo;all patients on this drug concept.\u0026rdquo; Q8: How does TypeDB enhance this model further? # TypeDB uses strong typing and roles to model relations: patient plays prescription:patient druginstance plays prescription:prescribed_drug Separate druginstance entity and prescription relation. Can add inference rules to dynamically associate drugs with higher-level concepts. Q9: What are the two approaches to harmonization in TypeDB? # Persisted: Insert actual hierarchy facts: (parent: heparin, child: drug) isa hierarchy. Rule-based: Use rules like: rule heparin-rule: when { $d has ndc \u0026#34;xxxx\u0026#34;; $c has purl \u0026#34;...\u0026#34;; } then { (parent: $c, child: $d) isa hierarchy; } Q10: What are the tradeoffs between approaches? # Model Pros Cons SQL Ubiquitous, accessible Poor semantics, duplication of logic Neo4j Intuitive graph model, reusable concepts Fragile schemas, performance concerns TypeDB Semantic precision, rule engine Newer ecosystem, complexity, fewer tools RDF Graph Web standard, portable Very steep learning curve Q11: What are the broader lessons? # Separate clinical knowledge from code. Choose modeling strategies based on: Project duration Tooling maturity Frequency of schema changes Collaboration needs Use graphs or TypeDB to encode reusable logic and keep queries clean. 🧠 Curriculum Task-Based Summary (Chapter 4) # 🔹 1. Understanding EHR Data Models # Compare OMOP, FHIR, i2b2, PCORnet, ADaM, SDTM. Explore role of implementation guides and FHIR profiles. 🔹 2. Setup and Load EHR Data (MIMIC-III) # Use SQLite to ingest .csv files (Example 4-1). Use Neo4j or TypeDB containers (Docker). Load and explore data with basic queries. 🔹 3. Medication Harmonization Use Case # Focus on heparin, and identify pitfalls with NDC codes. Extract and deduplicate NDCs from prescriptions. Build queries that target therapeutic use only. 🔹 4. Query and Harmonize in Three Paradigms # Task SQL Neo4j TypeDB Load data pandas + sqlite3 pandas + NeoInterface pandas + TypeDB client Query for \u0026ldquo;heparin\u0026rdquo; LIKE on drug names toLower(drug_name) CONTAINS drug has name contains Harmonization Filter with ndc IN (...) Create concept node + edges Insert facts or define rules 🔹 5. Linkage and Reasoning # Create custom drug concepts. Track prescriptions per patient. Link concepts using: SQL joins Neo4j (:Patient)-[:has_prescription]-\u0026gt;(:Drug) TypeDB roles + rules. 🔹 6. Evaluate Tradeoffs and Performance # Review table of pros/cons (Table 4-2). Balance: Query simplicity vs data model reusability. Rule-driven inference vs static mapping. Ecosystem maturity. ✅ End of Chapter Outcome # You should now be able to:\nChoose the right data model (SQL, graph, hypergraph) for your RWD task. Implement and harmonize medication concepts. Balance engineering choices with clinical accuracy and long-term maintainability. Begin thinking about integrating terminologies (UMLS, SNOMED CT) into your models. "},{"id":48,"href":"/healthcare/domain_knowledge/hands-on-healthcare-data/ch6_graph_ml/","title":"Ch6 ML and Graph Analytics","section":"Hands-On Healthcare Data","content":" Ch6 Machine Learning \u0026amp; Graph-Based Analytics - ConvSummary # Part 1: Q\u0026amp;A Summary # 1. What is the difference between cleaning, harmonization, and feature engineering? # Cleaning: Removing errors or inconsistencies in the raw data. Harmonization: Mapping and aligning data semantically across datasets (e.g., converting NDC to RxNorm). Feature Engineering: Transforming data to fit the needs of specific algorithms or analysis (e.g., PCA, one-hot encoding). 2. Why are graphs more useful for harmonization than feature engineering? # Graphs help link concepts across vocabularies, terminologies, or systems. Feature engineering tends to be model-specific and harder to generalize. 3. What are the downsides of repeating cleaning/harmonization for each project? # Redundancy: Same steps are repeated across projects. Inefficiency: Each team member duplicates similar work. Inconsistency: No central source of truth for processed data. 4. What is a feature store and how does it help? # A feature store centralizes reusable, preprocessed features. Helps reduce redundancy and promotes consistency. 5. How do knowledge graphs improve the pipeline? # Data is cleaned and harmonized once at the graph level. All downstream users can reuse the harmonized view via queries or APIs. 6. What assumptions are made when using a knowledge graph? # Patient-level data and terminology concepts are stored in the same graph. Nodes/edges are tagged with metadata (e.g., timestamps, source). The graph is a supergraph enabling subgraph extraction. 7. What are graph embeddings and why are they useful? # They convert graph structures into vectors usable in ML models. Enable pattern detection, similarity analysis, and deep learning. 8. What is node2vec? # Random walk-based graph embedding technique. Uses return (p) and in-out (q) parameters to tune graph walk. Captures homophily and structural equivalence. 9. What is cui2vec? # Embeds UMLS CUIs based on co-occurrence in various RWD sources. Context-aware (claims, notes, publications). Useful for understanding concept similarity. 10. What is med2vec? # Uses temporal sequence of medical events to create visit-based embeddings. Retains longitudinal context. 11. What is snomed2vec? # Embeds SNOMED CT concepts using hierarchical and network-based methods. Includes alternatives like metapath2vec and Poincaré embeddings. 12. What are some challenges with pretrained embeddings? # Risk of overfitting to training data domain (e.g., CMS claims). May not generalize well to other populations or use cases. Introduces extra model layer to maintain and tune. Part 2: Curriculum-Style Breakdown with \u0026ldquo;Why\u0026rdquo; # 🧭 Phase 1: Understand the Motivation # Task: Read and distinguish between cleaning, harmonization, and feature engineering. Why: Clarifies each pipeline component and prevents misuse of graphs for tasks like feature engineering. 🧱 Phase 2: Explore Pipeline Challenges # Task: Analyze Figures 6-6 to 6-9 on pipeline repetition and inefficiency. Why: Understand how lack of standardization leads to duplicated efforts. 🧠 Phase 3: Learn about Feature Stores # Task: Study how feature stores centralize and reuse engineered features. Why: Saves time, increases reproducibility, and reduces tech debt. 🌐 Phase 4: Integrate Knowledge Graphs # Task: Understand what goes into a knowledge graph (patient data + ontologies). Why: Enables one-time harmonization per data source, allowing scalable reuse. 🧩 Phase 5: Explore Graph Embedding Techniques # Task: Implement node2vec on a small graph. Why: Learn homophily vs structural equivalence, key for biomedical graph reasoning. 🧬 Phase 6: Biomedical Concept Embeddings # Task: Compare and contrast cui2vec, med2vec, and snomed2vec. Why: Appreciate how embeddings differ by data type (temporal, co-occurrence, hierarchical). ⚠️ Phase 7: Real-World Concerns with Embeddings # Task: Evaluate pretrained embeddings and consider limitations (overfitting, generalizability). Why: Embeddings may look good on paper but can fail in new domains. 🔁 Phase 8: Apply to Your Use Case # Task: Pick a small real-world use case and simulate a pipeline using a knowledge graph and embedding. Why: Reinforces learning and identifies operational gaps in pipeline design. "},{"id":49,"href":"/ai-workflows/genai-systems/ai_agents/openai_api_function_calling_comparison/","title":"ChatGPT Prompt-Only vs OpenAI API + Function Calling","section":"AI Agents","content":" ChatGPT Prompt-Only vs OpenAI API + Function Calling # This document shows a side-by-side comparison between using ChatGPT manually (prompt-only) and using the OpenAI API with function calling.\n🟦 ChatGPT (Prompt-Only / UI Version) # This is what you do in the browser:\nUser: What’s the weather in Paris? ChatGPT: The weather in Paris is likely mild and partly cloudy this time of year. No function is actually called The model guesses based on training data Not real-time, not accurate, not programmable 🟨 OpenAI API + Function Calling Version (Working + Realistic) # import openai import json openai.api_key = \u0026#34;your_api_key_here\u0026#34; # 1. Define the available function functions = [ { \u0026#34;name\u0026#34;: \u0026#34;get_weather\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Get the current weather in a given city.\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;city\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Name of the city\u0026#34;} }, \u0026#34;required\u0026#34;: [\u0026#34;city\u0026#34;] } } ] # 2. Send user prompt + function list to GPT response = openai.ChatCompletion.create( model=\u0026#34;gpt-4-0613\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What’s the weather in Paris?\u0026#34;}], functions=functions, function_call=\u0026#34;auto\u0026#34; # Let GPT decide ) # 3. GPT returns a function call (it won\u0026#39;t run it) function_call = response[\u0026#34;choices\u0026#34;][0][\u0026#34;message\u0026#34;].get(\u0026#34;function_call\u0026#34;) if function_call: # Parse the function name and arguments fn_name = function_call[\u0026#34;name\u0026#34;] args = json.loads(function_call[\u0026#34;arguments\u0026#34;]) # 4. Run the function yourself (e.g., call a real API or simulate) def get_weather(city): # In reality, you\u0026#39;d call a weather API like OpenWeatherMap here return f\u0026#34;The current weather in {city} is 68°F and sunny.\u0026#34; result = get_weather(args[\u0026#34;city\u0026#34;]) # 5. Send result back to GPT for natural language output followup = openai.ChatCompletion.create( model=\u0026#34;gpt-4-0613\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What’s the weather in Paris?\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;name\u0026#34;: fn_name, \u0026#34;content\u0026#34;: result} ] ) print(followup[\u0026#34;choices\u0026#34;][0][\u0026#34;message\u0026#34;][\u0026#34;content\u0026#34;]) else: print(\u0026#34;GPT did not call a function.\u0026#34;) 🔍 Summary Table # Step Prompt-Only (UI) API + Function Calling Input User types text prompt Dev sends user prompt + function definitions Function selection None — model guesses GPT chooses from provided functions Execution Not possible Developer runs function (e.g., API call) Output Text only, not real-time GPT formats real function output into natural reply "},{"id":50,"href":"/healthcare/clinical_ai/clinical_ai_usecase/","title":"Clinical Data Science","section":"AI","content":" Clinical Data Science # Core Priority: Retrieval-Augmented Generation (RAG) # RAG is one of the most in-demand skills in clinical GenAI due to:\nThe need to ground LLMs in real patient data Compliance, privacy, and traceability Applications like: Clinical Question Answering Summarization of EHRs Evidence-based recommendations Key Tools: # Vector DBs: Vertex AI Search, Pinecone, FAISS LLMs: Gemini, GPT-4, PaLM, Med-PaLM Frameworks: LangChain, LlamaIndex, Vertex Extensions Other High-Demand Skillsets # Clinical NLP \u0026amp; Information Extraction\nNamed Entity Recognition (NER) Negation detection Temporal event extraction Tools: scispaCy, MedSpaCy, cTAKES, ClinicalBERT LLMOps \u0026amp; GenAI Engineering\nPrompt tracking and versioning Chain-of-Thought reasoning pipelines RAG monitoring and evaluation Tools: LangChain, LangSmith, PromptLayer, Trulens 3. Knowledge Graphs \u0026amp; Ontologies # - UMLS, SNOMED, HPO integration - Graph-based document ranking - Symbolic-neural hybrid reasoning - **Tools**: Neo4j, BioPortal APIs, KG-BERT 4. Temporal Modeling \u0026amp; Phenotyping # - Patient timeline extraction - Longitudinal modeling - Conversion to OMOP/FHIR representations - **Tools**: PyOMOP, Synthea, FHIR parsers 5. Multimodal Clinical AI # - OCR and document understanding - Fusion of tables, images, and text - Radiology + Report generation - **Tools**: Document AI (GCP), Form Recognizer (Azure), BioGPT-Vision "},{"id":51,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/nlp_clinical_text/","title":"Clinical Text Feature Extraction Using Dictionary-Based Filtering","section":"C2 Clinical Data","content":" 🧬 Clinical Text Feature Extraction Using Dictionary-Based Filtering # This guide demonstrates a simplified approach for processing clinical text without removing PHI directly. Instead, it extracts only medical terms from a predefined dictionary (simulated knowledge graph), which passively excludes PHI and enables downstream analyses.\n✅ Objective # Extract present, positive mentions of clinical concepts (e.g., diseases, symptoms, drugs). Avoid mentions that are negated or refer to historical/family context. Demonstrate the principle: \u0026ldquo;Keep only medical terms\u0026rdquo; as an alternative to direct PHI removal. 🧾 Input Example # Patient complains of chest pain. No signs of pneumonia. History of diabetes mellitus. Prescribed metformin. Mother had breast cancer. 🧠 Procedure Overview # Define a medical term dictionary (simulating a knowledge graph). Split the clinical note into sentences. Ignore sentences with negation or irrelevant context. Match and extract terms from the dictionary. Output structured features for downstream use. 🧪 Code Implementation (Python) # import re # 1. Simulated clinical note clinical_note = \u0026#39;\u0026#39;\u0026#39; Patient complains of chest pain. No signs of pneumonia. History of diabetes mellitus. Prescribed metformin. Mother had breast cancer. \u0026#39;\u0026#39;\u0026#39; # 2. Simulated knowledge graph (medical term dictionary) medical_terms = { \u0026#34;chest pain\u0026#34;: \u0026#34;symptom\u0026#34;, \u0026#34;pneumonia\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;diabetes mellitus\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;metformin\u0026#34;: \u0026#34;drug\u0026#34;, \u0026#34;breast cancer\u0026#34;: \u0026#34;disease\u0026#34; } # 3. Split into sentences sentences = re.split(r\u0026#39;\\.\\s*\u0026#39;, clinical_note.strip()) features = [] # 4. Process each sentence for sentence in sentences: sentence_lower = sentence.lower() # 5. Skip negated or historical context if \u0026#34;no \u0026#34; in sentence_lower or \u0026#34;history of\u0026#34; in sentence_lower or \u0026#34;mother had\u0026#34; in sentence_lower: continue # 6. Match medical terms for term in medical_terms: if term in sentence_lower: features.append({ \u0026#34;term\u0026#34;: term, \u0026#34;type\u0026#34;: medical_terms[term], \u0026#34;sentence\u0026#34;: sentence.strip() }) # 7. Output extracted features for feature in features: print(f\u0026#34;Found {feature[\u0026#39;type\u0026#39;]} → \u0026#39;{feature[\u0026#39;term\u0026#39;]}\u0026#39; in: \\\u0026#34;{feature[\u0026#39;sentence\u0026#39;]}\\\u0026#34;\u0026#34;) 📤 Sample Output # Found symptom → \u0026#39;chest pain\u0026#39; in: \u0026#34;Patient complains of chest pain\u0026#34; Found drug → \u0026#39;metformin\u0026#39; in: \u0026#34;Prescribed metformin\u0026#34; 📌 Summary # This method:\nAvoids direct PHI detection Extracts useful clinical concepts only Can be adapted to larger vocabularies and real NLP tools (e.g., spaCy, scispaCy, NegEx) Perfect for research scenarios where structured clinical features are needed but full de-identification is too complex.\n"},{"id":52,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/clinical_text_mining_pipeline/","title":"Clinical Text Mining Pipeline (Steps 1–5)","section":"C2 Clinical Data","content":" 🏥 Clinical Text Mining Pipeline (Steps 1–5) # This document outlines a high-level clinical text mining pipeline using knowledge graphs, NLP, and structured indexing. The goal is to extract, enrich, and analyze clinical concepts from raw EMR text.\n🧾 Step 1: Preprocessing Clinical Documents # Goal: Prepare and normalize clinical notes for processing.\nTools: Text cleaning, sentence segmentation, tokenizer.\n# Example: Clean and split into sentences import re clinical_note = \u0026#34;Pt c/o chest pain. No signs of pneumonia. History of stroke. Prescribed metformin.\u0026#34; sentences = re.split(r\u0026#39;\\.\\s*\u0026#39;, clinical_note.lower()) 🧠 Step 2: Extract Terms Using Knowledge Graph + NLP # Goal: Identify medical terms using a knowledge graph and remove ambiguous, negated, or contextual mentions.\nTools: Knowledge Graph (e.g., UMLS), NegEx, ConText\n# Simulated medical term dictionary (knowledge graph-based) medical_terms = {\u0026#34;chest pain\u0026#34;: \u0026#34;symptom\u0026#34;, \u0026#34;pneumonia\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;stroke\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;metformin\u0026#34;: \u0026#34;drug\u0026#34;} # Filtered sentences (simulate negation/context removal) filtered_mentions = [] for s in sentences: if \u0026#34;no \u0026#34; in s or \u0026#34;history of\u0026#34; in s: continue for term in medical_terms: if term in s: filtered_mentions.append(term) 🗂️ Step 3: Index Positive, Present Mentions # Goal: Store structured, filtered term mentions for later search.\nTools: JSON/DB-based indexing, storing patient-term mappings.\nindexed_mentions = [ {\u0026#34;patient_id\u0026#34;: 1, \u0026#34;term\u0026#34;: \u0026#34;chest pain\u0026#34;}, {\u0026#34;patient_id\u0026#34;: 1, \u0026#34;term\u0026#34;: \u0026#34;metformin\u0026#34;}, ] 🧭 Step 4: Query-Time Semantic Expansion # Goal: Expand the user’s query using KG (synonyms, variants, etc.) and disambiguate based on context.\nTools: Knowledge Graph (UMLS), synonym/semantic type lookup, optional filters\nquery = \u0026#34;stroke\u0026#34; expanded_terms = [\u0026#34;stroke\u0026#34;, \u0026#34;cva\u0026#34;, \u0026#34;cerebrovascular accident\u0026#34;] # Disambiguate (simplified) def is_valid(term, patient_age, season): return not (term == \u0026#34;heatstroke\u0026#34; and patient_age \u0026lt; 18 and season == \u0026#34;summer\u0026#34;) 📊 Step 5: Build Patient-Feature Matrix for Analysis # Goal: Aggregate term mentions per patient for cohort selection and modeling.\nTools: Pandas, matrix construction, temporal tagging\nfrom collections import defaultdict feature_matrix = defaultdict(lambda: {\u0026#34;stroke_mention\u0026#34;: 0}) patient_metadata = {1: {\u0026#34;age\u0026#34;: 65, \u0026#34;season\u0026#34;: \u0026#34;spring\u0026#34;}} for mention in indexed_mentions: pid = mention[\u0026#34;patient_id\u0026#34;] term = mention[\u0026#34;term\u0026#34;] if term in expanded_terms and is_valid(term, patient_metadata[pid][\u0026#34;age\u0026#34;], patient_metadata[pid][\u0026#34;season\u0026#34;]): feature_matrix[pid][\u0026#34;stroke_mention\u0026#34;] += 1 print(dict(feature_matrix)) ✅ Summary # Step Goal Tools 1 Clean \u0026amp; tokenize notes Regex, NLP 2 Extract clean medical terms KG, NegEx, filtering 3 Store structured mentions JSON, DB 4 Expand/interpret queries KG, synonyms, disambiguation 5 Analyze for research Patient-feature matrix, Pandas This modular pipeline separates data preparation from query-time flexibility, making it robust and reusable.\n"},{"id":53,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/data_quality_labeling_weak_supervision_qa/","title":"Data Quality, Labeling, and Weak Supervision in Clinical ML","section":"C3 ML Healthcare","content":" Data Quality, Labeling, and Weak Supervision in Clinical ML # Q1: What does \u0026ldquo;Garbage In, Garbage Out\u0026rdquo; mean in machine learning? # It means that no model, no matter how advanced, can compensate for poor-quality data. If your input data is noisy, biased, irrelevant, or mislabeled, your model will reflect those flaws.\n✅ The choice of data and problem matters more than the algorithm itself.\nQ2: Can large, rich datasets still be garbage? # Yes — if the data is fundamentally flawed or based on faulty assumptions (like phrenology), more volume just means more noise.\n📌 Data quality ≠ data quantity.\nQ3: What makes clinical data especially tricky to label accurately? # Lack of standardized label definitions Evolving medical criteria (e.g. changing diabetes thresholds) Some labels (e.g., mortality) are easier to pin down Others (e.g., pneumonia, hypertension) require complex confirmation (labs, imaging, notes) 🧠 Medical label creation is hard, expensive, and often subjective.\nQ4: How do we deal with label noise in practice? # Label noise is inevitable, but manageable.\nStrategies:\nHave domain experts label a subset for benchmarking Use multiple reviewers to estimate disagreement rate Triangulate labels (e.g., combine ICD codes + meds + clinician notes) 📉 This reduces label noise but often shrinks dataset size.\nQ5: Is it ever okay to use noisy labels? # Surprisingly, yes — if you have enough data.\n📈 A Stanford study found:\n90% label accuracy ≈ baseline (with 50% more data) 85% label accuracy ≈ baseline (with 100% more data) ✅ Rule of thumb:\n10% noise → 1.5× more data 15% noise → 2× more data Q6: What is weak supervision? # Weak supervision refers to learning from labels that are:\nNoisy Incomplete Imperfectly defined This is common in healthcare due to:\nThe cost of expert labeling The complexity of clinical truth 👨‍⚕️ That’s why domain experts + scalable label strategies are a key bottleneck in clinical ML.\nQ7: If training data is noisy, what about the test set? # The test set must be as clean as possible.\nWhy?\nIf test labels are noisy, your evaluation metrics will be inaccurate It may underestimate model performance, leading to incorrect conclusions 📌 Training set: can handle some noise.\n📌 Test set: must approximate gold-standard ground truth.\n🔑 Final Takeaways # Principle Why It Matters Garbage In, Garbage Out Bad input = bad model, no matter the algorithm Labels ≠ Truth Always validate how close your labels are to clinical reality More Data ≠ Better Data Large, outdated, or noisy datasets can harm performance Weak Supervision Works (With Scale) Noisy labels can be offset by higher volume Test Set Must Be Clean Final evaluation must reflect ground truth "},{"id":54,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/diagnostic_metrics_and_curves/","title":"Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations","section":"C3 ML Healthcare","content":" Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations # This guide summarizes the core diagnostic metrics based on anchoring logic (condition vs. prediction), and how these metrics relate to ROC and PR curves — especially under balanced vs. imbalanced class distributions.\n🔹 Test-Centric Metrics (Anchored on Actual Condition) # Evaluates test performance, independent of disease prevalence. Anchor: Ground truth label (actual condition). Positive-Focused (Sensitivity) # Fix actual Positive label. Incorrect prediction: False Negative (FN). Pair: (TP, FN) Sensitivity = TP / (TP + FN) Negative-Focused (Specificity) # Fix actual Negative label. Incorrect prediction: False Positive (FP). Pair: (TN, FP) Specificity = TN / (TN + FP) 🔸 Outcome-Centric Metrics (Anchored on Prediction) # Evaluates usefulness of test result, dependent on both test performance and prevalence. Anchor: Test result (prediction output). Positive-Focused (PPV / Precision) # Fix Positive prediction. Incorrect prediction: False Positive (FP). Pair: (TP, FP) Positive Predictive Value (PPV) = TP / (TP + FP) Negative-Focused (NPV) # Fix Negative prediction. Incorrect prediction: False Negative (FN). Pair: (TN, FN) Negative Predictive Value (NPV) = TN / (TN + FN) 📊 Extension to ROC and PR Curves # 🎯 ROC Curve (Receiver Operating Characteristic) # What it does: # Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) across thresholds. TPR = Sensitivity = TP / (TP + FN) FPR = 1 − Specificity = FP / (FP + TN) These metrics are calculated by conditioning on the actual class labels, not predictions. Anchoring View: # ✅ Test-Centric / Condition-Anchored Starts from actual condition and evaluates how well the test distinguishes between classes. Independent of class imbalance in its calculation. Use Case: # Suitable when both positive and negative classes are equally important. Can be misleading in highly imbalanced datasets (e.g., rare disease). 📈 Precision-Recall (PR) Curve # What it does: # Plots Precision (PPV) vs. Recall (Sensitivity) across thresholds. Precision = TP / (TP + FP) Recall = TP / (TP + FN) Anchoring View: # ✅ Outcome-Centric / Prediction-Anchored Focuses on the model’s positive predictions and how often they are correct. Particularly useful for evaluating performance on the positive class in imbalanced datasets. Use Case: # Ideal for problems with class imbalance, where the positive class is rare but important (e.g., cancer detection, fraud, anomaly detection). Answers: “When the model says positive, can I trust it?” 🧠 Summary of Metric Anchors and Curve Use # Curve Type Metrics Used Anchored On Evaluation Focus Best For ROC TPR (Sensitivity), FPR Actual condition Discrimination ability Balanced class settings PR Precision (PPV), Recall Prediction output Precision of predictions Imbalanced settings 💡 Takeaway: # ROC Curve is a test-centric (condition-anchored) tool: great for balanced data, focuses on test performance across thresholds. PR Curve is an outcome-centric (prediction-anchored) tool: best for imbalanced data, reflects how reliable positive predictions are. "},{"id":55,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/ethics_in_ai_healthcare_qna/","title":"Ethics in AI for Healthcare","section":"C2 Clinical Data","content":" Ethics in AI for Healthcare: A Guided Q\u0026amp;A Framework # This document presents a structured chain-of-thought (CoT) using guiding questions and answers to understand ethical considerations in the development and deployment of AI in healthcare, based on Module 7 from the Stanford \u0026ldquo;Introduction to Clinical Data\u0026rdquo; course.\n1. Why is ethics important in the context of AI in healthcare? # Answer:\nAI tools impact patients directly or indirectly, whether through their development (research) or their deployment (clinical practice). Each of these domains carries different ethical responsibilities that must be considered and governed carefully.\n➡️ Leads to: Understanding the foundations of research ethics.\n2. How has the field of research ethics developed over time? # Answer:\nThrough responses to unethical practices (e.g., Tuskegee Study, Nazi experiments), a series of ethical frameworks and regulations emerged, including the Nuremberg Code, the Declaration of Helsinki, and most notably, the Belmont Report.\n➡️ Leads to: A deeper look into the Belmont Report and its enduring impact.\n3. What does the Belmont Report contribute to research ethics? # Answer:\nIt introduces three core principles:\nRespect for Persons: Informed consent and autonomy Beneficence: Minimize harm, maximize benefit Justice: Fair distribution of research benefits and burdens ➡️ Leads to: Applying these principles to modern AI data sources.\n4. Where does AI get its data, and what ethical concerns arise? # Answer:\nAI uses data from research repositories, clinical records, and even consumer devices. Ethical concerns include consent validity, privacy, data security, and the risk of underrepresenting vulnerable populations.\n➡️ Leads to: Addressing secondary uses of data and consent workarounds.\n5. How can researchers ethically use data collected for other purposes? # Answer:\nVia:\nQA exemptions Use of de-identified data IRB-approved waiver of consent\nThese methods are sometimes necessary but ethically controversial due to risks of eroding public trust. ➡️ Leads to: The ethical dilemma of returning individual results.\n6. Should researchers return results to participants? # Answer:\nIt depends. Options range from never returning results (to avoid harm/confusion) to always returning them (to respect autonomy). Most agree on a middle ground: only return results that are valid and actionable.\n➡️ Leads to: Examining systems where research and practice are merged—like a Learning Health System.\n7. What is a Learning Health System (LHS), and how does it relate to AI? # Answer:\nAn LHS continuously learns from clinical care data to improve outcomes. AI is central to this feedback loop, but it blurs the line between research and care, making traditional ethical boundaries harder to apply.\n➡️ Leads to: Rethinking ethical frameworks for hybrid systems like LHS.\n8. Is there an ethical model better suited for a Learning Health System? # Answer:\nYes. A proposed model includes duties to:\nRespect patients (via transparency, not just consent) Improve care (beneficence) Reduce inequality (justice) Engage both clinicians and patients in the learning process\nHowever, it lacks strict rules for handling trade-offs between these duties. Summary:\nEach principle in the Belmont Report supports the others. Respect enables informed choice, beneficence ensures that choice isn\u0026rsquo;t harmful, and justice guarantees fairness across all participants. As AI transforms healthcare, our ethical thinking must evolve accordingly.\n"},{"id":56,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/foundatoin_model/","title":"Foundation Models for Healthcare","section":"C3 ML Healthcare","content":" Foundation Models for Healthcare # What are Foundation Models? # Foundation models are trained on massive amounts of unlabeled data using self-supervised or unsupervised learning. They are \u0026ldquo;foundational\u0026rdquo; because they can be adapted to multiple downstream tasks with high efficiency and minimal data. They demonstrate sample efficiency and can handle multiple modalities like text, images, genomics, etc. Few-Shot vs. Zero-Shot Learning # Few-Shot Learning: Learns from just a few labeled examples per class and generalizes to new examples. Zero-Shot Learning: Learns to perform tasks it hasn’t seen in training, relying on general knowledge from pretraining. ➡️ These abilities allow foundation models to generalize efficiently across healthcare tasks, even with limited supervision.\nWhat kind of data powers these models? # Foundation models are trained on multi-modal health data:\nText: Clinical notes, EHRs, literature Images: X-rays, MRIs, CTs Sequences: Genomics, proteomics Graphs: Molecular structures Time Series: ECGs, continuous monitoring Video: Ultrasound Who provides this data? # Hospitals, pharma, insurance payers, academic researchers, patients (via wearables), and public forums. Downstream Use Cases # For providers: Diagnosis, treatment planning, trial recruitment, drug discovery. For patients: QA, health education, personalized guidance, assistive care. Foundation models serve as AI interfaces to improve decision-making and patient engagement.\nWhy foundation models are timely now # Human systems evolve linearly, but technology is exponential. Data is growing rapidly, e.g.: 1950s: Data doubled every 50 years. 2020s: Every 73 days. Healthcare data exploded from 150 EB (2013) → over 2,000 EB (2020). The Chessboard Paradox # A grain of rice doubled per square = \u0026gt;9B grains by square 64. Shows how exponential growth is counterintuitive to humans. Compute acceleration # Moore’s Law + AI accelerators (e.g., GPUs) made it feasible to train large foundation models. ➡️ These forces combine to make now the critical window to apply foundation models in healthcare.\nNarrow vs. General AI # Narrow AI: Performs one task; static. General AI: Learns multiple tasks and evolves over time. Foundation models aim for General AI characteristics. Emergent Behaviors # Large-scale models exhibit behaviors not explicitly programmed. Example: Google’s PaLM: 8B: Basic QA \u0026amp; language understanding 62B: Summarization, code completion 540B: Common-sense reasoning, joke explanation, logic chaining Risk: Hallucination # Model may generate confident but false outputs (e.g., imaging results that don’t exist). Needs human oversight for reliability. Transformer Components # Self-attention: Captures relationships between all tokens. Encoder: Converts input tokens into vector embeddings. Decoder: Generates output from internal representation. RLHF: Reinforcement Learning with Human Feedback # Step 1: Train a supervised model on human examples. Step 2: Collect human preferences to train a reward model. Step 3: Fine-tune the language model using reinforcement learning (PPO). This aligns model outputs with human intent and preferences.\nWhat is Prompt Engineering? # Crafting inputs to steer output behavior of foundation models. Prompt Types # Simple instructions Role-based prompts Few-shot examples Chain-of-Thought (CoT) Zero-shot-CoT (\u0026ldquo;Let\u0026rsquo;s think step by step.\u0026rdquo;) Self-consistency (multiple CoTs, pick majority) Generative knowledge prep (generate before answering) Text-Based Applications in Healthcare # Appointment scheduling Inbox management Chart summarization Trial eligibility Decision support Medical QA and patient communication ➡️ These tools reduce burnout and support both provider productivity and patient engagement.\nModalities Beyond Text # Imaging (X-rays, CTs) Genomics/proteomics Signal data (ECG) VATT-like models process multiple data types in a unified transformer architecture. Do Foundation Models “Understand” Imaging? # They can generate plausible results, but: Miss clinical context Can\u0026rsquo;t compare time-series or integrate history like radiologists Imaging as a Biomarker Source # CT @ L3 can yield: Fat/muscle measurements Aortic calcification Organomegaly Predictive health markers Foundation models unlock quantitative phenotyping from visual data.\nClinical Readiness Gaps # 83% clinicians want AI in training 70% feel overwhelmed by new tech Academia vs. Industry # Industry dominates in compute + data Collaboration is essential for clinical relevance \u0026amp; ethical development Model Drift Risks # Data Drift: Input data distribution changes Model Drift: Degrading performance over time Deployment Best Practices # Monitor performance regularly Update models with new data Ensure data quality Audit for fairness and bias Collaborate across sectors ➡️ Treat foundation models like medical devices — with continuous monitoring, recalibration, and governance.\n📚 Additional Readings # Attention Is All You Need The Illustrated Transformer The Annotated Transformer Opportunities and Risks of Foundation Models (CRFM) Shifting ML for Healthcare – Nature Biomedical Engineering "},{"id":57,"href":"/ai-workflows/alignment-reasoning/graph-reasoning/","title":"Graph Reasoning","section":"Alignment \u0026 Reasoning","content":" Graph Reasoning: Knowledge Graph (KG) vs. GraphRAG # Knowledge Graph (Structure) ➕ LLM (Reasoning) ➡️ GraphRAG (Hybrid QA System) 🔄 Relationship Between KG and GraphRAG # A Knowledge Graph can serve as the data backbone for GraphRAG. KG provides the structured facts, GraphRAG adds flexible retrieval and LLM reasoning. GraphRAG extracts subgraphs, embeds them, and uses LLMs to generate natural language answers. LLMs can also help enrich the KG, creating a feedback loop. GraphRAG is essentially a modern hybrid system, combining symbolic structure with neural flexibility. Side-by-Side Comparison # Feature KG GraphRAG 🧩 Core Idea Graph-based representation of entities and relationships Retrieval-Augmented Generation with a graph-structured retrieval backend 🏗️ Structure Nodes (entities/concepts) + edges (relations) Combines a graph + retriever + LLM (generator) 🔍 Primary Use Case Semantic search, reasoning, data integration, and explainable AI Answering complex queries with structured reasoning + natural language generation 🧠 Reasoning Type Symbolic / rule-based / graph traversal Hybrid: retrieval + neural reasoning over graph paths 🧮 Tech Stack RDF, OWL, Neo4j, Blazegraph, SPARQL LangChain, LlamaIndex, HuggingFace Transformers, Neo4j/NetworkX for graph, vector DBs 🧪 Inference Deterministic (SPARQL, rules, logic) or probabilistic (PGMs) LLM-based generation informed by graph-aware retrieval 🔗 Integration with LLMs Optional; LLMs can query or summarize KG Essential; LLMs decode retrieved graph info into answers 📘 Example in Healthcare \u0026ldquo;What drugs interact with Warfarin?\u0026rdquo; → Uses drug-drug interaction KG \u0026ldquo;What treatments are best for elderly diabetic patients with hypertension?\u0026rdquo; → Uses patient-condition-treatment graph to guide LLM 📈 Scalability Can grow large but needs curation and consistency Scalable via modular retrievers; dynamic context injection 📣 Explainability High: paths are interpretable Medium: explainable only if LLM is instructed to reason with trace 📚 Data Format Triples: (subject, predicate, object) Graph-augmented documents, vector embeddings, node-context pairs 🎯 Strengths Precision, transparency, semantic integrity Flexibility, context-aware QA, natural language synthesis 🧱 Weaknesses Hard to build/maintain at scale, brittle for unstructured text Less structured, can hallucinate, graph reasoning quality depends on retriever design 🩺 Healthcare Use Cases # Use Case KG GraphRAG 1. Drug Interaction Checks KG connects drugs via known interaction relationships from structured databases (e.g., RxNorm, DrugBank).\n🔹 \u0026ldquo;Does Warfarin interact with NSAIDs?\u0026rdquo; → Traverse KG paths. GraphRAG retrieves documents discussing drug interactions, side effects, or contraindications and summarizes them.\n🔹 \u0026ldquo;What should patients taking Warfarin avoid?\u0026rdquo; 2. Clinical Decision Support Encodes clinical guidelines as rules and semantic paths (e.g., \u0026ldquo;If diabetic AND hypertensive THEN consider ACE inhibitors\u0026rdquo;). Retrieves relevant chunks of guidelines and case studies, then LLM synthesizes a tailored answer.\n🔹 \u0026ldquo;Best treatment plans for elderly diabetic patients with kidney disease?\u0026rdquo; 3. Patient Phenotyping Uses ontologies (e.g., SNOMED CT, HPO) to infer phenotypes based on coded EHR data.\n🔹 Identify \u0026ldquo;Type 2 Diabetes\u0026rdquo; from a network of symptoms and lab values. Retrieves semantically similar patient trajectories or phenotypes, helping answer: 🔹 \u0026ldquo;How was this phenotype managed in similar patients?\u0026rdquo; 4. Rare Disease Diagnosis Graph-based inference across symptoms, genes, and conditions to suggest candidate diseases. Combines graph paths with medical literature to support LLM-based diagnostics and explanations.\n🔹 \u0026ldquo;What rare diseases match these symptoms?\u0026rdquo; 5. Biomedical Research Discovery Connects genes, diseases, pathways, and drugs to suggest new hypotheses.\n🔹 \u0026ldquo;Which genes are linked to both Parkinson’s and depression?\u0026rdquo; Retrieves multi-hop literature paths and generates natural language hypotheses.\n🔹 \u0026ldquo;What is the link between gut microbiome and Alzheimer’s?\u0026rdquo; 6. Clinical Trial Matching Links patient features to trial eligibility criteria through structured relationships. Matches unstructured patient notes with trials via hybrid graph + text retrieval.\n🔹 \u0026ldquo;Which clinical trials is this patient eligible for?\u0026rdquo; 7. Medical Education / Q\u0026amp;A Students query a structured KG to explore medical knowledge interactively. Natural language Q\u0026amp;A system over combined textbook + graph data.\n🔹 \u0026ldquo;Explain why beta-blockers are contraindicated in asthma patients.\u0026rdquo; "},{"id":58,"href":"/ai-workflows/alignment-reasoning/graph-reasoning/graphrag/","title":"GraphRAG","section":"Graph Reasoning","content":"Content for the GraphRAG section.\n"},{"id":59,"href":"/healthcare/domain_knowledge/hands-on-healthcare-data/","title":"Hands-On Healthcare Data","section":"Domain","content":" Based on \u0026ldquo;Hands-On Healthcare Data: Taming the Complexity of Real-World Data\u0026rdquo;\nCh1 Ch2 Ch3 Ch4 Ch5 Ch6. Machine Learning \u0026amp; Graph-Based Analytics in Healthcare Ch7 "},{"id":60,"href":"/healthcare/data/healthcare_data/","title":"Healthcare Data Layers","section":"Data","content":" Healthcare Data Layers # 1️⃣ Data Sources (Raw Data \u0026amp; Collection Level) # These are the foundational data sources used in healthcare analysis, originating from clinical trials, hospitals, insurance claims, and patient records.\nClinical Data (RCTs, EHR, OMOP, CDM) – Structured, controlled, and often randomized data used for regulatory and research applications. Real-World Data (RWD: EHR, Claims, Registries) – Observational and confounded, requiring advanced causal inference methods to extract meaningful insights. Relationship: Clinical Data is typically highly structured and standardized, whereas RWD is heterogeneous, requiring bias correction. 2️⃣ Data Management \u0026amp; Standardization (Processing \u0026amp; Infrastructure Level) # This layer ensures that raw clinical \u0026amp; real-world data are cleaned, structured, and made interoperable for analysis.\nHealthcare Informatics – The framework for data integration, ETL processes, standardization (OMOP, FHIR, CDMs), interoperability, and terminology mapping (SNOMED, LOINC, ICD). Healthcare Informatics acts as a bridge between data collection (clinical \u0026amp; RWD) and analytics. Without informatics, AI models and statistical analyses would lack clean, structured, and standardized data. 3️⃣ Data Analytics \u0026amp; Decision Intelligence (AI \u0026amp; Statistical Analysis Level) # This layer applies statistical, machine learning (ML), and deep learning (DL) models to structured and unstructured healthcare data for actionable insights.\nTraditional Data Science \u0026amp; Statistical Analysis (Used for both Clinical \u0026amp; RWD)\nBiostatistics, Bayesian Methods, Survival Analysis, Causal Inference (PSM, DAGs, DiD) Used to control bias, estimate treatment effects, and generate regulatory-grade evidence (RWE). AI in Healthcare (Machine Learning \u0026amp; Deep Learning Applications)\nSupervised Learning (Logistic Regression, Decision Trees, Random Forests) Deep Learning (CNNs, Transformers, NLP, Reinforcement Learning) Model Interpretability (SHAP, LIME) and AI Fairness (Bias Mitigation) Relationship:\nAI \u0026amp; ML rely on structured, clean data (from Healthcare Informatics) and leverage Clinical Data \u0026amp; RWD to generate predictions and automate decision-making. Statistical analysis methods (causal inference, survival analysis) are critical for ensuring valid results before AI is applied. "},{"id":61,"href":"/healthcare/data/healthcare_sources/","title":"Healthcare Data Sources","section":"Data","content":" Healthcare Data Sources # Phenotype KnowledgeBase (PheKB) # Description:\nA collaborative portal for sharing and validating electronic phenotype definitions used in observational health research.\nTags: phenotyping, EHR, cohort definitions\nUse Cases:\nStandardized phenotype definitions for conditions like diabetes, asthma, etc. Sharing phenotype algorithms across institutions MIMIC-IV (Medical Information Mart for Intensive Care) # Description:\nA large, publicly available critical care database containing de-identified health data from ICU patients at the Beth Israel Deaconess Medical Center.\nTags: ICU, de-identified data, clinical research\nUse Cases:\nPredictive modeling in critical care Benchmarking clinical algorithms Training deep learning models Access Requirements:\nRequires credentialed training and data use agreement via PhysioNet\nOHDSI / OMOP Common Data Model # Description:\nAn open community initiative and standard model for organizing observational health data across institutions and studies.\nTags: standardization, EHR, interoperability, CDM\nUse Cases:\nConverting disparate data sources into a consistent format Enabling federated analysis across healthcare systems Supporting tools like ATLAS for cohort building National COVID Cohort Collaborative (N3C) # Description:\nA centralized, secure platform for analyzing harmonized COVID-19 clinical data from dozens of healthcare providers across the US.\nTags: COVID-19, federated research, clinical data\nUse Cases:\nStudying disease trajectories and treatment effects Multisite analytics using harmonized EHR data Evaluating outcomes for long COVID Access Requirements:\nApplication and institutional affiliation required\nBioPortal # Description:\nA comprehensive repository of biomedical ontologies from the National Center for Biomedical Ontology.\nTags: ontologies, terminology, semantic web, linked data\nUse Cases:\nAccessing ontologies like SNOMED CT, ICD, LOINC, RxNorm Mapping data to standard vocabularies Enabling semantic interoperability Unified Medical Language System (UMLS) # Description:\nIntegrates over 200 biomedical vocabularies to support natural language processing, terminology mapping, and EHR data harmonization.\nTags: NLP, standard vocabularies, concept mapping\nUse Cases:\nLinking clinical terms to standard codes Enhancing search and retrieval in clinical systems Supporting NLP tools like MetaMap and cTAKES Access Requirements:\nFree license from NLM, requires annual agreement\nAphrodite # Description:\nAn R package developed by OHDSI that supports semi-supervised phenotype algorithm development using feature engineering and machine learning methods on OMOP Common Data Model (CDM) datasets.\nTags: phenotyping, machine learning, semi-supervised, OMOP, OHDSI\nUse Cases:\nRapid development of phenotype classifiers using imperfectly labeled data. Applying machine learning models to predict phenotypes based on structured EHR data. Feature extraction from OMOP CDM to support supervised or semi-supervised learning tasks. "},{"id":62,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/healthcare_use_cases_for_non_textual_unstructured_data/","title":"Healthcare Use Cases for Non-textual Unstructured Data","section":"C3 ML Healthcare","content":" Healthcare Use Cases for Non-textual Unstructured Data # 1. Are foundation models limited to text? # No. Although many early applications focus on text, foundation models are inherently multimodal. They can process and learn from images, audio, video, and other forms of unstructured data without modifying the core model.\n2. What kind of unstructured data exists in healthcare beyond text? # Medical images: X-rays, CT, MRI Audio data: Patient speech, clinical voice notes Video data: Endoscopy, movement assessments Digital pathology and genomic data: High-resolution slides and sequence strings 3. How do image-text foundation models work in healthcare? # They learn from paired image and text data (e.g., medical images and associated reports) to understand content holistically. This allows for better diagnostic performance and contextual understanding than single-modality models.\n4. What is the potential impact of image-text models on radiology? # Radiology is AI-friendly due to digital formats and standards. Current AI tools support quantification, detection, classification, etc. Foundation models go beyond—integrating prior exams, clinical context, and treatment recommendations. 5. Why is the current model development approach inefficient? # Today’s approach builds narrow, task-specific models requiring large labeled datasets. Foundation models reduce this burden via few-shot learning, general knowledge transfer, and multimodal reasoning.\n6. What advantages do foundation models bring to medical imaging? # Combine image + text for deeper insight Reduce false positives/negatives via context Extract insights beyond human interpretation Generate full radiology reports or treatment suggestions Accelerate model development with less labeled data 7. How are foundation models used in image-based diagnosis? # They can:\nUnderstand body composition via imaging biomarkers Detect osteoporosis, aortic calcium, visceral fat, etc. Predict adverse events from routine imaging 8. Can foundation models incorporate genomic or pathology data? # Yes. They can process complex biomedical data:\nDigital pathology slides Genomic sequences in FASTA format Gene expression and mutation patterns This enables discovery of clinically meaningful patterns across modalities.\n9. What is the value of multimodal foundation models? # Integrate text, imaging, genomics, clinical history Offer personalized care and richer diagnostics Support communication with patients in any language or literacy level 10. How do foundation models help in image model development? # They support:\nPreprocessing Data augmentation Synthetic data generation This accelerates model iteration and reduces time to deployment.\n11. Can voice-text data be used in healthcare applications? # Yes. Foundation models can:\nTranscribe speech Analyze speech patterns for diagnosis (e.g., Parkinson’s) Enable voice prosthetics for patients who’ve lost speech Support virtual medical assistants and mental health chatbots 12. What are some patient-facing applications of voice-text models? # Mental health bots using speech/text input Virtual assistants for disabled individuals Real-time transcription and communication aids 13. What cognitive shift do foundation models support? # They help users move from:\nComprehension-based reasoning → using known knowledge Fluid reasoning → solving unfamiliar problems with abstracted understanding Foundation models act as co-pilots, enhancing human decision-making.\n14. Do foundation models replace human decision-making? # No. They augment, not replace. Final decisions still rest with trained professionals who interpret the model’s output with judgment and context.\n15. What’s the catch with all this power? # Even with their vast capabilities, foundation models are subject to the no free lunch theorem. They have trade-offs, biases, and limitations—topics discussed in follow-up modules.\n"},{"id":63,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/healthcare_use_cases_for_text_data/","title":"Healthcare Use Cases for Text Data","section":"C3 ML Healthcare","content":" Healthcare Use Cases for Text Data # 1. Can large language models like ChatGPT perform at a physician level? # Yes. ChatGPT has demonstrated performance comparable to expert physicians on tasks like the USMLE medical board exam. This raises important questions about the evolving role of human expertise in healthcare as LLMs continue to advance.\n2. Should LLMs be integrated into medical training or exams? # Possibly. LLMs could enhance the medical licensing exam process by reflecting real-world clinical scenarios. However, it\u0026rsquo;s essential for healthcare professionals to understand their benefits and limitations before full integration.\n3. What are the risks of integrating LLMs into clinical systems? # While useful for tasks like recipe generation or patient education, LLMs can fail unexpectedly, hallucinate references, or output incorrect information. Human oversight and validation remain critical.\n4. What are practical healthcare use cases for LLMs today? # Clerical task automation: Scheduling, patient communication, and triaging. Inbox management: Reducing message overload and provider burnout. Collaborative assistant: Recommending actions based on patient history. Low-code innovation: Empowering clinicians to build apps/tools. 5. How do LLMs support medical data processing? # LLMs streamline key NLP tasks:\nTokenization: Segmenting clinical notes into analyzable units. Named Entity Recognition (NER): Identifying drugs, diseases, etc. Negation Detection: Understanding sentiment/context (e.g., \u0026ldquo;no cancer\u0026rdquo;). Relation Extraction: Mapping relationships between entities. De-identification: Masking PHI for privacy compliance. 6. Why are foundation models better than traditional NLP for medical text? # They handle variation across institutions, formats, and languages with few-shot/zero-shot learning, reducing the need for custom engineering and enabling broader generalization.\n7. Can LLMs handle complex clinical queries without structured data? # Yes. Prompts like:\n“Find all named entities related to diabetes management” “De-identify this record per HIPAA” “How has cancer progressed after Keytruda treatment?” show how LLMs can perform analytics directly from unstructured text. 8. Can LLMs be further trained on clinical data? # Yes. Training LLMs on patient records, trials, and guidelines can increase domain-specific accuracy. Applications include:\nClinical Decision Support Drug Interaction Warnings Guideline Recommendations 9. How can LLMs help with clinical trial recruitment? # LLMs can evaluate eligibility based on:\nPatient history Medications Lab results They can also explain trials directly to patients, improving enrollment.\n10. What role can LLMs play in patient communication? # They can:\nAnswer health-related questions Translate jargon into plain language Provide reminders and follow-ups Offer multilingual, conversational support 11. How can LLMs assist with billing and coding? # With medical terminology knowledge, LLMs can:\nAssign billing codes Improve record-keeping Reduce administrative burden 12. Can LLMs support public health efforts? # Yes. They can monitor:\nOutbreak detection using EHRs, social media Pattern recognition across data sources This enables faster responses to public health threats. 13. Can LLMs process and learn from genomic data? # Yes. Genomic data (e.g., FASTA format) is text-based. LLMs can:\nIdentify mutations linked to diseases Predict disease risk Integrate with clinical and lifestyle data 14. What is the benefit of multimodal analysis in genomics? # LLMs can combine:\nGenomic sequences EHRs Environmental/lifestyle data This integration enables personalized care and discovery of complex health patterns.\n15. Can LLMs support pharmacogenomics? # Yes. They can identify:\nDrug responses Adverse reactions Genetic factors impacting efficacy This paves the way for precision medicine.\n16. How do LLMs improve drug discovery? # Applications include:\nVirtual screening: Identify promising molecules Lead optimization: Improve safety and effectiveness Toxicity prediction: Flag unsafe compounds early Mechanism of action prediction: Understand how a drug works 17. What is the long-term outlook for LLMs in healthcare? # The future is expansive:\nFrom analytics and operations to clinical care and research Support for providers, patients, researchers Accelerating breakthroughs in drug development and personalized medicine LLMs are set to revolutionize the healthcare landscape, and we are only scratching the surface.\n"},{"id":64,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/how_foundation_models_work_in_healthcare_applications/","title":"How Foundation Models Work","section":"C3 ML Healthcare","content":" How Foundation Models Work (in Healthcare Applications) # 1. What are foundation models and why are large language models (LLMs) a key example? # Foundation models are large-scale machine learning models trained on vast and diverse datasets, enabling them to perform well across multiple tasks. Large Language Models (LLMs) like GPT are a major class of these, powering tools like ChatGPT and being early, widely successful examples.\n2. What architecture do foundation models, especially LLMs, rely on? # They rely heavily on the Transformer architecture, introduced in the paper “Attention Is All You Need”. This architecture uses self-attention mechanisms to allow models to learn relationships across sequences of data, making them highly effective for tasks involving language and other sequential data.\n3. How does the attention mechanism work in transformers? # The attention mechanism enables the model to focus on different parts of the input when making predictions. Self-attention lets each element of the input sequence attend to all others, allowing the model to integrate contextual information holistically.\n4. How are transformers structured in large language models? # Transformers in LLMs are deep neural networks built from stacks of layers combining attention with traditional neural layers. They process input as sequences—ideal for language data—and use tokenization to convert text into manageable units (words, subwords, etc.).\n5. Can tokenization be applied to other data types? # Yes. Tokenization can be generalized to image patches, audio frames, or structured data. This generalization enables foundation models to learn from and operate across multiple modalities.\n6. What are transformer encoders and how are they used in models like BERT? # A transformer encoder transforms input sequences into dense vector representations. BERT, for example, is pretrained using self-supervised learning by masking words and predicting them, allowing it to learn language representations without labeled data.\n7. What about transformer decoders and models like GPT? # Transformer decoders, unlike encoders, generate sequences from input vectors or initial prompts. GPT (Generative Pre-trained Transformer) models are based on decoder-only architectures and are trained to predict the next token in a sequence—a task aligned with language generation.\n8. How do models like GPT-3 perform impressive tasks with little supervision? # Thanks to transfer learning, GPT models demonstrate zero-shot and few-shot learning, performing tasks they weren\u0026rsquo;t explicitly trained for by using prompts and examples within the input.\n9. How does GPT handle complex tasks like medical Q\u0026amp;A or clinical reasoning? # LLMs can respond to medical questions, explain reasoning for clinical answers, and even alter case scenarios to change correct answers. This shows their potential for medical education and diagnostic simulation.\n10. What are the limitations of LLMs trained like GPT-3? # Since they’re trained on general web and literature data, LLMs may generate plausible but incorrect outputs, due to misalignment with human values or domain knowledge. Their training objective (next-word prediction) does not inherently align with truth or utility.\n11. How does ChatGPT improve upon this with human feedback? # ChatGPT uses Reinforcement Learning from Human Feedback (RLHF):\nStep 1: Supervised Fine-Tuning (SFT) with human-written responses. Step 2: Reward Model trained from human-ranked outputs. Step 3: Reinforcement learning (e.g., PPO) optimizes the model to favor preferred responses. 12. What is a “prompt” and why does it matter? # A prompt is the input that guides the model\u0026rsquo;s output. Its structure and content influence results dramatically, making prompt engineering a critical skill for getting reliable, targeted responses from foundation models.\n13. What are some common prompt engineering styles? # Instruction prompt: Direct task requests (e.g., “How to manage diabetes?”) Role-based prompt: Assigning the model a persona (e.g., “You are a nurse…”) Few-shot prompt: Providing examples before the actual question Chain-of-thought (CoT): Encouraging the model to reason step-by-step Zero-shot CoT: Adding phrases like “Let’s think step-by-step” to guide reasoning Self-consistency prompting: Generating multiple answers and choosing the majority Generative Knowledge prompting: First generating facts, then reasoning over them 14. Why is chain-of-thought prompting effective, and what are its limits? # CoT prompting helps elicit reasoned answers, particularly in clinical contexts. However, it’s most effective in larger models (100B+ parameters) and less so in smaller ones.\n15. Do foundation models apply beyond language? # Yes. The transformer architecture extends to:\nImages: via patch tokenization (e.g., Vision Transformers) Audio/Speech: like OpenAI’s Whisper for transcription Multimodal data: like DALL·E 2 for text-to-image generation 16. How does a multimodal model like DALL·E 2 work? # DALL·E 2:\nEncodes a text prompt into a vector. Maps it to visual feature space. Uses a diffusion model decoder to generate images from that representation. 17. What are diffusion models and how do they help? # Diffusion models learn to denoise images progressively, allowing them to generate high-quality, realistic outputs. They’re widely used in modern generative vision models like DALL·E 2.\n18. What\u0026rsquo;s the future outlook for foundation models in healthcare? # Foundation models, particularly LLMs and multimodal transformers, are rapidly evolving. With human feedback, prompt engineering, and domain-specific fine-tuning, they offer immense potential for clinical decision support, medical education, and personalized care.\n"},{"id":65,"href":"/ipark/","title":"Inhee Park, PhD - Resume","section":"","content":" "},{"id":66,"href":"/ai-workflows/alignment-reasoning/graph-reasoning/knowledge-graphs/","title":"Knowledge Graphs","section":"Graph Reasoning","content":"Content for the Knowledge Graphs section.\n"},{"id":67,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/missing_values/","title":"Missing Data Scenarios in Healthcare Modeling","section":"C2 Clinical Data","content":" Missing Data Scenarios in Healthcare Modeling # 1. Should Be Measured But Wasn’t # Description: The value is expected but is missing due to random or procedural issues (e.g., lab error, missed test). Technical Term: MCAR: Missing Completely At Random MAR: Missing At Random Example: A routine blood test wasn\u0026rsquo;t recorded because the sample was lost. Strategy: Impute (mean, median, or model-based). Add a missingness indicator variable (e.g., var_missing = 1). Rationale: The missingness is unrelated to the value itself, so estimation is relatively safe. 2. Mostly Zero Due to Rare Occurrence # Description: Not truly missing — the value is zero or absent for most patients because the condition/event is rare. Technical Term: Not Missing (No abbreviation needed) Example: HIV diagnosis column is 0 for most patients. Strategy: Do not impute — the 0s are meaningful and reflect true absence. Rationale: These are real values, and zeros carry clinical meaning. 3. Deliberately Not Recorded # Description: Clinician or system chooses not to record a value based on context (e.g., patient clearly stable or too ill). Technical Term: MNAR: Missing Not At Random Example: Sodium level not tested because the patient was clearly stable. Strategy: Avoid imputation if possible — it may introduce bias. Use models that handle missingness natively (e.g., decision trees, XGBoost, LightGBM). Consider adding a missingness indicator. Rationale: The missingness depends on the unobserved value and may carry predictive signal. Summary Table # Case Description Abbreviation Impute? Extra Notes 1 Should be measured but wasn’t MCAR / MAR ✅ Yes Add indicator if signal is likely 2 Mostly zero (rare condition) Not Missing 🚫 No Keep as is — zeros are informative 3 Deliberately not recorded MNAR ⚠️ Caution Use native handling + possible indicator "},{"id":68,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/omop_vs_rlhf_comparison/","title":"OMOP vs. RLHF","section":"C2 Clinical Data","content":" OMOP vs. RLHF: A Side-by-Side Comparison # This document compares OMOP (Observational Medical Outcomes Partnership) in healthcare with RLHF (Reinforcement Learning from Human Feedback) in generative AI, focusing on their structures, purposes, and alignment with Learning Health System (LHS) principles.\n🔍 Summary Table # Aspect OMOP (Healthcare) RLHF (GenAI) Domain Clinical/healthcare data Natural language modeling Purpose Standardize and structure real-world patient data for learning, analytics, and AI Align AI model behavior with human preferences and values Core Process ETL (Extract-Transform-Load) clinical data into a common format for analysis Fine-tune a pretrained LLM using human-labeled preferences or rewards Data Source EHRs, claims, labs, devices Human judgments on AI-generated outputs Feedback Type Structured medical events (diagnoses, drugs, labs, etc.) Human preference signals on outputs (better/worse answers) Learning Method Enables observational \u0026amp; causal learning from patient data Reinforcement learning from ranked or scored examples Governance Layer Ethics via IRB, consent, privacy laws Ethics via safety research, alignment goals, red-teaming Use in Feedback Loops LHS uses OMOP to “learn from care to improve care” RLHF uses feedback to “teach the model to behave better” 🔁 Conceptual Analogy # OMOP + Learning Health System (LHS) is to the health system\nas\nRLHF is to a generative AI model.\nIn both cases:\nData flows through a system Human-derived feedback loops guide improvement The system continuously adapts and aligns with user or patient needs 🧠 Key Takeaways # Both OMOP and RLHF are feedback-driven learning architectures grounded in human data. OMOP is part of an ecosystem (LHS) that feeds learning back into medical care. RLHF aligns generative models with human preferences through iterative fine-tuning. Each reflects a shift toward real-time, adaptive, ethically grounded learning. Would you like to extend this comparison with diagrams, code examples, or regulatory implications?\n"},{"id":69,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/oap_framework_healthcare/","title":"Output-Action Pairing (OAP) Framework in Healthcare","section":"C3 ML Healthcare","content":" 🧠 Output-Action Pairing (OAP) Framework in Healthcare # This guide provides real-world examples of the Output-Action Pairing (OAP) framework: aligning machine learning model outputs with concrete clinical actions to improve care.\n📋 OAP Template # Output (Prediction) Action Taken Who Acts Why It Helps What the model predicts The clinical step or decision triggered The role/team responsible How it improves outcomes or safety ✅ Real-World Examples # 1. Sepsis Prediction # Output: High risk of sepsis in next 6 hours Action: Alert care team, initiate fluids/labs/antibiotics Who acts: Rapid response team (nurses + physicians) Why it helps: Early treatment improves survival 2. Readmission Risk Score # Output: 30% chance of readmission within 30 days Action: Extra discharge planning, follow-up calls, medication check Who acts: Care coordinator + pharmacist Why it helps: Reduces avoidable readmissions 3. Pneumothorax Detection on Chest X-ray # Output: Pneumothorax detected Action: Immediate flag to radiologist and ER for review Who acts: Radiologist + ER team Why it helps: Enables life-saving chest tube intervention 4. COVID-19 Triage # Output: High risk of severe COVID progression Action: ICU evaluation, enhanced monitoring, begin treatment Who acts: Hospitalist or ICU triage physician Why it helps: Allocates ICU resources effectively 5. Fall Risk in Hospital # Output: High fall risk during admission Action: Enable fall precautions (alarms, sitter, etc.) Who acts: Nursing team Why it helps: Prevents injury and hospital complications 6. Stroke Detection via CT # Output: Acute stroke suspected on scan Action: Notify neurologist, activate stroke protocol (tPA window) Who acts: Radiologist + Stroke Response Team Why it helps: Reduces time to brain-saving treatment 🔄 Summary # The OAP framework ensures that ML predictions translate to action, improving clinical relevance and patient safety. Every model in healthcare should answer:\nWhat is the output? What is the action? Who will act on it? How does it help the patient? "},{"id":70,"href":"/ai-workflows/alignment-reasoning/rlhf/comparison_ppo/","title":"PPO in LLMs vs PPO in Walker2D","section":"RLHF","content":" 🤖🦿 Understanding PPO: From Language Generation to Robot Control — Code, Concepts, and Comparisons # Proximal Policy Optimization (PPO) in both large language models (LLMs, e.g., GPT-style) and classical control environments (e.g., Walker2D), focusing on the structure of the PPO update and how actions are selected during inference.\n1. 🧾 PPO step() Call — Argument-by-Argument Breakdown # ppo_trainer.step( queries=[input_ids[0]], # Prompt (tokenized) — represents the current state responses=[response_ids[0]], # Generated tokens — represents the action taken rewards=[reward] # Scalar from reward model — score for that action ) Mapping to Classic RL (Walker2D) # PPO Argument 🤖 LLM (RLHF) 🦿 Walker2D (Classic RL) queries = [input_ids[0]] Prompt as input (discrete tokenized state) Robot\u0026rsquo;s continuous state (joint angles, velocities) responses = [response_ids[0]] Generated tokens (sequence of actions) Applied joint torques (vector of real numbers) rewards = [reward] Reward model output (alignment score) Environment reward (e.g., distance walked) 2. 🎯 Action Selection in PPO # How does the agent choose its next action, given a state/prompt?\n🤖 LLMs (Text Generation) # # Given a prompt (state) input_ids = tokenizer(\u0026#34;What causes rain?\u0026#34;, return_tensors=\u0026#34;pt\u0026#34;).input_ids # Model outputs token logits for next action outputs = model(input_ids=input_ids) logits = outputs.logits[:, -1, :] probs = torch.softmax(logits, dim=-1) # Sample the next token (action) from distribution next_token = torch.multinomial(probs, num_samples=1) # Repeat to generate full response 🦿 Walker2D (Physical Control) # # Get current robot state state = get_env_state() # vector like [θ1, θ2, v1, v2...] # Policy network outputs action distribution parameters mean, std = policy_net(state) # Sample a continuous action (e.g., torque values) action = torch.normal(mean, std) # Apply action to environment next_state, reward, done, info = env.step(action) 🔁 Comparison of Action Logic # Component 🤖 LLM (RLHF) 🦿 Walker2D (Classic RL) State Prompt text Robot’s physical state Action Next token (discrete) Joint torques (continuous) Policy Output Token logits (softmaxed) Mean \u0026amp; std dev of Gaussian per action dim Sampling Method Multinomial over vocab Sample from Gaussian Result Extend response with chosen token Step to new physical state 🔁 PPO Mapping in LLMs (RLHF) vs Classical RL # Category PPO in LLMs (RLHF) 🦿 PPO in Walker2D (Classic RL) Agent Language Model (e.g., GPT-2, o1) Control Policy Network Environment Static or semi-static prompt context Physics-based simulator (e.g., MuJoCo) State Prompt (or full token context so far) Robot’s current physical state (joint angles, velocity, etc.) Action Next token in the sequence (discrete, vocabulary-sized) Torque values for each joint (continuous, multi-dimensional) Trajectory Sequence of tokens (prompt + response) Sequence of joint states and actions over time Reward Signal Given after full response (from reward model trained via human preferences) Immediate reward at each time step (distance walked, balance maintained, etc.) Reward Nature Sparse, episodic, scalar (usually one reward per episode) Dense, frequent, multi-dimensional (continuous feedback per step) Goal Generate text aligned with human values/preferences Learn movement to walk forward efficiently without falling Policy Network Transformer LM (large, ~billions of params) Feedforward or RNN-based controller (small, e.g., MLP) Reference Model Frozen copy of base LM (used for KL-penalty regularization) Usually none (KL not common in Walker2D PPO) Training Stability Needs KL penalty to prevent mode collapse / nonsense generations PPO alone is usually enough due to continuous feedback Evaluation Human evals, reward model scores (e.g., helpfulness, safety) Distance walked, steps survived, control energy used 🗣️ “Say the right thing, the way a human likes” 🦿 “Move the right way, so you don’t fall” Actions are words, optimizing a sequence to match human preference Actions are forces, optimizing to physically walk and stay balanced Reference https://rlhfbook.com/ https://gymnasium.farama.org/ "},{"id":71,"href":"/ai-workflows/genai-systems/prompt_engineering/","title":"Prompt Engineering","section":"GenAI Systems","content":" Prompt Engineering Library # Step-by-Step CoT Q\u0026amp;A Guide with Generative Agents "},{"id":72,"href":"/ai-workflows/alignment-reasoning/rlhf/","title":"RLHF","section":"Alignment \u0026 Reasoning","content":" PPO in LLMs vs PPO in Walker2D 🚀 Why RLHF? # Reinforcement Learning from Human Feedback (RLHF) is a foundational technique for aligning large language models (LLMs) with human preferences. Rather than optimizing for likelihood of next-token prediction alone, RLHF adds a human-in-the-loop feedback mechanism to fine-tune model behavior.\nModern AI systems — especially LLMs — are capable of generating coherent text, code, and dialogue. However, raw model outputs often:\nLack safety or factuality Misalign with user intent Fail to follow task constraints RLHF helps solve this by using human preferences to shape model behavior in alignment with real-world goals.\n🧠 Core Components # RLHF typically unfolds in three stages:\nSupervised Fine-Tuning (SFT) A pretrained LLM is fine-tuned on curated high-quality prompts and responses. Reward Modeling A separate model is trained to predict human preference scores (better vs worse answers). Reinforcement Learning (PPO) The main model is optimized using a reward signal from the reward model, via Proximal Policy Optimization. Each of these stages ensures the model is not just fluent — but aligned.\n🧩 Topics Covered Here # ✍️ Human Preference Collection Paired responses, rating scales, feedback annotation 🧱 Reward Modeling Architecture, loss functions, dataset design ♻️ Reinforcement Learning Fine-Tuning PPO algorithm, TRL (HuggingFace), hyperparameter tuning 🧪 Evaluating RLHF Models Alignment metrics, human evals, reward hacking prevention 🔁 Comparison with Other Alignment Methods DPO, Constitutional AI, Self-Instruct 🧭 Why RLHF Belongs in Alignment-Reasoning # RLHF is not just a training trick — it\u0026rsquo;s a method for structuring model behavior using human feedback as the reward signal. It represents a bridge between:\nOptimization (via reinforcement learning) Intent modeling (via human preference) Structural alignment (via value feedback loops) That’s why RLHF sits squarely within the broader alignment-reasoning framework of this knowledge base.\n"},{"id":73,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/diabetes_phenotype_pipeline/","title":"Rule-Based Electronic Phenotyping Example: Type 2 Diabetes","section":"C2 Clinical Data","content":" Rule-Based Electronic Phenotyping Example: Type 2 Diabetes # This notebook walks through the process of defining an electronic phenotype using a rule-based approach, with a focus on Type 2 Diabetes. The pipeline includes concept mapping, multi-patient evaluation, and phenotype logic visualization.\n🔹 Step 1: Simulated Vocabulary Lookup (UMLS / OMOP) # We define the clinical concept (Type 2 Diabetes) using relevant ICD-10 and RxNorm codes.\n# Simulated UMLS/OMOP vocab mapping UMLS_LOOKUP = { \u0026#34;type2_diabetes\u0026#34;: { \u0026#34;icd10\u0026#34;: {\u0026#34;E11.9\u0026#34;, \u0026#34;E11.65\u0026#34;, \u0026#34;E11.00\u0026#34;}, \u0026#34;rxnorm\u0026#34;: {\u0026#34;metformin\u0026#34;, \u0026#34;insulin\u0026#34;}, } } 🔹 Step 2: Multi-Patient Phenotyping Logic # Each patient is checked for:\nPresence of ≥2 relevant ICD-10 codes Any matching diabetes-related medication (RxNorm) Start date of phenotype based on first matching code from typing import List, Dict, Set from datetime import datetime def has_required_codes(patient, valid_codes: Set[str], source_field: str, min_count=1) -\u0026gt; bool: return len([code for code in patient[source_field] if code in valid_codes]) \u0026gt;= min_count def find_start_date(patient, valid_codes: Set[str], code_dates: Dict[str, List[str]]) -\u0026gt; str: dates = [] for code in valid_codes: if code in code_dates: dates.extend(code_dates[code]) return min(dates) if dates else None def apply_phenotype_definition(patient, concept_map) -\u0026gt; Dict: icd_match = has_required_codes(patient, concept_map[\u0026#34;icd10\u0026#34;], \u0026#34;icd10_codes\u0026#34;, min_count=2) med_match = has_required_codes(patient, concept_map[\u0026#34;rxnorm\u0026#34;], \u0026#34;medications\u0026#34;) start_date = find_start_date(patient, concept_map[\u0026#34;icd10\u0026#34;] | concept_map[\u0026#34;rxnorm\u0026#34;], patient[\u0026#34;code_dates\u0026#34;]) return { \u0026#34;phenotype_positive\u0026#34;: icd_match and med_match, \u0026#34;has_diabetes_codes\u0026#34;: icd_match, \u0026#34;has_diabetes_med\u0026#34;: med_match, \u0026#34;start_date\u0026#34;: start_date } # Sample patient data (multiple patients) patients = [ { \u0026#34;id\u0026#34;: \u0026#34;P001\u0026#34;, \u0026#34;icd10_codes\u0026#34;: [\u0026#34;E11.9\u0026#34;, \u0026#34;E11.9\u0026#34;], \u0026#34;medications\u0026#34;: [\u0026#34;metformin\u0026#34;], \u0026#34;code_dates\u0026#34;: { \u0026#34;E11.9\u0026#34;: [\u0026#34;2023-01-01\u0026#34;, \u0026#34;2023-03-01\u0026#34;], \u0026#34;metformin\u0026#34;: [\u0026#34;2023-01-05\u0026#34;] } }, { \u0026#34;id\u0026#34;: \u0026#34;P002\u0026#34;, \u0026#34;icd10_codes\u0026#34;: [\u0026#34;I10\u0026#34;], # Hypertension only \u0026#34;medications\u0026#34;: [\u0026#34;lisinopril\u0026#34;], \u0026#34;code_dates\u0026#34;: { \u0026#34;I10\u0026#34;: [\u0026#34;2023-04-01\u0026#34;], \u0026#34;lisinopril\u0026#34;: [\u0026#34;2023-04-05\u0026#34;] } } ] # Apply phenotype to all results = {} for patient in patients: result = apply_phenotype_definition(patient, UMLS_LOOKUP[\u0026#34;type2_diabetes\u0026#34;]) results[patient[\u0026#34;id\u0026#34;]] = result # Show results for pid, res in results.items(): print(f\u0026#34;{pid}: {res}\u0026#34;) 🔹 Step 3: Phenotype Logic Flowchart # Below is a visual flowchart that shows the phenotype logic step-by-step.\nAlt text ✅ Summary # This markdown covers:\nRule-based phenotyping using ICD and RxNorm codes Handling multiple patients Simulated code-date structure Logical combination of conditions (AND logic) A visual diagram of the rule logic This framework can be expanded to:\nInclude real UMLS/OMOP lookups via API Support more complex logic (time gaps, lab thresholds) Incorporate chart-reviewed gold standards "},{"id":74,"href":"/ai-workflows/genai-systems/prompt_engineering/step_by_step_cot_qna_guide_generative_agents/","title":"Step-by-Step CoT Q\u0026A Guide with Generative Agents","section":"Prompt Engineering","content":" Step-by-Step Chain-of-Thought Q\u0026amp;A Learning Guide # Why Q\u0026amp;A Format is Efficient for Learning # The Q\u0026amp;A format is highly effective because it mirrors how our brain naturally processes and retrieves information.\n1. Active Recall \u0026amp; Retrieval Practice # Encourages active memory engagement. Strengthens long-term retention better than passive review. Makes it easier to spot gaps in understanding. 2. Chunking \u0026amp; Cognitive Load Management # Each Q\u0026amp;A is a small, focused unit of knowledge. Helps learners process complex topics one bite at a time. 3. Mimics Real-Life Problem Solving # Trains you to think in terms of \u0026ldquo;why\u0026rdquo; and \u0026ldquo;how.\u0026rdquo; Useful for interviews, exams, and interactive learning. 4. Easy to Personalize and Iterate # You can tailor questions to your learning needs. Easy to edit, reorder, or update. 5. Encourages Metacognition # Prompts you to evaluate what you really understand. Offers built-in feedback for where to go deeper. 6. Efficient Review # You can skim questions as cues. Supports flashcards, spaced repetition, and study guides. Definition: Step-by-Step Chain-of-Thought (CoT) Q\u0026amp;A # A Step-by-Step Chain-of-Thought Q\u0026amp;A is a structured learning format where complex content is unpacked through a connected sequence of Q\u0026amp;A pairs. Each pair builds on the last, promoting progressive reasoning, conceptual layering, and active learning.\nKey Elements # Element Description Step-by-Step Each Q\u0026amp;A builds on the previous one. Chain-of-Thought Encourages logical flow and progressive insight. Q\u0026amp;A Format Uses question and answer pairs to enforce retrieval. Interlinked Flow Transitions guide the learner through complexity. Use Cases in GenAI Instruction # LLM Tutors Interactive Study Bots Curriculum Builders Interview Simulators RAG-based Reasoning Chains Template: Step-by-Step CoT Q\u0026amp;A Guide # Topic / Paragraph: (Paste or describe your learning content here)\n[Section Title: Basic Concept] # Q1. [Definition or identification]\nA: [Brief answer]\nQ2. [Role or importance]\nA: [Contextualize the concept]\n➡️ (Transition: “Now that we know what it is…”)\n🔍 [Section Title: Deeper Understanding] # Q3. [How it works?]\nA: [Explain mechanism]\nQ4. [Use cases or applications?]\nA: [Add examples]\n➡️ (Transition: “Let’s go further…”)\n[Advanced Reasoning or Connections] # Q5. [Limitations or misconceptions?]\nA: [Clarify]\nQ6. [How does it relate to other topics?]\nA: [Link concepts]\n[Wrap-Up Summary] # Q7. [Key takeaway?]\nA: [Final reinforcement]\nPrompt for GenAI # You are a step-by-step reasoning tutor. Given the following paragraph or topic, generate a Chain-of-Thought Q\u0026amp;A guide that progresses from basic to advanced understanding. - Start with definition and significance. - Then explain mechanisms or processes. - Include reasoning, misconceptions, connections to other topics. - Use Q\u0026amp;A format with numbered questions. - Add logical transitions (e.g., \u0026#34;Next,\u0026#34; \u0026#34;Building on that,\u0026#34; \u0026#34;Let’s go deeper\u0026#34;). - End with a summary or key takeaway. Topic: [Insert paragraph or subject] Example: Generative Agents # Q1. What are generative agents?\nA: Generative agents are AI-powered characters or entities capable of simulating human-like behaviors, decisions, and conversations in interactive environments using large language models (LLMs).\nQ2. Why are they important?\nA: They demonstrate how LLMs can emulate planning, memory, reflection, and social interaction—paving the way for believable AI characters in simulations, education, gaming, and research.\n➡️ Let’s go deeper into how they work\u0026hellip;\nQ3. How do generative agents function technically?\nA: They integrate multiple components: a memory system to store and retrieve past events, a reflection mechanism to infer insights from experience, and a planning loop powered by LLMs to generate behavior over time.\nQ4. Where are generative agents commonly applied?\nA: In simulated societies, open-ended games, educational roleplay, autonomous NPCs (non-player characters), and human-AI interaction studies.\n➡️ Now let\u0026rsquo;s explore challenges and extensions\u0026hellip;\nQ5. What are the limitations of generative agents?\nA: They can hallucinate, struggle with temporal consistency, and lack grounding in real-world perception or physical environments.\nQ6. How do generative agents relate to other AI systems?\nA: They intersect with reinforcement learning agents, cognitive architectures, and retrieval-augmented generation (RAG) systems in their use of memory, planning, and generative reasoning.\nSummary # Q7. What is the core idea to remember about generative agents?\nA: Generative agents simulate believable, context-aware behavior using LLMs combined with memory and reasoning components—representing a major step in human-like AI interaction.\nUse this format in Hugo Book or Notion for modular, progressive learning.\n"},{"id":75,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/precision_vs_recall_in_healthcare/","title":"Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare","section":"C3 ML Healthcare","content":" Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare # This guide summarizes two key scenarios in healthcare where we might prefer:\nHigh Precision but Lower Recall High Recall but Lower Precision (1) High Precision, Lower Recall # ✅ When to Use: # When false positives are costly or harmful When resources are limited In early screening/filtering stages 📌 Justification: # You want to be very confident before taking action. Missing some real cases is acceptable if wrongly flagging someone leads to emotional, financial, or clinical harm. 💡 Examples: # Genetic Testing for Rare Diseases: Only flag patients when you\u0026rsquo;re very sure. A false positive could cause unnecessary panic or life changes. ICU Bed Allocation: If you only have 5 beds, you’d want to use them for patients who are most certainly critical. Drug Discovery Pre-Screening: Select molecules that are most likely to work, even if some potential candidates are missed. (2) High Recall, Lower Precision # ✅ When to Use: # When missing a real case is dangerous When early detection can improve outcomes When follow-up tests or actions are safe and cheap 📌 Justification: # It\u0026rsquo;s better to catch every possible case, even if you have some false alarms. Especially important in serious or rapidly progressing conditions. 💡 Examples: # Cancer Screening: Better to flag more patients for follow-up than miss someone with early-stage cancer. Sepsis Prediction in ER: Alerting the care team early—even with some false alarms—can save lives. COVID-19 Testing in High-Risk Areas: Broad detection to prevent spread, even if some healthy people test positive. 🧠 Summary Table # Scenario Priority Justification Example High Precision, Lower Recall Precision 🟢 Avoid harm/cost from false positives Genetic testing, ICU triage High Recall, Lower Precision Recall 🟢 Avoid missing critical or contagious conditions Cancer screening, sepsis alert "},{"id":76,"href":"/ai-workflows/genai-systems/transformer_attention_concepts/","title":"Transformer Attention: Full Conceptual Breakdown","section":"GenAI Systems","content":" Transformer Attention: Full Conceptual Breakdown # This document summarizes an in-depth discussion on attention mechanisms in Transformers, with a special focus on vocabulary embeddings, Q/K/V matrices, and multi-head attention.\n📌 1. Understanding the Self-Attention Image # The image shows a single-head self-attention computation. Each row is a token (element) at a position, with a feature vector (embedding). The attention weights (left column) are used to compute a weighted sum over these vectors. The final output vector is shown at the bottom — this is the attention output for one token. 🔍 2. Element vs. Position # Element: the actual word or token in the input sequence. Position: the index of the element in the sequence. Though tightly coupled (1:1), they are conceptually different. Transformers rely on positional encoding to retain order, since attention alone is orderless. 🤖 3. How Attention Scores Are Computed # Input embeddings X are projected into:\nQueries (Q) Keys (K) Values (V) Attention score between token i and j:\nscore = dot(Q[i], K[j]) / sqrt(d_k) Apply softmax to get weights.\nMultiply each Value by its weight and sum → gives the final output vector.\n🧠 4. What Is X in the Diagram? # The large matrix on the right of the image is the input embedding matrix X. Shape: sequence_length × embedding_dim It is built by looking up each token’s vector from the vocabulary embedding matrix. 🔄 5. What Is Multi-Head Attention? # Single-head attention is shown in the image. Multi-head attention: Splits X into smaller chunks (d_model / n_heads) Computes self-attention in parallel on each chunk (head) Concatenates results from all heads Applies a final linear projection 🔡 6. Vocab Embedding Matrix vs. Q/K/V # Vocabulary embedding matrix: Initialized randomly Trained to map each token to a vector Q, K, V: Computed from X using learned matrices W_Q, W_K, W_V Not stored in the vocabulary matrix Are trainable and persistent ♻️ 7. Lifetime of W_Q, W_K, W_V # These matrices are: Initialized once Trained over time Reused across batches They are not reset per input or per batch. Gradients update them through backpropagation. 📥 8. Is Vocabulary Matrix Also Trainable? # ✅ Yes.\nIt is randomly initialized and trained alongside the rest of the model. Each token lookup retrieves a vector from this matrix. This matrix evolves to encode semantic relationships between words. 📦 9. Use Cases After Training # Goal Uses Vocab Matrix Uses W_Q/K/V Inference on new sentence ✅ ✅ Static embedding for a token ✅ ❌ Contextual embedding in sentence ✅ ✅ 📐 10. Dimensions of X, Q, K, V, and Attention # Let:\nL = sequence length d_model = embedding dimension (e.g. 512) n_heads = number of attention heads d_k = d_model / n_heads Component Shape Input X (L, d_model) W_Q, W_K, W_V (d_model, d_model) Q, K, V (stacked) (n_heads, L, d_k) Attention output (head) (L, d_k) Concatenated heads (L, d_model) Final output (L, d_model) ❓ 11. Why Isn’t the Final Output a Distribution Over Vocabulary? # This is a great question that highlights a common confusion.\nThe output of multi-head attention (and the full Transformer stack) is:\n(L, d_model) But the vocabulary distribution comes after applying a final linear layer:\nW_vocab ∈ ℝ^(d_model × vocab_size) logits = output × W_vocab → (L, vocab_size) Then softmax gives:\nprobability distribution over vocabulary for each token position Stage Output Shape Multi-head Attention (L, d_model) Final Linear Projection (L, vocab_size) Softmax (L, vocab_size) So the discrepancy is resolved when we remember that attention is only a component — the final vocabulary distribution is computed later in the model pipeline.\nPrepared as a study summary by ChatGPT based on a thread of detailed conceptual questions.\n"},{"id":77,"href":"/ai-workflows/genai-systems/bert_cls_classification_summary/","title":"Understanding How to Use BERT's CLS Token for Classification","section":"GenAI Systems","content":"Date: 2025-03-31\n❓ Question # How can we use the [CLS] token (i.e., h_cls) from the last layer of BERT for classification tasks? Given that the BERT output has shape [batch_size, sequence_length, hidden_size], how is it valid to pass only [batch_size, hidden_size] to a nn.Linear(hidden_size, num_classes) without flattening the sequence? And why don\u0026rsquo;t we flatten the whole sequence — wouldn\u0026rsquo;t that destroy order?\n✅ Answer # 🔹 BERT Output and the [CLS] Token # BERT outputs a tensor of shape:\n[batch_size, sequence_length, hidden_size] But for classification tasks, we typically use only the [CLS] token, which is located at position 0 in the sequence:\nh_cls = outputs.last_hidden_state[:, 0, :] # Shape: [batch_size, hidden_size] This token is designed to act as a summary representation of the entire sequence, and this output shape matches exactly what a nn.Linear(hidden_size, num_classes) expects — no flattening needed.\n🔹 Why Not Flatten? # Flattening the whole sequence (e.g., [batch_size, sequence_length * hidden_size]) loses:\nToken order Positional embeddings Sequence structure In NLP, this breaks the semantic and syntactic structure of the input. Instead, use:\n🔸 Recommended Pooling Strategies # Strategy Description [CLS] Token Use outputs[:, 0, :]; trained as a sequence summary Mean Pooling outputs.mean(dim=1); averages token embeddings Max Pooling outputs.max(dim=1).values; takes strongest signal Attention Pooling Learns weights to summarize tokens adaptively 📚 Sources and Justification # BERT Paper: Devlin et al. (2018) — [CLS] token for classification Sentence-BERT: Reimers \u0026amp; Gurevych (2019) — Mean pooling often better for embeddings Hugging Face Transformers: Practical implementation patterns NLP Community Practices: Kaggle, blogs, and tutorials 🧪 Summary # Use [CLS] or pooling (not flattening) for sequence-level tasks. Flattening destroys sequence information and is rarely appropriate in NLP. The linear layer works on [batch_size, hidden_size] — no need to flatten across tokens. "},{"id":78,"href":"/ai-workflows/genai-systems/self_attention_summary/","title":"Understanding Self-Attention in Transformers: A Visual Breakdown","section":"GenAI Systems","content":" 🔍 Understanding Self-Attention in Transformers: A Visual Breakdown # This document summarizes key questions about self-attention, embedding vectors, positions, and the input matrix in Transformers — using the image you provided as the foundation.\n🧠 What Is Happening in the Diagram? # The figure shows how self-attention computes the output for a specific position (\u0026ldquo;detection\u0026rdquo;) by:\nGenerating attention weights between that position and all other positions. Using those weights to compute a weighted sum of the input feature vectors. 🧩 Key Concepts Explained # Term Meaning Element A token or word in the input sequence. Each row in the matrix is one. Position The index (0-based) of each element. Used to maintain order. Sequence The full ordered list of elements (e.g. a sentence). Word The natural-language item each element may represent. Feature Values Vector representation of the element (its embedding). While element and position are tightly linked (1:1), they are conceptually distinct: Position = slot/index Element = content in that slot 🧮 How Attention Scores Are Computed # Self-attention uses scaled dot-product attention:\nInput matrix X (from the figure) holds all embeddings. It is projected into Q, K, V using learned weights. Attention scores = dot(Q[i], K[j]) / sqrt(d_k) Softmax turns scores into attention weights. Output vector = weighted sum over all V[j], using those weights. The purple bar on the left in the figure shows these attention weights (e.g., [0.3, 0.2, 0.1, 0.3, 0, ...]).\n✅ What the Image Represents # Part of Image Concept in Transformer Right-side matrix (rows) Input feature matrix X Each row One input element (word/token) Left-side purple weights Attention scores for one position Final row at bottom Output vector (weighted sum of inputs) Prepared with explanations from ChatGPT based on your questions.\n"},{"id":79,"href":"/healthcare/clinical_ai/why_clinical_ai/","title":"Why Clinical NLP \u0026 GenAI Are Growing in Healthcare","section":"AI","content":" 🚀 Why Clinical NLP \u0026amp; GenAI Are Growing in Healthcare # Clinical NLP \u0026amp; GenAI are growing rapidly in healthcare because they unlock massive untapped value in unstructured data — which has historically been hard to use, yet contains the richest clinical context.\n1. 80% of Clinical Data is Unstructured # EHRs are full of free-text clinical notes, discharge summaries, radiology reports, operative notes, etc. Traditional models work well with structured data (ICD, labs), but miss context like: “Patient denies chest pain” “Family history of diabetes” “Patient expressed concern about medication side effects” NLP allows us to extract clinical meaning from this text and turn it into computable features.\n2. LLMs Unlocked Previously Impossible Use Cases # Older NLP methods (regex, rule-based, small transformers) had limited scope and brittle performance. LLMs like GPT-4, BioGPT, Med-PaLM, ClinicalBERT now: Understand clinical language Handle ambiguity and nuance (negation, temporality, coreference) Can answer questions, summarize, or extract entities with minimal supervision We now have zero-shot/few-shot models that can generalize better and faster.\n3. Tooling and Ecosystem Improvements # LLMOps tools (e.g., LangChain, LlamaIndex) make it easy to build: RAG pipelines from medical knowledge bases (e.g., UpToDate, PubMed) Clinical chatbots, document summarizers, question-answering tools Medical NLP toolkits are becoming better: scispaCy, medspaCy, MetaMap, cTAKES, MedCAT HuggingFace models like BioClinicalBERT, BlueBERT, PubMedBERT 4. Real Clinical Needs Driving Demand # Physicians are overwhelmed by documentation — GenAI is helping with: Ambient scribes (auto-documenting patient visits) Auto-summarization of notes, referrals, discharge instructions Researchers want to extract phenotypes or chart review signals at scale Payers want to mine notes for HCC coding or prior authorization info NLP reduces chart review time from hours to seconds\n5. Regulatory and Business Shifts # FDA and CMS are recognizing NLP-derived features in trials and risk models Private sector is investing heavily (e.g., Nuance, Abridge, AWS HealthScribe, Epic’s NoteReader, Google Med-PaLM) NLP applications align with value-based care and documentation burden reduction, two big industry trends 6. Surge in Research and Commercial Applications # Explosion of clinical NLP papers, open datasets (MIMIC-III notes, i2b2, n2c2), and competitions Many startups and research labs focus entirely on GenAI for clinical use cases Oncology-Specific Use Case for Clinical NLP # Use Case: Automated Tumor Board Summarization # Problem: Oncologists review vast free-text data for tumor board meetings, including pathology, radiology, and progress notes. NLP Solution: Extracts key findings (e.g., tumor staging, mutations, response to therapy) from notes Summarizes patient’s oncologic timeline Suggests evidence-based treatment pathways using integrated knowledge bases Impact: Saves time preparing for multi-disciplinary meetings Ensures consistent and comprehensive reviews Enables decision support and documentation automation ✅ Summary: Why Clinical NLP \u0026amp; GenAI Are Growing # Factor Description Untapped Data 80% of EHR is free text — highly valuable, underused LLM Capabilities GPT, BioGPT, etc., can extract, summarize, reason Tooling Libraries and APIs make NLP workflows easier to deploy Clinical Demand Ambient documentation, summarization, triage tools Market Forces Reimbursement, policy, burnout, and value-based care Research Fuel Rich open datasets (MIMIC), benchmarks, HuggingFace "}]