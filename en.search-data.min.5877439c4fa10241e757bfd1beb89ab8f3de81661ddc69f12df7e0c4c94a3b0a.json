[{"id":0,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/toc_course2/","title":"[ToC] Course 2","section":"C2 Clinical Data","content":" ToC of Course 2/5: Introduction to Clinical Data # Module 1: Asking and Answering Questions via Clinical Data Mining # Introduction to the data mining workflow Real Life Example Example: Finding similar patients Example: Estimating risk Putting patient data on timeline Revisit the data mining workflow steps Types of research questions Research questions suited for clinical data Example: making decision to treat Properties that make answering a research question useful Module 2: Data Available from Healthcare Systems # Review of the healthcare system Review of key entities and the data they collect Actors with different interests Common data types in Healthcare Strengths and weaknesses of observational data Bias and error from the healthcare system perspective Bias and error of exposures and outcomes How a patient\u0026rsquo;s exposure might be misclassified How a patient\u0026rsquo;s outcome could be misclassified Electronic medical record data Claims data Pharmacy Surveillance datasets and Registries Population health data sets A framework to assess if a data source is useful Module 3: Representing Time, and Timing of Events, for Clinical Data Mining # Introduction Time, timelines, timescales and representations of time Timescale: Choosing the relevant units of time What affects the timescale Representation of time Time series and non-time series data Order of events Implicit representations of time Different ways to put data in bins Timing of exposures and outcomes Clinical processes are non-stationary Module 4: Creating Analysis Ready Datasets from Patient Timelines # Turning clinical data into something you can analyze Defining the unit of analysis Using features and the presence of features How to create features from structured sources Standardizing features Dealing with too many features The origins of missing values Dealing with missing values Summary recommendations for missing values Constructing new features Examples of engineered features When to consider engineered features Main points about creating analysis ready datasets Structured knowledge graphs So what exactly is in a knowledge graph What are important knowledge graphs How to choose which knowledge graph to use Module 5: Handling Unstructured Healthcare Data: Text, Images, Signals # Introduction to unstructured data What is clinical text The value of clinical text What makes clinical text difficult to handle Privacy and de-identification A primer on Natural Language Processing Practical approach to processing clinical text Summary - Clinical text Overview and goals of medical imaging Why are images important? What are images? A typical image management process Summary - Images Overview of biomedical signals Why are signals important? What are signals? What are the major issues with using signals? Summary - Signals Module 6: Putting the Pieces Together: Electronic Phenotyping # Introduction to electronic phenotyping Challenges in electronic phenotyping Specifying an electronic phenotype Two approaches to phenotyping Rule-based electronic phenotyping Examples of rule based electronic phenotype definitions Constructing a rule based phenotype definition Probabilistic phenotyping Approaches for creating a probabilistic phenotype definition Software for probabilistic phenotype definitions Module 7: Ethics # Introduction to Research Ethics and AI The Belmont Report: A Framework for Research Ethics Ethical Issues in Data sources for AI Secondary Uses of Data Return of Results AI and The Learning Health System Ethics Summary "},{"id":1,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/toc_course3/","title":"[ToC] Course 3","section":"C3 ML Healthcare","content":" ToC of Course 3/5: Fundamentals of Machine Learning for Healthcare # Module 3: Concepts and Principles of Machine Learning in Healthcare # Introduction to Deep Learning and Neural Networks Deep Learning and Neural Networks Cross Entropy Loss Gradient Descent Representing Unstructured Image and Text Data Convolutional Neural Networks Natural Language Processing and Recurrent Neural Networks The Transformer Architecture for Sequences Commonly Used and Advanced Neural Network Architectures Advanced Computer Vision Tasks and Wrap-Up Module 4: Evaluation and Metrics for Machine Learning in Healthcare # Introduction to Model Performance Evaluation Overfitting and Underfitting Strategies to Address Overfitting, Underfitting and Introduction to Regularization Statistical Approaches to Model Evaluation Receiver Operator and Precision Recall Curves as Evaluation Metrics Module 5: Strategies and Challenges in Machine Learning in Healthcare # Introduction to Common Clinical Machine Learning Challenges Utility of Causative Model Predictions Context in Clinical Machine Learning Intrinsic Interpretability Medical Data Challenges in Machine Learning Part 1 Medical Data Challenges in Machine Learning Part 2 How Much Data Do We Need? Retrospective Data in Medicine and \u0026ldquo;Shelf Life\u0026rdquo; for Data Medical Data: Quality vs Quantity Module 6: Best Practices, Teams, and Launching Your Machine Learning Journey # Clinical Utility and Output Action Pairing Taking Action - Utilizing the OAP Framework Building Multidisciplinary Teams for Clinical Machine Learning Governance, Ethics, and Best Practices On Being Human in the Era of Clinical Machine Learning Death by GPS and Other Lessons of Automation Bias Module 7: Foundation Models # Introduction to Foundation Models Adapting to Technology General AI and Emergent Behavior How Foundation Models Work Healthcare Use Cases for Text Data Healthcare Use Cases for Non-textual Unstructured Data Challenges and Pitfalls Conclusion "},{"id":2,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/toc_course4/","title":"[ToC] Course 4","section":"C4 AI Evaluations","content":" ToC of Course 4/5: Evaluations of AI Applications in Healthcare # Module 1: AI in Healthcare # Learning Objectives Common Definitions Overview Why AI is needed in Healthcare Examples of AI in Healthcare Growth of AI in Healthcare Questions Answered by AI AI Output Think beyond area under the curve Module 2: Evaluations of AI in Healthcare # Learning Objectives Recap: Framework Stakeholders Clinical Utility Outcome: Action Pairing, An Overview Lead Time Type of Action OAP Examples Number Needed to Treat Net Benefits Decision Curves Feasibility overview Implementation Costs Clinical Evaluation and Uptake Summary Module 3: AI Deployment # Learning Objectives The Problem Practical Questions Prior to Deployment Deployment Pathway Design and Development Stakeholder Involvement Data Type and Sources Settings In Silico Evaluation Net Utility \u0026amp; Work Capacity Statistical Validity Care Integration, Silent Mode Clinical Integration, Considerations Technical Integration Deployment Modalities Continuous Monitoring and Maintenance Challenges of Deployment Sepsis Example Summary Module 4: Downstream Evaluations of AI in Healthcare: Bias and Fairness # Learning Objectives Real World Examples of AI Bias Introduction - Types of Bias Historical Bias Representation Bias Measurement Bias Aggregation Bias Evaluation Bias Deployment Bias What is algorithmic Fairness Anti-classification Parity Classification Calibration Applying Fairness Measures Lack of Transparency Minimal Reporting Standards Opportunities and Challenges Summary Module 5: The Regulatory Environment for AI in Healthcare # Learning Objectives The Problem International Definitions Used for Regulatory Purposes Definition Statement \u0026amp; Risk Framework Valid Clinical Association Analytical Evaluation Clinical Evaluation General Control de novo Notifications Software Modification TPLC Locked vs Adapted AI solutions Examples Non-Regulated Products EU Regulations Chinese Guidelines OMB Guidelines Summary Module 7: AI and Medicine (Optional Content) # Introduction: Navigating the Intersections of AI and Medicine Life Cycle of AI A Deep Dive into Historical and Societal Dimensions Race-Based Medicine and Race-Aware Approach Bias Mitigation Strategies Exploring Potentials and Ethical Quandaries Dismantling Race-Based Medicine Deploying AI into Healthcare Settings Conclusion "},{"id":3,"href":"/ai-workflows/data/data-centric-ai/data-centric-vs-model-centric/","title":"Data-Centric AI vs Model-Centric AI","section":"Data-Centric AI (DCAI)","content":" Data-Centric AI vs Model-Centric AI MIT Lecture : https://dcai.csail.mit.edu/ GitHub for Labs: https://github.com/dcai-course/dcai-lab Q1: What is the traditional model-centric approach to machine learning? # Traditionally, machine learning has focused on model-centric AI, where the dataset is fixed and the goal is to tune the model:\nLearn different model architectures Modify hyperparameters and training losses Focus on improving model performance given clean data This is how ML is often taught in courses (e.g., MIT 6.036) Q2: What challenges arise in real-world ML settings? # Real-world data is:\nOften messy and error-prone Not fixed or clean Contains label errors, noise, and distribution shifts Therefore, improving data quality can be more impactful than model tinkering.\nQ3: What is Data-Centric AI (DCAI)? # Data-Centric AI focuses on improving data quality to enhance model performance:\nGiven any model, systematically improve the dataset. Examples: Curriculum Learning (easy to hard examples) Confident Learning (identify and remove label errors) Q4: How is Data-Centric AI different from Model-Centric AI? # Feature Model-Centric AI Data-Centric AI Goal Improve model Improve dataset Fixed/Variable Dataset is fixed Dataset is changeable Techniques Tuning, loss functions, archs Error correction, augmentation, etc. Real-world use Less applicable in messy domains Highly applicable Q5: What techniques fall under Data-Centric AI? # Outlier detection and removal Error correction Data augmentation Feature selection Active learning Establishing consensus labels Q6: Why is DCAI gaining attention now? # Large foundation models (e.g., DALL-E, GPT-3) suffer from data issues Examples: OpenAI identified label errors as major performance bottlenecks ChatGPT fine-tuned using better data, with human ranking Tesla‚Äôs Data Engine loops model outputs back to dataset improvements Q7: What‚Äôs a real example of a label error found in a famous dataset? # Hinton demonstrated a label error in the MNIST dataset (e.g., mislabeling of digit ‚Äò3‚Äô) using data-centric techniques.\nQ8: What is PU Learning and why is it called \u0026ldquo;the perceptron of Data-Centric AI\u0026rdquo;? # PU Learning stands for Positive-Unlabeled Learning. It deals with training models using:\nA small set of positive examples\nA large set of unlabeled data\nNo explicitly labeled negatives\nIt\u0026rsquo;s called \u0026ldquo;the perceptron of DCAI\u0026rdquo; because:\nIt is a foundational concept that demonstrates the importance of improving data rather than models. Just as the perceptron is a gateway to model-centric ML, PU Learning introduces core data-centric thinking. PU Learning is used when:\nFull labeling is impractical or costly (e.g., medical records, fraud detection) You need to build a classifier using partial label information Example:\nIf you only know a few patients with a disease (positives), and the rest of your records are unlabeled, PU Learning helps build a classifier without assuming all unlabeled cases are negative. "},{"id":4,"href":"/ai-workflows/genai/5-day-genai-google-2025/day1_foundational_llm_text_generation/","title":"Day 1 - Foundational LLMs \u0026 Text Generation","section":"5-Day GenAI with Google 2005","content":" Day 1 - Foundational LLMs \u0026 Text Generation Foundations of LLMs # 1. Why LLMs Matter # Traditional NLP systems were narrow, but Large Language Models (LLMs) offer general-purpose capabilities like translation, Q\u0026amp;A, and summarization‚Äîall without explicit task-specific programming.\n‚Üí How do LLMs work under the hood?\n2. What Powers LLMs: The Transformer # The Transformer is the core architecture enabling LLMs. Unlike RNNs that process data sequentially, Transformers handle inputs in parallel using self-attention, allowing them to model long-range dependencies more efficiently and scale training.\n‚Üí But to understand how Transformers process input, we need to examine how input data is prepared.\n3. Input Preparation \u0026amp; Embedding # Before data enters the transformer, it‚Äôs tokenized, embedded into high-dimensional vectors, and enhanced with positional encodings to preserve word order. These embeddings become the input that feeds into attention mechanisms.\n‚Üí So, once we have these embeddings‚Äîhow does the model understand relationships within the input?\n4. Self-Attention and Multi-Head Attention # The self-attention mechanism calculates how each word relates to every other word. Multi-head attention expands on this by letting the model attend to different relationships in parallel (e.g., syntax, co-reference). This enables rich, contextual understanding.\n‚Üí To manage this complexity across layers, the architecture needs stabilization techniques.\n5. Layer Normalization and Residual Connections # To avoid training instability and gradient issues, Transformers use residual connections and layer normalization, ensuring smooth learning across deep layers.\n‚Üí After stabilizing, each layer further transforms the data with an extra module‚Ä¶\n6. Feedforward Layers # Each token\u0026rsquo;s representation is independently refined using position-wise feedforward networks that add depth and non-linearity‚Äîenhancing the model‚Äôs ability to capture abstract patterns.\n‚Üí With these components, we now have building blocks for the full Transformer structure.\n7. Encoder-Decoder Architecture # In the original Transformer, the encoder turns input text into a contextual representation, and the decoder autoregressively generates output using that context. However, modern LLMs like GPT simplify this by using decoder-only models for direct generation.\n‚Üí As LLMs scale, new architectures emerge to improve efficiency and specialization.\n8. Mixture of Experts (MoE) # MoE architectures use specialized sub-models (experts) activated selectively via a gating mechanism. This allows LLMs to scale massively while using only a portion of the model per input‚Äîenabling high performance with lower cost.\n‚Üí But performance isn‚Äôt just about architecture‚Äîreasoning capabilities are equally vital.\n9. Building Reasoning into LLMs # Reasoning is enabled via multiple strategies:\nChain-of-Thought prompting: Guide the model to generate intermediate steps. Tree-of-Thoughts: Explore reasoning paths via branching. Least-to-Most: Build up from simpler subproblems. Fine-tuning on reasoning datasets and RLHF further optimize for correctness and coherence. ‚Üí To train these reasoning patterns, we need carefully prepared data and efficient training pipelines.\n10. Training the Transformer # Training involves:\nData preparation: Clean, tokenize, and build vocabulary. Loss calculation: Compare outputs to targets using cross-entropy. Backpropagation: Update weights with optimizers (e.g., Adam). ‚Üí Depending on the architecture, training objectives differ.\n11. Model-Specific Training Strategies # Decoder-only (e.g., GPT): Predict next token from prior sequence. Encoder-only (e.g., BERT): Mask tokens and reconstruct. Encoder-decoder (e.g., T5): Learn input-to-output mapping for tasks like translation or summarization. Training quality is influenced by context length‚Äîlonger context allows better modeling of dependencies, but at higher compute cost.\n12. The Evolution Begins: From Attention to Transformers # It started with the 2017 \u0026ldquo;Attention Is All You Need\u0026rdquo; paper‚Äîlaying the groundwork for all Transformer-based models. This sparked a sequence of breakthroughs in model architecture and training methods.\n13. GPT-1: Unsupervised Pre-training Breakthrough # GPT-1 was a decoder-only model trained on BooksCorpus with a pioneering strategy: pre-train on unlabeled data, fine-tune on supervised tasks. This showed that unsupervised learning scales better than purely supervised models and inspired the unified transformer approach.\n14. BERT: Deep Understanding through Masking # BERT introduced the encoder-only model that trains on masked tokens and sentence relationships. Unlike GPT, it‚Äôs not a generator but an understander, excelling in classification, NLU, and inference tasks.\n15. GPT-2: Scaling Up Leads to Zero-Shot Learning # By expanding to 1.5B parameters and using a diverse dataset (WebText), GPT-2 revealed that larger models can generalize better, even to unseen tasks‚Äîzero-shot prompting emerged as a surprising capability.\n16. GPT-3 to GPT-4: Generalist Reasoners with Instruction-Tuning # GPT-3 scaled to 175B parameters and needed no fine-tuning for many tasks. Later versions (GPT-3.5, GPT-4) added coding, longer context windows, multimodal inputs, and improved instruction following via InstructGPT and RLHF.\n17. LaMDA: Dialogue-Focused Language Modeling # Google‚Äôs LaMDA was purpose-built for open-ended conversations, emphasizing turn-based flow and topic diversity‚Äîunlike GPT, which handled more general tasks.\n18. Gopher: Bigger is Smarter (Sometimes) # DeepMind‚Äôs Gopher used high-quality MassiveText data. It scaled to 280B parameters and showed that size improves knowledge-intensive tasks, but not always reasoning‚Äîhinting at the importance of data quality and task balance.\n19. GLaM: Efficient Scaling with Mixture-of-Experts # GLaM pioneered sparse activation, using only parts of a trillion-parameter network per input. It demonstrated that MoE architectures can outperform dense ones using far less compute.\n20. Chinchilla: The Scaling Laws Revolution # Chinchilla showed that previous scaling laws (Kaplan et al.) were suboptimal. DeepMind proved that data-to-parameter ratio matters‚Äîa smaller model trained on more data can outperform much larger ones.\n21. PaLM and PaLM 2: Distributed and Smarter # PaLM (540B) used Google\u0026rsquo;s TPU Pathways for efficient large-scale training. PaLM 2 reduced parameters but improved performance via architectural tweaks, showcasing that smarter design beats brute force.\n22. Gemini Family: Multimodal, Efficient, and Scalable # Gemini models support text, images, audio, and video inputs. Key innovations include:\nMixture-of-experts backbone Context windows up to 10M tokens (Gemini 1.5 Pro) Versions for cloud (Pro), mobile (Nano), and ultra-scale inference (Ultra) Gemini 2.0 Flash enables fast, explainable reasoning for science/math tasks. 23. Gemma: Open-Sourced and Lightweight # Built on Gemini tech, Gemma models are optimized for accessibility. The 2B and 27B variants balance performance and efficiency, with Gemma 3 offering 128K token windows and 140-language support.\n24. LLaMA Series: Meta‚Äôs Open Challenger # Meta‚Äôs LLaMA models evolved with increased context length and safety. LLaMA 2 introduced chat-optimized variants; LLaMA 3.2 added multilingual and visual capabilities with quantization for on-device use.\n25. Mixtral: Sparse Experts and Open Access # Mistral AI‚Äôs Mixtral 8x7B uses sparse MoE with only 13B active params per token, excelling in code and long-context tasks. Instruction-tuned variants rival closed-source models.\n26. OpenAI O1: Internal Chain-of-Thought # OpenAI‚Äôs ‚Äúo1‚Äù models use deliberate internal CoT reasoning to excel in programming, science, and Olympiad-level tasks, aiming for thoughtful, high-accuracy outputs.\n27. DeepSeek: RL Without Labels # DeepSeek-R1 uses pure reinforcement learning without labeled data. Their GRPO method enables self-supervised reasoning with rejection sampling and multi-stage fine-tuning, matching ‚Äúo1‚Äù performance.\n28. The Open Frontier # Multiple open models are pushing the boundaries:\nQwen 1.5 (Alibaba): up to 72B params, strong multilingual support. Yi (01.AI): 3.1T token dataset, 200k context length, vision support. Grok 3 (xAI): 1M context tokens, trained with RL for strategic reasoning. 29. Comparing the Giants # Transformer models have scaled in size, context, and capability. From 117M to 1T+ parameters, from 512-token limits to 10M-token contexts. Key insights:\nBigger is not always better‚Äîefficiency, data quality, and training methods matter more. Reasoning and instruction-following are now central. Multimodality and retrieval-augmented generation are shaping next-gen LLMs. Fine-Tuning and Using LLMs # 30. From Pretraining to Specialization: Why Fine-Tune? # LLMs are pretrained on broad data to learn general language patterns. But for real-world use, we often need them to follow specific instructions, engage in safe dialogues, or behave reliably. This is where fine-tuning comes in.\n31. Supervised Fine-Tuning (SFT): The First Specialization Step # SFT improves LLM behavior using high-quality labeled datasets. Typical goals:\nBetter instruction-following Multi-turn dialogue (chat) Safer, less toxic outputs Example formats: Q\u0026amp;A, summarization, translations‚Äîeach with clear input-output training pairs.\n32. Reinforcement Learning from Human Feedback (RLHF) # SFT gives positive examples. But what about discouraging bad outputs? RLHF introduces a reward model trained on human preferences, which then helps guide the LLM via reinforcement learning to:\nPrefer helpful, safe, and fair responses Avoid toxic or misleading completions Advanced variants include RLAIF (AI feedback) and DPO (direct preference optimization) to reduce reliance on human labels.\n33. Parameter Efficient Fine-Tuning (PEFT): Adapting Without Full Retraining # Full fine-tuning is costly. PEFT methods train small, targeted modules instead:\nAdapters: Mini-modules injected into LLM layers, trained separately LoRA: Low-rank matrices update original weights efficiently QLoRA: Quantized LoRA for even lower memory Soft Prompting: Trainable vectors (not full prompts) condition the frozen model PEFT enables plug-and-play modules across tasks, saving memory and time.\n34. Fine-Tuning in Practice (Code Example) # Google Cloud\u0026rsquo;s Vertex AI supports SFT using Gemini models with JSONL datasets and APIs. A few lines of code initialize the model, start fine-tuning, and use the new endpoint‚Äîall on cloud infrastructure.\n35. Using LLMs Effectively: Prompt Engineering # LLMs respond differently based on how you ask:\nZero-shot: Just the instruction Few-shot: Add 2‚Äì5 examples Chain-of-thought: Show step-by-step reasoning Effective prompting is key to controlling tone, factuality, or creativity.\n36. Sampling Techniques: Controlling Output Style # After generating probabilities, sampling chooses the next token:\nGreedy: Always highest prob (safe but repetitive) Random/Temperature: More creativity Top-K / Top-P: Add diversity while maintaining focus Best-of-N: Generate multiple candidates, choose best Choose based on your goal: safety, creativity, or logic.\n37. Task-Based Evaluation: Beyond Accuracy # As LLMs become foundational platforms, reliable evaluation is critical:\nCustom datasets: Reflect real production use System-level context: Include RAG and workflows, not just model Multi-dimensional ‚Äúgood‚Äù: Not just matching ground truth but business outcomes 38. Evaluation Methods # Traditional metrics: Fast but rigid Human evaluation: Gold standard, but costly LLM-powered autoraters: Scalable evaluations with rubrics, rationales, and subtasks Meta-evaluation calibrates autoraters to human preferences‚Äîessential for trust.\nConclusion # This section links training, fine-tuning, and usage of LLMs in a production-ready loop:\nTrain generally ‚Üí fine-tune specifically Prompt smartly ‚Üí sample selectively Evaluate robustly Together, these techniques ensure LLMs are accurate, safe, helpful, and aligned with real-world needs.\nAccelerating Inference in LLMs # 39. Scaling vs Efficiency: Why Speed Matters Now # LLMs have grown 1000x in parameter count. While quality has improved, cost and latency of inference have also skyrocketed. Developers now face an essential tradeoff: balancing performance with resource efficiency for real-world deployments.\n40. The Big Tradeoffs # a. Quality vs Latency/Cost # Sacrifice a bit of quality for big speed gains (e.g., smaller models, quantization). Works well for simpler tasks where top-tier quality isn\u0026rsquo;t needed. b. Latency vs Cost (Throughput) # Trade speed for bulk efficiency (or vice versa). Useful in scenarios like chatbots (low latency) vs offline processing (high throughput). 41. Output-Approximating Methods # These techniques may slightly affect output quality, but yield major gains in performance.\nüîπ Quantization # Reduce weight/activation precision (e.g., 32-bit ‚Üí 8-bit). Saves memory and accelerates math operations. Some quality loss, but often negligible with tuning. üîπ Distillation # Use a smaller student model trained to mimic a larger teacher model. Techniques: Data distillation: Generate synthetic data with teacher. Knowledge distillation: Match student output distributions. On-policy distillation: Reinforcement learning feedback per token. 42. Output-Preserving Methods # These do not degrade quality and should be prioritized.\nüîπ Flash Attention # Optimizes memory movement during attention. 2‚Äì4x latency improvement with exact same output. üîπ Prefix Caching # Cache attention computations (KV Cache) for unchanged inputs. Ideal for chat histories or uploaded documents across multiple queries. üîπ Speculative Decoding # A small \u0026ldquo;drafter\u0026rdquo; model predicts tokens ahead. Main model verifies in parallel. Huge speed-up with no quality loss, if drafter is well aligned. 43. Batching and Parallelization # Beyond ML-specific tricks, use general system-level methods:\nBatching: Handle multiple decode requests at once. Parallelization: Distribute heavy compute ops across TPUs/GPUs. Decode is memory-bound and can benefit from parallel batching as long as memory limits aren‚Äôt exceeded.\nSummary # Inference optimization is about smarter engineering, not just faster chips. You can:\nTrade off quality when it‚Äôs safe. Preserve output via caching and algorithmic improvements. Use hybrid setups like speculative decoding + batching. Choose methods based on your task: low-latency chat, high-volume pipelines, or edge deployment. Speed and cost matter‚Äîespecially at scale.\nApplications and Outlook # 44. LLMs in Action: Real-World Applications # After mastering training, inference, and prompting, the final step is applying LLMs to real tasks. These models have transformed how we interact with information across modalities‚Äîtext, code, images, audio, and video.\n43. Core Text-Based Applications # üîπ Code and Mathematics # LLMs support:\nCode generation, completion, debugging, refactoring Test case and documentation generation Language translation between programming languages Tools like AlphaCode 2, FunSearch, and AlphaGeometry push competitive coding and theorem solving to new heights. üîπ Machine Translation # LLMs understand idioms and context:\nChat translations in apps Culturally-aware e-commerce descriptions Voice translations in travel apps üîπ Text Summarization # Use cases:\nSummarizing news with tone Creating abstracts for scientific research Thread summaries in chat apps üîπ Question-Answering # LLMs reason through queries with:\nPersonalization (e.g. in customer support) Depth (e.g. in academic platforms) RAG-enhanced factuality and improved prompts üîπ Chatbots # Unlike rule-based bots, LLMs handle:\nFashion + support on retail sites Sentiment-aware entertainment moderation üîπ Content Generation # Ads, marketing, blogs, scriptwriting Use creativity-vs-correctness sampling tuning üîπ Natural Language Inference # Legal analysis, diagnosis, sentiment detection LLMs bridge subtle context to derive conclusions üîπ Text Classification # Spam detection, news topic tagging Feedback triage, model scoring as \u0026ldquo;autoraters\u0026rdquo; üîπ Text Analysis # Market trends from social media Thematic and character analysis in literature 44. Multimodal Applications # Beyond text, multimodal LLMs analyze and generate across data types:\nCreative: Narrate stories from images or video Educational: Personalized visual+audio content Business: Chatbots using both image+text inputs Medical: Scans + notes = richer diagnostics Research: Drug discovery using cross-data fusion Multimodal systems build on unimodal strengths, scaling to more sensory and intelligent interactions.\nSummary # Transformer is the backbone of modern LLMs. Model performance depends on size and training data diversity. Fine-tuning strategies like SFT, RLHF, and safety tuning personalize models for real-world needs. Inference optimization is critical‚Äîuse PEFT, Flash Attention, prefix caching, and speculative decoding. Prompt engineering and sampling tuning matter for precision or creativity. Applications are exploding‚Äîtext, code, chat, multimodal interfaces. LLMs are not just tools‚Äîthey\u0026rsquo;re platforms. They‚Äôre reshaping how we search, chat, learn, create, and discover.\n"},{"id":5,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/summary_m1/","title":"[Summary] Module 1: Asking Answering Questions via Clinical DataMining","section":"C2 Clinical Data","content":" Module 1: Asking Answering Questions via Clinical DataMining # 1 Introduction to the data mining workflow # Q: What is the main goal of this course? # A: To explain how clinical data can be used to answer research questions that improve patient and population health.\nQ: What is the structure of the course? # A: It begins with choosing meaningful research questions, followed by understanding the healthcare system, exploring data types, reviewing processing and analysis methods, and addressing bias and error.\nQ: What is the data mining workflow in this context? # A: It consists of four key steps:\nPose a research question Identify suitable data sources Extract and transform data Conduct analysis using appropriate methods Q: Why is posing the research question considered the most important step? # A: Because all subsequent steps rely on having a clear and meaningful question to guide data selection and analysis methods.\n‚û°Ô∏è What makes a research question useful to answer?\n2 Real Life Example # Q: What clinical scenario is used to illustrate the data mining process? # A: A teenager named Laura with systemic lupus erythematosus (SLE) develops proteinuria, pancreatitis, and antiphospholipid antibodies ‚Äî raising the question of whether she should receive anticoagulant medication.\nQ: What makes this case clinically significant? # A: The complexity and severity of symptoms, especially in a teenager, highlight a rare and serious condition that lacks straightforward treatment guidelines.\nQ: What is the key research question raised? # A: Should teenagers with SLE, proteinuria, and antiphospholipid antibodies be treated with anticoagulants?\nQ: What is the traditional method for answering such questions? # A: Reviewing medical literature or relying on clinical experience.\nQ: Why is data mining proposed instead? # A: Because traditional evidence might be limited or absent for such specific populations, making retrospective data mining valuable for generating insights.\n‚û°Ô∏è How can we find similar patients to inform treatment decisions?\n3 Example: Finding similar patients # Q: What is the next step in answering the clinical question? # A: Extract and transform data from the electronic medical record (EMR) to find patients who match the research criteria.\nQ: What challenge does EMR data present? # A: EMR data are not organized for easy searching and often require clinical expertise to map to useful criteria, like diagnosis codes or lab values.\nQ: How would one identify relevant pediatric patients? # A: By querying based on age to find those under 18, and then filtering for those with a diagnosis code indicating systemic lupus erythematosus (SLE).\nQ: How is proteinuria identified? # A: Through lab values in urine tests.\nQ: How about antiphospholipid antibodies? # A: These are ideally found via numeric lab data, but often reside in unstructured text, requiring natural language processing (NLP) or manual review.\n‚û°Ô∏è How do we estimate risk in patients like Laura using this data?\n4 Example: Estimating risk # Q: What is the objective of the analysis in this case? # A: To estimate the risk of clotting in teenagers with SLE, proteinuria, and antiphospholipid antibodies compared to the baseline risk in teenagers with just SLE.\nQ: How is the patient cohort defined? # A: By selecting patients who meet the specified criteria and identifying which of them had a blood clot event.\nQ: What is a major practical challenge in this step? # A: Extracting usable patient data from the EMR, which is often the bottleneck due to unstructured or fragmented data.\nQ: What assumption is made for the purposes of this example? # A: That the data extraction step is bypassed, allowing focus directly on the risk analysis.\n‚û°Ô∏è How can patient data be visualized over time using timelines?\n5 Putting patient data on timeline # Q: What is the purpose of using a patient timeline? # A: To visually arrange and analyze a patient\u0026rsquo;s clinical data chronologically to understand event sequences and compute outcomes like risk.\nQ: What types of data are placed on the timeline? # A: Diagnosis codes, lab results, medication orders, and clinician notes from the EMR.\nQ: How are relevant patients identified and flagged? # A: Pediatric patients are filtered first, followed by those with SLE, and then those developing proteinuria and antiphospholipid antibodies.\nQ: What does the timeline help determine in this context? # A: Whether a patient developed a blood clot after the onset of each condition, enabling computation of relative risk based on timing.\n‚û°Ô∏è How do we revisit and refine the steps of the data mining workflow?\n6 Revisit the data mining workflow steps # Q: What was the clinical question in the example revisited here? # A: Whether the risk of clotting in teenagers with SLE, proteinuria, and antiphospholipid antibodies justifies treatment with anticoagulants.\nQ: What was the data source? # A: The Electronic Medical Record (EMR).\nQ: What steps were involved in extracting and transforming the data? # A: Identifying teenagers with SLE, forming subgroups based on clinical criteria, using diagnosis codes, crafting search terms, and occasionally applying proxy terms with follow-up confirmation.\nQ: How was analysis conducted? # A: By comparing clotting risk in subgroups to the general risk in teenagers with SLE, guiding the decision to treat.\nQ: What is the broader implication of this example? # A: It introduces the core elements of the data mining workflow, which will be elaborated in the course‚Äîespecially regarding accurate execution and bias mitigation.\n‚û°Ô∏è What types of research questions can be asked using clinical data?\n7 Types of research questions # Q: Why is it important to understand types of research questions? # A: It helps in formulating one\u0026rsquo;s own questions and critically evaluating the validity of others‚Äô findings using clinical data.\nQ: What is the simplest type of research question? # A: Descriptive questions ‚Äî they summarize data using counts or proportions, e.g., \u0026ldquo;What proportion of the population has familial hypercholesterolemia?\u0026rdquo;\nQ: What comes after descriptive questions? # A: Exploratory questions ‚Äî they aim to find patterns in data, such as identifying subtypes of a disease like autism.\nQ: What types of methods are used in exploratory questions? # A: Techniques like clustering or statistical modeling to identify patterns without predefined hypotheses.\n‚û°Ô∏è Which research questions are best suited for clinical data?\n8 Research questions suited for clinical data # Q: What types of questions are clinical data best suited to answer? # A: Descriptive, exploratory, inferential, and predictive questions.\nQ: What types of questions are harder to answer with clinical data? # A: Causal and mechanistic questions, which often require carefully designed experiments and new data collection.\nQ: What are the two primary goals of asking research questions in medicine? # A:\nRisk stratification ‚Äî determining whether to treat a patient. Data-driven treatment selection ‚Äî deciding how best to treat a patient. Q: Why is it important to match question type with purpose? # A: To ensure both the research question and analysis methods are appropriate and meaningful for clinical decision-making.\n‚û°Ô∏è How does data mining help in making treatment decisions?\n9 Example: making decision to treat # Q: What distinction is made between the question asked and the question answered? # A: The original question was whether to treat Laura, a specific patient. The analysis, however, answered a descriptive question ‚Äî what proportion of similar patients developed clots.\nQ: What kind of analysis was conducted? # A: A descriptive risk stratification, grouping patients based on risk of clotting using historical data.\nQ: What assumption underlies the application of this analysis to Laura? # A: That Laura‚Äôs outcome is likely to mirror those of similar past patients.\nQ: How does this analysis support decision-making? # A: It helps determine whether Laura belongs in a high-risk group, supporting a treatment recommendation such as anticoagulation.\nQ: What additional considerations are necessary in real life? # A: We must also consider the risks of adverse events from the proposed treatment before making a final clinical decision.\n10 Properties that make answering a research question useful # Q: What determines whether answering a question is useful in clinical data mining? # A: Usefulness is assessed via a checklist of aspects, not a strict formula. Key considerations include impact, actionability, and downstream effects.\nQ: What is the first major factor? # A: The number of lives affected ‚Äî including the disease burden and scope of patient populations influenced by the answer.\nQ: What is the second factor? # A: The probability that the results will lead to beneficial changes for clinicians or patients, improving clinical care or outcomes.\nQ: What is the third key aspect? # A: The real-world consequences: does the answer help reduce mortality/morbidity, lower healthcare costs, increase care access, or guide medical decisions?\nQ: What is an important implication of this approach? # A: Slight rephrasing of the question can often make it significantly more useful or relevant.\n‚û°Ô∏è How is clinical data mining used in a real-world example?\n"},{"id":6,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/","title":"C2 Clinical Data","section":"AI in Healthcare","content":" üìò Course 2: Clinical Data # [ToC] Course 2 [Summary] Module 1: Asking Answering Questions via Clinical DataMining [Summary] Module2: Data Available From Healthcare Systems [Summary] Module3: Representing Time Timing Events For Clinical Data Mining [Summary] Module4 : Creating Analysis Ready Dataset from Patient Timelines Clinical Text Feature Extraction Using Dictionary-Based Filtering Clinical Text Mining Pipeline (Steps 1‚Äì5) Ethics in AI for Healthcare Missing Data Scenarios in Healthcare Modeling OMOP vs. RLHF Rule-Based Electronic Phenotyping Example: Type 2 Diabetes üß≠ Module 1: Asking and Answering Questions via Clinical Data Mining # 1. What\u0026rsquo;s the Problem?\nClinicians and researchers have important questions but lack a structured approach to answering them using clinical data.\n2. Why Does It Matter?\nWithout a systematic workflow, decisions may rely on anecdotal evidence or outdated knowledge, leading to suboptimal care.\n3. What\u0026rsquo;s the Core Idea?\nThe 4-step clinical data mining workflow: (1) Ask the right question ‚Üí (2) Find suitable data ‚Üí (3) Extract/transform data ‚Üí (4) Analyze and iterate.\n4. How Does It Work?\nStart with a real clinical scenario, define inclusion/exclusion criteria, search EMRs using codes/tests, and compute outcomes. Use a timeline and patient-feature matrix to support decisions.\n5. What\u0026rsquo;s Next?\nThis foundation enables accurate data selection (Module 2), temporal modeling (Module 3), and building datasets (Module 4).\nüè• Module 2: Data Available from Healthcare Systems # 1. What\u0026rsquo;s the Problem?\nHealthcare data is fragmented, inconsistently coded, and filled with biases and errors.\n2. Why Does It Matter?\nUsing flawed or incomplete data without understanding its origin can lead to misleading conclusions or unsafe decisions.\n3. What\u0026rsquo;s the Core Idea?\nCategorize and understand different healthcare data types, sources, and their limitations, including EMR, claims, registries, and patient-generated data.\n4. How Does It Work?\nStudy the roles of key actors (patients, providers, payers), structured vs. unstructured data types, and typical biases (selection, misclassification, incentives).\n5. What\u0026rsquo;s Next?\nProvides the context for building timelines (Module 3) and feature matrices (Module 4) while recognizing biases that need correction.\nüï∞Ô∏è Module 3: Representing Time in Clinical Data # 1. What\u0026rsquo;s the Problem?\nMost databases don\u0026rsquo;t represent or reason well about time, yet clinical reasoning depends heavily on event timing.\n2. Why Does It Matter?\nIncorrect ordering or missing timestamps can invalidate exposure-outcome relationships and confuse chronic vs. acute processes.\n3. What\u0026rsquo;s the Core Idea?\nUse patient timelines and time-aware logic to represent, bin, and reason about clinical events over time.\n4. How Does It Work?\nDefine index times, use bins to aggregate events, calculate time-to-event, handle censoring, and test for non-stationarity.\n5. What\u0026rsquo;s Next?\nEstablishes the temporal framework needed for building structured datasets (Module 4) and modeling disease progression (Module 6).\nüß± Module 4: Creating Analysis-Ready Datasets # 1. What\u0026rsquo;s the Problem?\nRaw timelines are complex and inconsistent ‚Äî they can\u0026rsquo;t be directly used in analysis or machine learning.\n2. Why Does It Matter?\nPoor feature engineering or ignoring missingness leads to weak, biased, or uninterpretable models.\n3. What\u0026rsquo;s the Core Idea?\nBuild a patient-feature matrix by selecting, cleaning, imputing, and engineering features from structured/unstructured data.\n4. How Does It Work?\nStandardize features, reduce dimensionality, handle missingness with imputation or removal, and use domain knowledge or PCA to create meaningful features.\n5. What\u0026rsquo;s Next?\nFeeds directly into downstream modeling, classification (Module 6), and cohort identification with better interpretability.\nüìÑ Module 5: Handling Unstructured Data # 1. What\u0026rsquo;s the Problem?\nValuable clinical information is trapped in unstructured formats like notes, images, and signals.\n2. Why Does It Matter?\nFailing to extract this information limits your ability to detect key conditions, traits, or outcomes that are not coded elsewhere.\n3. What\u0026rsquo;s the Core Idea?\nUse text mining, signal processing, and image interpretation to turn unstructured data into usable features.\n4. How Does It Work?\nApply NLP (e.g., negation/context detection), use knowledge graphs for term recognition, and process signals/images with appropriate tools.\n5. What\u0026rsquo;s Next?\nEnhances the patient-feature matrix (Module 4) and improves phenotyping accuracy and completeness (Module 6).\nüß¨ Module 6: Electronic Phenotyping # 1. What\u0026rsquo;s the Problem?\nIdentifying who truly has a disease or condition is challenging using only raw or coded data.\n2. Why Does It Matter?\nMisclassified patients lead to invalid cohorts, incorrect inferences, and flawed clinical decisions or model training.\n3. What\u0026rsquo;s the Core Idea?\nDefine phenotypes using rule-based or probabilistic methods to accurately identify conditions of interest.\n4. How Does It Work?\nUse inclusion/exclusion criteria (rule-based) or train classifiers (probabilistic) with anchors, weak labels, and features from Modules 4‚Äì5.\n5. What\u0026rsquo;s Next?\nEnables reliable cohort creation for clinical trials, observational studies, and AI/ML applications.\n‚öñÔ∏è Module 7: Clinical Data Ethics # 1. What\u0026rsquo;s the Problem?\nUsing patient data without safeguards risks violating privacy, losing trust, and causing harm.\n2. Why Does It Matter?\nUnethical data use can lead to legal issues, exclusion of vulnerable groups, and poor public perception of healthcare AI.\n3. What\u0026rsquo;s the Core Idea?\nApply ethical frameworks like the Belmont Report and Learning Health System to govern data use, consent, and fairness.\n4. How Does It Work?\nEnsure de-identification, obtain proper consent (or waiver), handle return of results thoughtfully, and consider justice in access and outcomes.\n5. What\u0026rsquo;s Next?\nProvides ethical boundaries and practices for applying all previous modules responsibly in real-world systems.\n"},{"id":7,"href":"/ai-workflows/genai/5-day-genai-google-2025/day1_prompt_engineering/","title":"Day 1 ‚Äì Prompt Engineering","section":"5-Day GenAI with Google 2005","content":" Day 1 ‚Äì Prompt Engineering 1. Why Prompt Engineering Matters # We start with the need for controlling LLM behavior. Although everyone can write prompts, crafting high-quality prompts is complex. The model, structure, tone, and context all affect the outcome. Prompt engineering is an iterative process requiring optimization and experimentation.\n‚Üí how do we guide LLMs effectively without retraining them?\n2. How LLMs Predict Text # LLMs are token prediction machines. They predict the next likely token based on previous tokens and training data. Prompt engineering means designing inputs that lead the model toward the desired outputs using this prediction mechanism.\n‚Üí how do prompt structure and context impact token prediction?\n3. Controlling Output Length # Setting the number of output tokens affects cost, latency, and completeness. Shorter outputs don‚Äôt make the model more concise‚Äîthey just truncate the output. Prompts must be adapted accordingly.\n‚Üí how do we engineer prompts to work well with shorter output limits?\n4. Temperature ‚Äì Controlling Randomness # Temperature tunes the creativity vs. determinism of responses. Lower = more deterministic. Higher = more diverse. Temperature = 0 means always selecting the highest-probability token.\n‚Üí what randomness level best matches your use case: precision or creativity?\n5. Top-K vs. Top-P # Top-K selects from the K most likely tokens. Top-P includes tokens whose cumulative probability is under P. Together with temperature, they control diversity. Improper values can lead to repetition loops or incoherence.\n‚Üí what is the optimal balance between relevance and novelty?\n6. Putting Sampling Together # The sampling settings interact:\nTemp=0 overrides others (most probable token only) Top-K=1 behaves similarly (greedy decoding) At extremes, sampling settings may cancel out or be ignored ‚Üí how do we experiment with sampling settings to avoid repetition and improve quality?\n7. Zero-shot Prompting # The simplest form‚Äîjust a task or question without examples. Effective when LLMs are pre-trained well. Clarity in phrasing is key.\n‚Üí how do we design zero-shot prompts that still get structured, accurate answers?\n8. One-shot and Few-shot Prompting # One-shot: One example is provided before the prompt. Few-shot: Multiple examples guide the model‚Äôs pattern recognition. Ideal for steering structure and increasing precision. ‚Üí how many examples are enough for complex or high-variance tasks?\n9. System Prompting # Defines the LLM‚Äôs role and constraints at a high level‚Äîsuch as format, safety rules, or output requirements. It‚Äôs useful to enforce style, format, or structure like JSON outputs.\n‚Üí how can we use system prompts to enforce safety and structured outputs?\n10. Role Prompting # Assigns a persona (e.g., travel guide, teacher). This helps shape tone, depth, and relevance of the response. Adding style (humorous, formal) further guides model behavior.\n‚Üí how do personas influence LLM outputs in nuanced ways?\n11. Contextual Prompting # Injects situational context into the prompt to make responses more accurate. Effective for dynamic environments (e.g., blogs, customer support).\n‚Üí how can contextual prompts adapt to real-time or user-specific tasks?\n12. Step-back Prompting # Starts with a general question to activate background knowledge before solving a specific task. Encourages critical thinking and can reduce bias.\n‚Üí how do we leverage LLMs\u0026rsquo; latent knowledge more strategically?\n13. Chain of Thought (CoT) # LLMs struggle with math and logic unless they break problems into steps. Chain of Thought (CoT) prompting makes the model reason step by step. This adds interpretability, reduces drift across models, and improves answer accuracy.\n‚Üí how can we make reasoning visible to both developers and users?\n14. Self-Consistency # Instead of relying on a single reasoning path, Self-Consistency samples multiple responses (with higher temperature), then picks the majority answer. It increases reliability‚Äîespecially for ambiguous or hard-to-evaluate tasks.\n‚Üí how can we trade off cost for improved reliability in decision-critical tasks?\n15. Tree of Thoughts (ToT) # Generalizes CoT by enabling multiple reasoning paths at once. Like a decision tree, the model explores and evaluates different intermediate steps before selecting the best route. Powerful for complex planning and exploration.\n‚Üí what‚Äôs the best way to structure exploration and backtracking in LLM reasoning?\n16. ReAct (Reason + Act) # ReAct prompts mix reasoning with external tool calls (e.g., Google Search). It creates a loop: Think ‚Üí Act ‚Üí Observe ‚Üí Rethink. This enables LLMs to handle multi-step problems using real-world data or APIs.\n‚Üí how can we design LLMs that interactively use tools and adapt in real time?\n17. Automatic Prompt Engineering (APE) # Prompts are hard to write. APE uses LLMs to generate and refine their own prompts. You ask the model to create N variations of a task prompt, then rank and select the best one based on performance metrics (e.g., BLEU, ROUGE).\nFinal insight: what happens when LLMs become their own prompt engineers‚Äîand how can we guide that process safely?\n18. Prompts for Writing Code # LLMs like Gemini can generate well-documented scripts, e.g., renaming files with Bash. It reduces developer overhead for common tasks. Prompts should include goal, language, and behavior clearly.\n‚Üí how do we craft prompts that result in reusable, safe, and tested code?\n19. Prompts for Explaining Code # LLMs can reverse-engineer logic from code. Useful in team settings or code reviews. Helps onboard new developers or document legacy scripts.\n‚Üí how do we evaluate explanation correctness‚Äîespecially for critical systems?\n20. Prompts for Translating Code # Language models can convert code between languages (e.g., Bash ‚Üí Python). This helps modernize or modularize projects while preserving logic.\n‚Üí what risks emerge in translation‚Äîsyntax, dependencies, or behavior drift?\n21. Prompts for Debugging and Reviewing Code # Prompting LLMs to diagnose bugs or suggest improvements enhances development speed. Common mistakes like undefined functions can be spotted easily.\n‚Üí how do we ensure debugging prompts scale with complex codebases?\n22. Multimodal Prompting # Combines inputs like text, images, and audio. Enables more flexible, human-like interaction. Useful in complex workflows or accessibility tasks.\n‚Üí how do we design for alignment across different input modalities?\n23. Best Practice ‚Äì Provide Examples # Incorporating examples (one-shot or few-shot) within prompts is the most reliable way to guide output. Acts as a template and sets style/tone expectations.\nFinal reflection: prompt engineering is more than writing‚Äîit\u0026rsquo;s designing a user interface for the LLM.\n24. Design with Simplicity # Clear, concise prompts yield better responses. Avoid overloading with context or ambiguous language. Use active verbs and break down complex requests.\n‚Üí how do we optimize for both human and machine comprehension in prompt design?\n25. Be Specific About Output # Specify the desired format, length, tone, and structure to reduce ambiguity and improve relevance. Instructions guide better than vague constraints.\n‚Üí how can we use instructions to improve precision without overconstraining the model?\n26. Use Variables # Abstract prompts using variables (e.g., {city}) to enhance reusability in apps or RAG systems. Helps modularize and scale prompt templates.\n‚Üí how can prompt modularity boost automation and maintainability?\n27. Experiment with Formats and Styles # Vary phrasing‚Äîquestions, statements, or instructions. Try structured outputs (e.g., JSON) and track results for consistency and quality.\n‚Üí how do output formats affect hallucination, readability, and parse-ability?\n28. Work With Schemas # Use structured input/output formats like JSON Schema to guide the model‚Äôs understanding. This enables field-level alignment and supports reasoning over attributes.\n‚Üí how do schemas improve accuracy in structured reasoning tasks like RAG or product gen?\n29. Best Practices for Chain-of-Thought (CoT) # Always place the final answer after reasoning Use temperature=0 for deterministic outputs Separate reasoning from the final output for evals ‚Üí how do we reliably extract and score CoT answers in evaluation pipelines?\n30. Document Prompt Experiments # Track prompt versions, models, sampling settings, and outputs using a table or Google Sheet. Log RAG details and system changes to trace variation.\nFinal principle: prompting is experimental‚Äîtrack what you try, and improve iteratively.\n24. Design with Simplicity # Use concise language and clear goals. Overly complex or vague prompts confuse both the user and the model.\n‚Üí how do we turn messy input into structured guidance for LLMs?\n25. Be Specific About Output # Specificity in instructions (e.g., format, style, length) yields more relevant, focused outputs.\n‚Üí how do we align prompts with precise user needs and formats?\n26. Prefer Instructions Over Constraints # Positive instructions are more intuitive than a list of ‚Äúdon‚Äôts.‚Äù Use constraints when safety or exact formatting is critical.\n‚Üí how do we encourage creativity while staying on target?\n27. Use Variables in Prompts # Dynamic placeholders (e.g., {city}) improve reusability and maintainability‚Äîespecially in production pipelines.\n‚Üí how do we modularize prompt logic for reuse across systems?\n28. Experiment with Input and Output Formats # Try different styles‚Äîquestion, statement, instruction‚Äîand output formats like JSON. JSON helps structure data and reduce hallucinations.\n‚Üí how do we balance human readability with system parsing needs?\n29. Use Schemas for Input and Output # JSON schemas define structure and types‚Äîgreat for grounding LLM understanding and making output usable in applications.\n‚Üí how can we give LLMs structured \u0026ldquo;expectations\u0026rdquo; to reduce drift?\n30. Document Prompt Versions # Track all attempts, model versions, and outcomes in structured logs. Helps with reproducibility, debugging, and future upgrades.\nFinal insight: prompt engineering is iterative design‚Äîevery prompt is a versioned artifact.\nSummary # Prompt types: zero/few-shot, role, system, contextual Reasoning: CoT, ToT, ReAct, Self-consistency, Step-back Automation: APE Code prompting: generate, translate, debug Multimodal and schema-guided prompting Best practices: Evaluation, formatting, variables, and documentation The journey from text to structured reasoning begins with a well-crafted prompt.\n"},{"id":8,"href":"/ai-workflows/data/data-centric-ai/label-errors/","title":"Label Errors","section":"Data-Centric AI (DCAI)","content":" Label Errors Q1: What are label errors and why do they matter? # Label errors: Incorrect labels in training/testing datasets. They cause worse model performance, benchmark misinterpretation, and deployment risks. Q2: What are the types of label noise? # Uniform/Symmetric noise: Random label flipping. Systematic/Asymmetric noise: Certain labels more likely flipped. Instance-dependent noise: Noise depends on input features (out of scope here). Q3: What is Confident Learning (CL)? # A framework to: Find label errors Rank examples by label issue likelihood Learn with noisy labels Characterize noise structure Model-agnostic: uses model-predicted probabilities. Q4: How does CL detect label errors? # Use predicted probabilities + noisy labels. Estimate joint distribution of noisy vs. true labels. Detect off-diagonal entries = label errors. Key techniques: Prune, Count, Rank. Q5: Why is a noise process assumption needed? # To separate model uncertainty (epistemic) and label noise (aleatoric). CL assumes class-conditional noise. Q6: Why not just sort by loss? # Sorting by loss doesn\u0026rsquo;t tell you: Where to cut off How many label errors exist How to automate error finding without human review Q7: How does CL achieve robustness to imperfect predictions? # Prune low-confidence examples Count robustly across examples Rank by predicted probabilities Q8: How does label noise affect real-world ML? # Real-world datasets are not random noise. Deep learning claims about noise robustness often assume unrealistic random noise. Q9: What happens when test sets have label errors? # Benchmark model rankings change. A \u0026ldquo;better\u0026rdquo; model might actually underperform in deployment. Quantifying label errors in test sets is critical. Q10: How can practitioners fix this? # Use corrected test sets. Benchmark using cleaned labels. Tools like cleanlab can automate finding label issues. Q11: Key Takeaways # Confident learning enables data-centric model improvements. Even small label error rates (~3-6%) can destabilize ML benchmarks. ML needs to quantify label noise to ensure real-world reliability. References # Confident Learning Paper (JAIR 2021) Pervasive Label Errors Paper (NeurIPS 2021) Label Errors Website Cleanlab GitHub "},{"id":9,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/summary_m2/","title":"[Summary] Module2: Data Available From Healthcare Systems","section":"C2 Clinical Data","content":" Module2: Data Available From Healthcare Systems # 1 Review of the healthcare system # Q: What is the focus of this module\u0026rsquo;s introduction? # A: To demonstrate how clinical data from the healthcare system can be used to ask and answer meaningful research questions.\nQ: What key topics are introduced in this session? # A:\nCommon data sources in healthcare Types of data generated Systematic inaccuracies in the data Strategies for working with imperfect data Q: How does this connect to the earlier module? # A: It builds on the discussion of research question formulation and data mining workflows by diving into real-world data availability and limitations.\n‚û°Ô∏è What entities exist in the healthcare system and what data do they collect?\n2 Review of key entities and the data they collect # Q: Who are the key entities in the healthcare system that generate data? # A:\nPatients : Generate little data when healthy, but more when they seek treatment. Search Engines : Capture data from patients researching health online. Healthcare Providers : Generate EMR data including diagnoses, test orders, and prescriptions. Pharmacies : Record dispensing data. Pharmaceutical Companies : Design and manufacture drugs. Insurers/Payers : Capture claims and billing data. Q: What kind of data is produced at each step of care? # A: Data are produced at nearly every step ‚Äî from search behavior before treatment, to EMR entries during care, to dispensing records from pharmacies, and payment data from insurers.\n‚û°Ô∏è Who are the actors in the healthcare system and what interests do they have?\n3 Actors with different interests # Q: Who are the main actors in the healthcare system? # A:\nPatients and their families Healthcare professionals Payers (e.g. insurance companies) Government and regulatory agencies Q: What are the interests of these different actors? # A:\nPatients want to stay healthy and recover quickly. Clinicians seek best practices to maximize benefit and minimize harm. Payers and regulators aim for societal cost-effectiveness, fairness, and justice. Q: How do differing interests create conflict? # A: A patient may want to pursue a high-risk treatment for hope, while a clinician may focus on safety, and insurers may prioritize cost-effectiveness ‚Äî leading to tension in care decisions.\n‚û°Ô∏è What are the common data types used in healthcare?\n4 Common data types in Healthcare # Q: What are the major types of healthcare data? # A: Structured and unstructured data.\nQ: What is structured data? # A: Data organized in consistent formats like tables with rows (patients) and columns (attributes such as age, DOB). Missing values are often marked with placeholders like \u0026ldquo;NA\u0026rdquo;.\nQ: What is unstructured data? # A: Data without a uniform format, including clinical notes, images, biomedical signals, and free text.\nQ: Why is this distinction important? # A: Structured data is easier to analyze computationally, while unstructured data requires specialized methods (e.g. NLP, image processing) to extract insights.\n‚û°Ô∏è What are the strengths and weaknesses of observational healthcare data?\n5 Strengths and weaknesses of observational data # Q: What are observational data in healthcare? # A: Data collected during routine clinical care for the primary purpose of delivering care ‚Äî not for research. They\u0026rsquo;re used secondarily for analysis.\nQ: What are the main strengths of observational data? # A:\nLarge scale : Covers millions to billions of records, capturing rare events. Real-world relevance : Reflects actual clinical practices and patient behaviors. Efficiency : Already collected, so no additional data gathering is needed. Q: What are the weaknesses of observational data? # A:\nBias and incompleteness : Not collected with research goals in mind. Lack of standardization : Varies across sites and systems. Potential confounding : Causal inference is difficult without randomized control. ‚û°Ô∏è What are the biases and errors introduced by the healthcare system itself?\n6 Bias and error from the healthcare system perspective # Q: How can data from the healthcare system be biased or inaccurate? # A: Each entity in the healthcare system contributes potential sources of bias, especially due to who seeks care and how care is recorded.\nQ: What is selection bias in this context? # A: It occurs when only certain patients (e.g., those who seek care) are recorded, leaving out healthy individuals or those managing illness at home or outside the system.\nQ: What patient-level factors influence bias? # A: Health literacy, financial situation, insurance coverage, and cultural beliefs can all affect whether and how patients engage with the healthcare system.\n‚û°Ô∏è How do biases and errors affect recorded exposures and outcomes?\n7 Bias and error of exposures and outcomes # Q: What are exposures and outcomes in clinical data analysis? # A:\nExposures are events or conditions that occur to a patient (e.g., diseases, procedures, medications). Outcomes are events or conditions assessed after the exposure (e.g., complications, lab results, costs). Q: Why is distinguishing exposures and outcomes important? # A: It provides a framework for analyzing how prior events influence later results and helps in identifying where bias or error may occur.\nQ: What type of biases can arise in this context? # A: Misclassification, timing errors, or missing data related to either exposures or outcomes can distort analysis and conclusions.\n‚û°Ô∏è How can patient exposure be misclassified in clinical data?\n8 How a patient exposure might be misclassified # Q: How can exposure to medication be misclassified in clinical data? # A: When prescription records don‚Äôt align with actual medication use, such as delays in filling, use of free samples, or interruptions in adherence.\nQ: What example illustrates this misclassification? # A: A doctor gives a 15-day free sample before a prescription is filled. The patient begins treatment immediately, but data may only show the pharmacy fill date, not the true start date.\nQ: Why does this matter? # A: Misclassification of the timing or existence of exposure can distort analyses linking exposures to outcomes, especially for time-sensitive effects.\n‚û°Ô∏è How might a patient outcome be misclassified in clinical records?\n9 How a patient outcome could be misclassified # Q: How can a patient‚Äôs outcome be misclassified in the medical record? # A: Sometimes a diagnosis code is added based on suspicion (e.g., diabetes) before a condition is confirmed, and it may remain even if the diagnosis is later ruled out.\nQ: What strategies help reduce outcome misclassification? # A:\nRequire multiple instances of a diagnosis code. Pair diagnosis codes with procedure codes specific to the condition. Look for treatment or intervention evidence supporting the diagnosis. Q: Why are procedure codes more reliable? # A: Procedures are typically documented only if they were actually performed, adding confirmatory weight to a diagnosis.\n‚û°Ô∏è What are the key sources of electronic medical record data?\n10 Electronic medical record data # Q: What are electronic medical records (EMRs)? # A: EMRs are digital versions of the traditional paper patient charts. They store detailed information collected in clinical settings.\nQ: What types of data are typically stored in EMRs? # A:\nPatient demographics Diagnosis and procedure codes Clinical notes Medication records Imaging and lab test results (Increasingly) genetic test results and wearable device data Q: How are EMRs generated? # A: As a byproduct of routine clinical care and documentation processes in hospitals and clinics.\nQ: Are EMRs and EHRs the same? # A: The terms are often used interchangeably, though technically EHR may imply a broader, longitudinal view across providers.\n‚û°Ô∏è What can we learn from claims data in healthcare systems?\n11 Claims data # Q: What are claims data in healthcare? # A: Claims data are records generated when healthcare providers submit bills to insurers for services rendered.\nQ: What information do claims typically include? # A:\nPatient identifiers Insurance status Diagnosis and procedure codes Requested charges and actual payments Q: How is coding related to billing? # A: Specific codes are used to categorize services for billing. These may differ between clinician entries, hospital submissions, and what insurers reimburse.\nQ: Why are claims data valuable? # A: They include detailed information on costs, utilization, and provider-payer interactions ‚Äî data often missing from EMRs.\n‚û°Ô∏è What data do pharmacies provide, and how is it used?\n12 Pharmacy # Q: What kind of data do pharmacies provide? # A: They document when prescriptions are written, filled, and paid for ‚Äî offering insight into medication access and fulfillment.\nQ: Why is pharmacy data valuable? # A: It goes beyond prescription intent (EMR) to show that a patient actually obtained the medication, which is a step closer to actual use.\nQ: What are limitations of pharmacy data? # A: It doesn\u0026rsquo;t guarantee the patient took the medication ‚Äî only that it was picked up.\nQ: How can pharmacy records be fragmented? # A: Patients may use multiple sources: retail chains (e.g., CVS), mail-order services, and online pharmacies, dispersing data across datasets.\n‚û°Ô∏è What are surveillance datasets and registries, and how are they used?\n13 Surveillance datasets and Registries # Q: What are surveillance datasets and why are they important? # A: They monitor adverse events and side effects of drugs or devices after approval (post-marketing surveillance) to catch safety issues early.\nQ: What are examples of surveillance systems in the U.S.? # A:\nFAERS : FDA Adverse Event Reporting System MAUDE : Manufacturer and User Facility Device Experience database Q: Who uses these datasets and for what purpose? # A: Government agencies like the FDA and CDC use them to track disease outbreaks (e.g. flu, Ebola) and monitor product safety. Local and state agencies often assist.\nQ: What are registries? # A: Registries are organized systems maintained by agencies or societies to collect consistent clinical data on specific conditions, devices, or populations.\n‚û°Ô∏è What are population health datasets and how are they used?\n14 Population health data sets # Q: How do population health datasets differ from patient-centric data? # A: They focus on aggregated health trends, costs, and resource use across populations ‚Äî not on individual patient records.\nQ: What are key examples of U.S. population health datasets? # A:\nNational Inpatient Sample (NIS) : Tracks hospital resource utilization, costs, and outcomes. Medical Expenditure Panel Survey (MEPS) : Surveys patients, providers, and employers on healthcare usage and spending. NHANES : Measures demographic, nutritional, and health variables through national surveys conducted by the CDC. Q: Why are these datasets useful? # A: They offer broad insights into national health patterns, disparities, and costs ‚Äî helping guide public policy and resource planning.\n‚û°Ô∏è How do we assess if a healthcare data source is useful?\n15 A framework to assess if a data source is useful # Q: What questions should you ask to assess a healthcare dataset\u0026rsquo;s usefulness? # A:\nIs there a well-documented data model? Poor or missing documentation makes the data hard to use. What is the data provenance? Understand how and where the data were collected. Are the data accessible and in what form? Consider legal restrictions and costs. What known errors or missingness exist? Evaluate data quality and be prepared to address gaps. Are data standards used (e.g., vocabularies or formats)? Standardization affects interoperability and analysis readiness. Q: Why is this framework important? # A: It helps researchers avoid costly or infeasible data efforts and ensures they can trust and interpret results from the dataset effectively.\n"},{"id":10,"href":"/ai-workflows/data/data-centric-ai/advanced-confident-learning/","title":"Advanced Confident Learning and Applications for GenAI","section":"Data-Centric AI (DCAI)","content":" Advanced Confident Learning and Applications for GenAI Q1: What is the main focus of this lecture? # Advanced Confident Learning (CL): Theory, methods, and applications, especially for Generative AI (images, text). Q2: How does Confident Learning (CL) work at its core? # Inputs: Noisy labels and predicted probabilities. Core idea: Find self-confidence thresholds per class to detect label errors. Estimate if an example is an error, correct label, or outlier. Q3: What is the quick intuition behind CL? # Off-diagonal entries in the predicted-vs-true label matrix reveal label errors. Q4: What makes CL robust to noise? # Prune principle: Remove low-confidence errors before training. Count principle: Use counts rather than raw outputs. Rank principle: Rank by model confidence, not rely on probabilities. Q5: How is CL better than just loss adjustment techniques? # CL avoids error propagation common in reweighting methods. Robust to stochastic/noisy outputs from real-world models. Q6: What is the theoretical guarantee of CL? # As long as correct labels dominate wrong ones in a class, CL can exactly find errors ‚Äî even if model probabilities are imperfect (up to ~33% wrong). Q7: Why does label noise in test sets matter? # 3.4% of labels in popular ML test sets are wrong. Small label error rates (~6%) can change model rankings drastically. Benchmark results can be misleading without corrected test sets. Q8: How to fix label errors in test sets? # Use majority consensus among reviewers to correct labels. Prune uncertain/multi-label examples. Q9: How is CL applied to Generative AI models? # Before training: Clean training data to avoid issues in model generation. After generation: Run CL on generated data (e.g., images/text) to remove/fix errors. Q10: Example use cases for CL in Generative AI? # Scenario Application Image generation (e.g., DALL-E) Improve datasets pre/post generation LLM outputs (e.g., GPT-4) Post-process outputs for better quality RAG (Retrieval-Augmented Generation) Clean retrieved answers Trustworthy Language Models (TLM) Attach confidence scores to outputs Q11: Final Takeaways # CL is model-agnostic. Improves reliability of both traditional ML models and Generative AI. One line of code to apply using cleanlab. References # Confident Learning: GitHub Repository Label Errors Website Trustworthy Language Models (TLM) Tutorial Related Papers: (GPT-3), (Northcutt et al., Pervasive Label Errors, 2021) "},{"id":11,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/","title":"C3 ML Healthcare","section":"AI in Healthcare","content":" üìò Course 3: Fundamentals of ML for Healthcare # [ToC] Course 3 [Summary] Module 3: Concepts and Principles of ML in Healthcare [Summary] Module 4: Evaluation and Metrics for ML in Healthcare [Summary] Module 5: Strategies and Challenges in ML for Healthcare [Summary] Module 6: Best Practices, Terms, and Launching Your ML Journey [Summary] Module 7: Foundation Models Case Study: The Hidden Danger of Correlation in Healthcare AI Categories of Machine Learning Applications in Healthcare Data Quality, Labeling, and Weak Supervision in Clinical ML Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations Foundation Models for Healthcare Healthcare Use Cases for Non-textual Unstructured Data Healthcare Use Cases for Text Data How Foundation Models Work Output-Action Pairing (OAP) Framework in Healthcare Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare ü§ñ Module 1: Why Machine Learning in Healthcare? # 1. What‚Äôs the Problem?\nUnderstanding how ML fits into the healthcare ecosystem and why traditional models are insufficient.\n2. Why Does It Matter?\nML has the potential to improve diagnosis, patient care, and reduce costs, but it also raises ethical and technical concerns.\n3. What‚Äôs the Core Idea?\nMachine learning leverages data to improve healthcare predictions, enabled by access to digital health data and modern computing.\n4. How Does It Work?\nML systems learn mappings between inputs and outputs from data, using statistical methods rather than hard-coded rules.\n5. What‚Äôs Next?\nExplore the basic types and terminology of ML, including supervised and unsupervised learning.\nüìò Module 2: Concepts and Principles of ML in Healthcare Part 1 # 1. What‚Äôs the Problem?\nClarifying foundational ML terms and how ML differs from traditional programming.\n2. Why Does It Matter?\nFoundational understanding is necessary for applying ML models correctly in clinical settings.\n3. What‚Äôs the Core Idea?\nML learns functions that map inputs to outputs using labeled (or unlabeled) data. Supervised learning uses labels; unsupervised does not.\n4. How Does It Work?\nML involves preprocessing data, training models, validating predictions, and testing accuracy with structured data splits.\n5. What‚Äôs Next?\nDelve into deep learning, neural network architectures, and their applications in healthcare.\nüß† Module 3: Concepts and Principles of ML in Healthcare Part 2 # 1. What‚Äôs the Problem?\nApplying and interpreting complex ML models like deep neural networks in healthcare.\n2. Why Does It Matter?\nNeural networks enable breakthroughs in imaging and text analysis but require understanding to avoid misuse.\n3. What‚Äôs the Core Idea?\nDeep learning uses layers of neurons to learn complex mappings. CNNs are suited for images; RNNs and Transformers for sequences.\n4. How Does It Work?\nTraining uses backpropagation and loss optimization. Model performance is evaluated with metrics like AUROC, accuracy, and precision.\n5. What‚Äôs Next?\nEvaluate ML models using statistical metrics, assess overfitting, and explore model generalizability.\nüìä Module 4: Evaluation and Metrics for Machine Learning in Healthcare # 1. What‚Äôs the Problem?\nEnsuring ML models are reliable and generalizable before clinical deployment.\n2. Why Does It Matter?\nPoorly evaluated models may be unsafe or ineffective in high-stakes healthcare settings.\n3. What‚Äôs the Core Idea?\nEvaluation includes accuracy, AUROC, precision-recall, and more. Proper data splits and validation strategies are critical.\n4. How Does It Work?\nUse learning curves, loss plots, cross-validation, and hyperparameter tuning to evaluate models. Choose appropriate thresholds.\n5. What‚Äôs Next?\nUnderstand practical barriers and strategies for deploying ML in real clinical environments.\nüõ†Ô∏è Module 5: Strategies and Challenges in Machine Learning in Healthcare # 1. What‚Äôs the Problem?\nDealing with real-world limitations like data bias, label noise, interpretability, and clinical relevance.\n2. Why Does It Matter?\nModels that lack robustness or interpretability may fail in practice or perpetuate health disparities.\n3. What‚Äôs the Core Idea?\nTactics include regularization, domain-specific feature engineering, human-centered design, and sensitivity to healthcare context.\n4. How Does It Work?\nApply dropout, saliency mapping, ensemble learning, and transparent reporting. Collaborate with clinicians for contextual validation.\n5. What‚Äôs Next?\nBuild multidisciplinary teams and prepare for real-world deployment including ethical review and monitoring.\nüöÄ Module 6: Best Practices, Teams, and Launching Your ML Journey # 1. What‚Äôs the Problem?\nBridging the gap between ML research and real-world clinical implementation.\n2. Why Does It Matter?\nSuccess depends on team composition, ethics, data stewardship, and designing for human-AI interaction.\n3. What‚Äôs the Core Idea?\nUse frameworks like Output-Action Pairing to define measurable goals. Involve stakeholders throughout the development cycle.\n4. How Does It Work?\nForm teams with technical, clinical, ethical, and operational expertise. Start small, iterate, and evaluate continuously.\n5. What‚Äôs Next?\nIdentify your project focus, assemble collaborators, and begin experimenting responsibly.\n"},{"id":12,"href":"/ai-workflows/genai/5-day-genai-google-2025/day2_embeddings_vectordb/","title":"Day 2 ‚Äì Embeddings \u0026 Vector Databases","section":"5-Day GenAI with Google 2005","content":" Day 2 ‚Äì Embeddings \u0026 Vector Databases 1. Why Embeddings? # We begin with the core problem of representing diverse data types. Images, text, audio, and structured data all need to be compared, retrieved, and clustered. Embeddings map these into a shared vector space where similarity can be computed numerically.\n‚Üí how can we measure and preserve semantic meaning across different data types?\n2. Mapping Data to Vector Space # Embeddings reduce dimensionality while preserving meaning. For example, just like latitude and longitude embed Earth‚Äôs surface into 2D coordinates, BERT embeds text into 768D space. Distances represent semantic similarity.\n‚Üí how do different embedding models impact representation fidelity and downstream performance?\n3. Key Applications # Embeddings power:\nSearch (e.g., RAG, internet-scale) Recommendations Fraud detection Multimodal integration (e.g., text + image) ‚Üí how do we design joint embeddings for multi-modal tasks?\n4. Quality Metrics # Evaluation focuses on how well embeddings retrieve similar items:\nPrecision@k: Are top results relevant? Recall@k: Do we get all relevant items? nDCG: Are the most relevant ranked highest? ‚Üí how can evaluation help us improve and select embedding models for specific applications?\n5. RAG and Semantic Search # A standard setup involves:\nEmbedding documents and queries via a dual encoder Storing doc embeddings in a vector DB (e.g., Faiss) At query time, embedding the question and retrieving nearest neighbors Feeding results into an LLM for synthesis ‚Üí how does the embedding model choice impact the quality of LLM-augmented answers?\n6. Operational Considerations # Embedding models keep improving (e.g., BEIR from 10.6 to 55.7). Choose platforms that:\nAbstract away model versioning Enable easy re-evaluation Provide upgrade paths (e.g., Vertex AI APIs) ‚Üí how do we future-proof embedding systems in production?\n7. Text Embedding Lifecycle # From raw strings to embedded vectors:\nTokenization ‚Üí Token IDs ‚Üí Optional one-hot encoding ‚Üí Dense embeddings Traditional one-hot lacks semantics, embeddings retain contextual meaning ‚Üí how does token context influence the quality of embeddings?\n8. Word Embeddings: GloVe, Word2Vec, SWIVEL # Early methods:\nWord2Vec (CBOW, Skip-Gram): Context windows define meaning GloVe: Combines global + local word statistics using matrix factorization SWIVEL: Fast training, handles rare terms, parallelizable ‚Üí are static embeddings enough, or do we need context-aware representations?\n9. Shallow and Deep Models # Two major paradigms:\nBoW Models (TF-IDF, LSA, LDA): Sparse, easy to compute but lack context Doc2Vec: Introduces a learned paragraph vector ‚Üí how do we encode long-range relationships and context in documents?\n10. BERT and Beyond # BERT revolutionized document embedding with:\nDeep bi-directional transformers Pretraining on masked tokens Next-sentence prediction It powers models like Sentence-BERT, SimCSE, E5, and now Gemini-based embeddings. ‚Üí what‚Äôs the trade-off between compute cost and performance in deep embeddings?\n11. Images and Multimodal Representations # Image embeddings from CNNs or ViTs (e.g., EfficientNet) Multimodal models (e.g., ColPali) map text + image into a shared space Enables querying images via text without OCR Closing this section: **what are the infrastructure needs to support scalable multimodal embedding workflows?\n12. Embeddings for Structured Data # Use dimensionality reduction (e.g., PCA) or learned embeddings Enable anomaly detection or classification with fewer labeled examples Especially useful when labeled data is scarce ‚Üí how can we compress structured data while retaining signal?\n13. User-Item \u0026amp; Graph Embeddings # Embed users and items into the same space for recommender systems Graph embeddings (e.g., Node2Vec, DeepWalk) capture node relationships Useful for classification, clustering, and link prediction ‚Üí how do we preserve both entity and relational meaning in embeddings?\n14. Dual Encoder \u0026amp; Contrastive Loss # Most embeddings today use dual encoders (e.g., query/doc or text/image towers) Trained with contrastive loss to pull positives close, push negatives away Often initialized from large foundation models (e.g., BERT, Gemini) ‚Üí how do we balance generalization vs. task-specific fine-tuning?\n15. Vector vs. Keyword Search # Keyword search fails for synonyms and semantic variants Vector search embeds documents and queries, enabling ‚Äúmeaning-based‚Äù retrieval Similarity measured via cosine similarity, dot product, or Euclidean distance ‚Üí what metric and database architecture optimize for your use case?\n16. Efficient Nearest Neighbor Techniques # Brute force is O(N)‚Äînot viable at scale LSH hashes similar vectors into the same bucket Tree-based methods (KD-tree, Ball-tree) work for low dimensions HNSW and ScaNN handle large-scale, high-dimensional spaces efficiently ‚Üí how can we trade off speed vs. accuracy using ANN techniques?\n17. What Are Vector Databases? # Built specifically to index and search embeddings Combine ANN search (e.g., ScaNN, HNSW) with metadata filtering Support hybrid search (semantic + keyword) with pre- and post-filtering ‚Üí how do we ensure low-latency, high-recall vector search at scale?\n18. Operational Considerations # Embeddings evolve over time‚Äîupdates may be costly Combine vector + keyword search for literal queries (e.g., IDs) Choose vector DBs based on workload (e.g., AlloyDB for OLTP, BigQuery for OLAP) ‚Üí how do we manage model/version drift and storage efficiency?\n19. Core Use Cases # Embeddings power:\nSearch \u0026amp; retrieval Semantic similarity \u0026amp; deduplication Recommendations Clustering \u0026amp; anomaly detection Few-shot classification Retrieval Augmented Generation (RAG) ‚Üí how do embeddings improve relevance and trust in LLM outputs?\n20. Retrieval-Augmented Generation (RAG) # RAG improves factual grounding and reduces hallucinations Retrieves documents ‚Üí augments prompt ‚Üí generates answer Return sources for transparency and human/LLM coherence check ‚Üí how do we design RAG workflows for auditability and safety?\n21. Key Takeaways # Choose models and vector stores based on data, latency, cost, and security needs Use ScaNN or HNSW for scalable ANN Use hybrid filtering to improve search accuracy RAG is critical for grounded LLMs Closing insight: Embeddings + ANN + RAG form the foundation of trustworthy, scalable, semantic applications.\n"},{"id":13,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m3/","title":"[Summary] Module 3: Concepts and Principles of ML in Healthcare","section":"C3 ML Healthcare","content":" Module 3: Concepts and Principles of ML in Healthcare # 1 Introduction to Deep Learning and Neural Networks # Q1: Why are neural networks considered a turning point in machine learning? # Neural networks mark a major departure from traditional ML models because they enable much deeper interactions between features and parameters. Unlike models like logistic regression or decision trees, neural networks‚Äîespecially deep ones‚Äîorganize parameters in layers, allowing complex feature transformations.\nTraditional ML: Parameters interact directly with input features. Neural networks: Parameters are arranged in layers; outputs of one layer become inputs to the next. Result: Increased expressive power and ability to model complex patterns. ‚û°Ô∏è To understand what truly sets deep learning apart, we need to explore the unique features of deep neural networks themselves.\nQ2: What makes deep learning different from traditional models? # Deep learning models, also called deep neural networks, often involve millions or even billions of parameters across multiple layers. This structure allows:\nHierarchical representation of data (low-level to high-level features). Repeated multiplication and addition of feature weights across layers. The final output is shaped by a sequence of transformations rather than a direct mapping. ‚û°Ô∏è These layered transformations contribute to the overall power of deep models‚Äîso how exactly do these layers work to increase complexity?\nQ3: How do layers in a neural network contribute to model complexity? # Each layer in a neural network captures increasingly abstract representations of the data:\nEarly layers might detect edges or basic patterns in an image. Middle layers combine these into shapes or motifs. Deeper layers can represent complex concepts like organs or specific pathologies in medical images. This layering increases the non-linearity and representational power of the model.\n‚û°Ô∏è To better grasp the idea behind these computational units, it helps to look at their biological inspiration.\nQ4: What are some biological inspirations behind neural networks? # The structure of neural networks was inspired by the human brain:\nEach neuron in a network is a mathematical function mimicking a brain cell. Neurons process inputs and produce outputs that can be fed into other neurons. This setup enables distributed computation, similar to how the brain processes information. ‚û°Ô∏è With such a powerful computational design, what makes neural networks particularly valuable in healthcare?\nQ5: Why are neural networks especially promising for healthcare applications? # Healthcare data is complex and high-dimensional‚Äîfrom imaging to text (EHRs), genomics, and time-series data. Deep learning shines in this space because:\nIt doesn\u0026rsquo;t need hand-engineered features. It can learn representations directly from raw data. It\u0026rsquo;s particularly powerful in image analysis, speech recognition, and clinical text processing. 2 Deep Learning and Neural Networks # Q1: What does the typical training loop of a neural network look like? # The training loop consists of iterative steps to improve model performance:\nStep 1: Pass each training sample through the model to generate predictions. Step 2: Compute a loss value to quantify prediction error. Step 3: Update model parameters using optimization to reduce the loss. This process is repeated over multiple epochs, with each full pass through the training data called an epoch. ‚û°Ô∏è Once training is underway, how do we ensure the model isn\u0026rsquo;t just memorizing the data?\nQ2: How do we validate whether the model is generalizing well? # Model evaluation is typically done on a validation dataset, which the model hasn\u0026rsquo;t seen during training.\nAfter each epoch (or a few), we evaluate the model‚Äôs performance on this set. This helps detect overfitting, where a model performs well on training data but poorly on unseen data. Generalization is key in healthcare to ensure predictions work on real-world, diverse patient populations. ‚û°Ô∏è Speaking of overfitting, one factor that contributes to this is the sheer number of parameters in neural networks.\nQ3: Why are deep learning models prone to overfitting? # Deep neural networks often contain millions of parameters, which gives them immense capacity to:\nMemorize training data rather than learning general patterns. Fit even random noise if not properly regularized. This is why data quantity and quality, as well as regularization strategies, are critical. ‚û°Ô∏è But what\u0026rsquo;s the actual structure of these models, and how do they transform data layer by layer?\nQ4: What happens within each layer of a neural network during computation? # Each layer performs a mathematical transformation:\nTakes input (either original features or previous layer output), Applies weighted summation and non-linear activation functions, Passes output to the next layer. This sequence of operations allows the model to build up increasingly abstract representations of the data.\n‚û°Ô∏è To build a solid foundation in understanding training, we need to connect this to how data and loss flow through the model.\nQ5: What is backpropagation and how does it optimize the model? # Backpropagation is a core algorithm for training neural networks:\nIt calculates how the loss changes with respect to each model parameter. These gradients are then used to update the parameters using an optimizer like Stochastic Gradient Descent (SGD). This iterative process enables the model to gradually learn better predictions. 3 Cross Entropy Loss # Q1: What is the purpose of a loss function in machine learning? # A loss function quantifies how far off a model\u0026rsquo;s predictions are from the actual labels. It\u0026rsquo;s a numerical signal used to update model parameters during training:\nLower loss = better prediction performance. The loss guides the optimization process. Without it, the model has no sense of how to improve. ‚û°Ô∏è For classification tasks, what specific loss function is widely used and why?\nQ2: What is cross-entropy loss and why is it used in classification? # Cross-entropy loss measures the dissimilarity between the predicted probability distribution and the true distribution (the one-hot encoded label).\nEspecially useful in multi-class classification. It penalizes wrong, confident predictions more heavily than uncertain ones. Helps push the model to make confident and correct predictions. ‚û°Ô∏è What does this loss look like mathematically and how is it interpreted?\nQ3: How is cross-entropy loss computed mathematically? # For a single class, the loss is defined as:\n\\[ L = -\\log(p) \\] Where p is the predicted probability for the true class. In general:\n\\[ L = -\\sum y_i \\log(p_i) \\] Where:\n( y_i ) is 1 for the correct class, 0 otherwise. ( p_i ) is the predicted probability for class ( i ). ‚û°Ô∏è Since the model outputs raw scores, how are these converted into probabilities?\nQ4: How does the softmax function turn logits into probabilities? # The softmax function transforms the model‚Äôs output scores (logits) into a probability distribution across classes:\n\\[ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} \\] Ensures the outputs are positive and sum to 1. Prepares predictions for comparison with actual labels using cross-entropy. ‚û°Ô∏è How does cross-entropy loss impact model training?\nQ5: How does cross-entropy guide parameter updates in training? # During backpropagation, gradients of the cross-entropy loss with respect to parameters are computed. These gradients are used to update weights so predictions align better with true labels. As training progresses, cross-entropy loss typically decreases, signaling improved classification. 4 Gradient Descent # Q1: Why is optimization necessary in training neural networks? # Optimization is the process that adjusts model parameters to minimize prediction error:\nNeural networks are trained by minimizing a loss function. The model learns by iteratively updating parameters to reduce this loss. Effective optimization is crucial for learning accurate patterns from data. ‚û°Ô∏è What specific algorithm is most commonly used to perform this optimization?\nQ2: What is gradient descent and how does it work? # Gradient descent is an algorithm used to minimize a function by moving in the direction of the steepest descent:\nIt calculates the gradient (slope) of the loss function with respect to each parameter. Parameters are updated by subtracting a portion (the learning rate) of the gradient. This process continues until the model reaches a local minimum. ‚û°Ô∏è How do we determine how big of a step to take in each update?\nQ3: What role does the learning rate play in gradient descent? # The learning rate controls how much the model updates its parameters in response to the calculated gradient:\nToo high: can overshoot and destabilize training. Too low: training can be very slow or get stuck in a poor local minimum. It\u0026rsquo;s often treated as a hyperparameter that must be tuned carefully. ‚û°Ô∏è Are there variations of gradient descent that address real-world training challenges?\nQ4: What are the common variants of gradient descent? # There are three main types:\nBatch Gradient Descent: Uses the entire training set for each update‚Äîslow but stable. Stochastic Gradient Descent (SGD): Updates using a single data point‚Äîfaster but noisier. Mini-batch Gradient Descent: A compromise that uses a small batch of data‚Äîefficient and commonly used in practice. ‚û°Ô∏è Beyond variants, can the algorithm adapt during training for better performance?\nQ5: What are some adaptive optimization methods beyond basic gradient descent? # Advanced optimizers improve training by adjusting learning rates automatically:\nMomentum: Adds a fraction of the previous update to the current one to smooth progress. RMSProp: Scales updates by a moving average of recent gradients. Adam: Combines momentum and RMSProp for adaptive, robust performance‚Äîpopular in practice. 9 Commonly Used and Advanced Neural Network Architectures # Q1: Why explore multiple neural network architectures in healthcare? # Different architectures are designed to solve specific problems:\nStandard models may not perform well on complex or domain-specific tasks. Specialized architectures improve performance, interpretability, and training speed. Understanding these models is key to designing solutions for clinical settings.\n‚û°Ô∏è What are some commonly used image-based architectures?\nQ2: What are ResNet and DenseNet, and what problems do they solve? # ResNet: Introduces residual connections to skip layers and prevent vanishing gradients. DenseNet: Connects each layer to every other layer to encourage feature reuse. Both architectures improve training of very deep networks, commonly used in radiology and pathology.\n‚û°Ô∏è Are there architectures tailored for medical image segmentation?\nQ3: What is U-Net and why is it useful in healthcare? # U-Net is designed for semantic segmentation, particularly in biomedical imaging:\nEncoder-decoder structure with skip connections. Captures both local and global context. Used for tasks like tumor segmentation, organ delineation, and cell counting. ‚û°Ô∏è Beyond visual data, how do we model structured or complex input spaces?\nQ4: What are Autoencoders and what role do they play? # Autoencoders are unsupervised neural networks that learn to reconstruct their input:\nUseful for dimensionality reduction, denoising, and anomaly detection. In healthcare: Identify rare diseases or compress high-dimensional patient data. Latent representations can be used as features in other models. ‚û°Ô∏è What about generating new data or simulating clinical scenarios?\nQ5: What are GANs and how are they applied in clinical ML? # Generative Adversarial Networks (GANs) consist of a generator and discriminator:\nUsed to generate synthetic but realistic data (e.g., images, waveforms). Helps with data augmentation, especially in rare disease settings. Also used in privacy-preserving machine learning and image translation (e.g., CT to MRI). "},{"id":14,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/summary_m3/","title":"[Summary] Module3: Representing Time Timing Events For Clinical Data Mining","section":"C2 Clinical Data","content":" Module3: Representing Time Timing Events For Clinical Data Mining # 1 Time, timelines, timescales and representations of time # Q: Why is it useful to place patient events on a timeline? # A: Timelines integrate diverse patient data sources, helping visualize when each event occurred, enabling analysis of sequence and duration.\nQ: What are two key reasons time matters in healthcare data? # A:\nPatient age: Impacts diagnosis, treatment decisions, metabolism, and insurance access. Event order: Helps infer causality ‚Äî exposures should precede outcomes. Q: How do timescales vary in medical questions? # A: Medical events can span milliseconds (e.g., EKG signals), days (e.g., symptom onset), or decades (e.g., chronic disease progression), requiring careful scale selection.\n‚û°Ô∏è How do we choose relevant units of time in clinical analysis?\n2 Timescale: Choosing the relevant units of time # Q: Why is timescale selection important in healthcare data? # A: Because relevant time units in clinical contexts can range from milliseconds to decades, depending on the nature of disease, technology, and healthcare system organization.\nQ: What factors influence the appropriate timescale? # A:\nThe biological process (e.g., acute vs chronic disease) Measurement resolution of available technology Clinical workflow and timing of interventions Q: How can timescales vary in practice? # A: Heart rate variability may be tracked in milliseconds, while cancer progression may be observed over years or decades.\n‚û°Ô∏è What influences the choice of timescale in a clinical study?\n3 What affects the timescale # Q: What determines the appropriate timescale in clinical analysis? # A: Two main factors:\nThe research question being asked The type of data being analyzed Q: How do data types influence timescale? # A:\nLab tests may be relevant over days to weeks. Diagnoses may span days to a lifetime. Procedures could have immediate or long-term effects. Q: How do disease types affect timescale? # A:\nAcute diseases (e.g., flu) involve short timelines. Chronic diseases (e.g., diabetes) require long-term tracking. Q: Why is this interaction important? # A: It informs feature selection, granularity, and the scope of data needed for analysis.\n‚û°Ô∏è How is time represented in clinical datasets?\n4 Representation of time # Q: Why is time representation important in healthcare data? # A: Because clinical timelines must eventually be converted into a structured format ‚Äî typically a patient-feature matrix ‚Äî for analysis.\nQ: What is a patient-feature matrix? # A: A table where each row represents a patient, and each column captures a feature (e.g., diagnosis, lab result, blood pressure).\nQ: Where does time representation come into play? # A: It influences how temporal data from a patient\u0026rsquo;s timeline are encoded into the matrix ‚Äî requiring different formats and strategies depending on the use case.\n‚û°Ô∏è How do time series differ from non-time series data in clinical contexts?\n5 Time series and non-time series data # Q: What is a time series in healthcare data? # A: A set of measurements sampled at regular intervals, usually of the same type (e.g., continuous EKG signals).\nQ: Where are time series especially common in clinical settings? # A: In intensive care units (ICUs), where patient vitals are continuously monitored via sensors.\nQ: What kind of methods are used to analyze time series? # A: Techniques from signal processing and electrical engineering.\nQ: How is non-time series data different? # A: Most clinical data are sampled irregularly, based on clinical need (e.g., labs, vitals), not clock time ‚Äî requiring different representation methods.\n‚û°Ô∏è How is the order of events captured and why does it matter?\n6 Order of events # Q: Why is the order of clinical events important? # A: It allows researchers to reason about relationships between conditions, treatments, and outcomes (e.g., \u0026ldquo;Did condition A precede condition B?\u0026rdquo;).\nQ: What makes reasoning about event order complex? # A: When events span time intervals (e.g., chronic illnesses), we must consider overlaps and relative start/end times ‚Äî not just simple time points.\nQ: What are different ways to interpret \u0026lsquo;A before B\u0026rsquo;? # A:\nA ends before B starts (no overlap) A starts before B starts (may overlap) A and B occur simultaneously (partial or full overlap) ‚û°Ô∏è How is time represented implicitly in healthcare data?\n7 Implicit representations of time # Q: What is an implicit representation of time in clinical data? # A: It involves ignoring exact timestamps and instead summarizing events over defined intervals (e.g., event counts in time bins).\nQ: What is binning in this context? # A: Dividing the patient timeline into intervals (bins) and counting occurrences of events within each bin. These counts become features in the analysis matrix.\nQ: What are key decisions in binning? # A:\nNumber and size of bins Interval granularity based on the clinical question‚Äôs timescale Whether to treat each bin as a separate feature or summarize them Q: What is the benefit of implicit time representation? # A: It simplifies complex event timelines into structured, analyzable data while preserving temporal context.\n‚û°Ô∏è What are the different ways to place data into time bins?\n8 Different ways to put data in bins # Q: What are common ways to summarize data within time bins? # A:\nCount of events Binary indicators (e.g., presence vs. absence) Aggregates like average, maximum, or most recent value Q: How do you choose the binning method? # A: It depends on the clinical item being measured and the nature of the research question.\nQ: Can you give a practical example? # A: For monitoring diabetes, the HBA1C lab test reflects average glucose levels over months. Thus, a single value may suffice instead of multiple timestamped entries.\n‚û°Ô∏è How do we consider the timing of exposures and outcomes in analysis?\n9 Timing of exposures and outcomes # Q: What is a cohort in clinical data analysis? # A: A group of patients meeting certain inclusion criteria, typically based on a shared exposure (e.g., condition, drug, or procedure).\nQ: What qualifies as an exposure? # A: Any condition or event that might affect the patient ‚Äî such as diseases, medications, procedures, or behaviors like drinking coffee.\nQ: What qualifies as an outcome? # A: Any event that happens after the exposure and is of interest ‚Äî such as complications, recovery, cost, or survival time.\nQ: Why is timing crucial in analyzing exposures and outcomes? # A: To establish a meaningful temporal relationship and investigate associations (or causality), it‚Äôs essential to ensure exposures precede outcomes and to define observation windows accurately.\n‚û°Ô∏è Why are clinical processes considered non-stationary over time?\n10 Clinical processes are non-stationary # Q: What does it mean for clinical data to be non-stationary? # A: It means that the distributions of data ‚Äî and the associations within them ‚Äî change over time due to evolving clinical practices, medications, coding systems, and care standards.\nQ: What is a stationary vs. non-stationary process? # A:\nStationary: Data distributions remain consistent over time. Non-stationary: Data distributions (and thus patterns and associations) change over time. Q: Why does non-stationarity matter in clinical data mining? # A: Because models trained on past data may become invalid as treatments, diagnostics, and data collection methods evolve. Analysts must carefully consider data timeframes and changes in system behavior.\n"},{"id":15,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/summary_m4/","title":"[Summary] Module4 : Creating Analysis Ready Dataset from Patient Timelines","section":"C2 Clinical Data","content":" Module4 : Creating Analysis Ready Dataset from Patient Timelines # 1 Turning clinical data into something you can analyze # Q: What is the main objective of this module? # A: To explain how to convert raw clinical data into a patient-feature matrix suitable for analysis and answering research questions.\nQ: What is a patient-feature matrix? # A: A structured table where each row represents a patient and each column represents a clinical feature or measurement.\nQ: What topics will this module cover? # A:\nChoosing and extracting features Managing too many or missing features Understanding how data transformations affect analysis outcomes ‚û°Ô∏è How do we define the unit of analysis in clinical datasets?\n2 Defining the unit of analysis # Q: What is the typical unit of analysis in clinical data studies? # A: The patient ‚Äî with each row in the data table representing one patient and each column a feature about them.\nQ: Are there other valid units of analysis? # A: Yes. Depending on the question, units can be drug-disease pairs, visits, procedures, or other clinical events.\nQ: Can you give an example of an alternative unit? # A: For studying off-label drug use, the unit might be a drug-disease pair, with features describing co-mentions, sequencing, and usage frequency.\n‚û°Ô∏è How do we use features and track their presence in patient data?\n3 Using features and the presence of features # Q: Should we use all possible features extracted from clinical data? # A: Generally yes ‚Äî start with a full feature set, unless constrained by computational limits or privacy considerations.\nQ: What are some reasons to remove features? # A:\nResource limitations Low predictive value Sensitivity concerns (e.g., HIV status) Q: How can presence or absence of data itself become a feature? # A: Metadata such as the act of ordering a test (regardless of result) can indicate clinical concern and may be predictive on its own.\nQ: What role does metadata play in feature design? # A: It can be highly informative, capturing clinical behavior that indirectly reflects patient status.\n‚û°Ô∏è How do we create features from structured clinical sources?\n4 How to create features from structured sources # Q: What are structured vs. unstructured healthcare data? # A:\nStructured: Organized in tables with rows/columns (e.g., lab results, diagnosis codes) Unstructured: Free text, images, or signals Q: Why are structured sources important? # A: They are the most readily available and easiest to transform into features for analysis.\nQ: What are the main steps in using structured data? # A:\nAccessing the database Querying with SQL Joining tables using patient IDs Standardizing and reshaping features Handling missing or excessive features Optionally constructing new ones ‚û°Ô∏è Why is standardizing features important and how is it done?\n5 Standardizing features # Q: What does it mean to standardize features? # A: Transforming feature values into a common numerical scale, often called normalization.\nQ: Why is standardization important? # A: It prevents features with large ranges from dominating distance-based or scale-sensitive algorithms in downstream analysis.\nQ: What are common standardization techniques? # A:\nMin-max scaling: Rescales values to a range (e.g., 0 to 1) Z-score normalization: Transforms values to have mean 0 and standard deviation 1 ‚û°Ô∏è How do we deal with having too many features?\n6 Dealing with too many features # Q: Why might you avoid using all available features in clinical data? # A: While comprehensive data is ideal, practical concerns can necessitate reducing features.\nQ: What are reasons to reduce the number of features? # A:\nIrrelevance: Some features offer no useful signal (e.g., record access frequency) Missingness: Some are missing in most patients Sparsity: Too many features missing in a given patient Redundancy: Highly correlated features interfere with some analyses Computational cost: More features = slower analysis Privacy: More features increase re-identification risk Q: How can this affect patient privacy? # A: A rich set of features can unintentionally reveal a patient‚Äôs identity.\n‚û°Ô∏è Where do missing values in clinical data originate?\n7 The origins of missing values # Q: Why do missing values occur in clinical datasets? # A: Missing values often result from converting complex patient timelines into a simplified matrix format where not all features are applicable or expected.\nQ: How is missingness different in routine care vs. prospective studies? # A:\nIn prospective studies, missing data usually implies an error or oversight. In routine care, absence might mean the data was never meant to be recorded. Q: What are three possible reasons a value is missing? # A:\nIt should have existed, but wasn‚Äôt recorded. It\u0026rsquo;s absent due to matrix transformation ‚Äî not clinically expected. Its absence has meaning ‚Äî like no diagnosis implies a negative finding. ‚û°Ô∏è How do we handle missing values in practice?\n8 Dealing with missing values # Q: What is imputation in clinical data analysis? # A: Imputation is a technique used to fill in missing data using predictions based on other available information.\nQ: What is column-mean imputation? # A: A simple method where missing values are replaced with the average of non-missing values in the same column.\nQ: Why might column-mean imputation be unsuitable in medicine? # A: Because one patient\u0026rsquo;s lab value isn\u0026rsquo;t necessarily informative for another ‚Äî clinical data often lack this kind of inter-patient correlation.\nQ: What is a better alternative in clinical contexts? # A: Use within-patient imputation: infer a missing value from other known values of that same patient using correlated features.\n‚û°Ô∏è What are the recommended practices for handling missing values?\n9 Summary recommendations for missing values # Q: Should you always impute missing values in clinical datasets? # A: Not necessarily ‚Äî it depends on the extent and distribution of missingness. Expert guidance is recommended.\nQ: When is imputation appropriate? # A: If most values are present and only a few are missing, imputation is usually a good choice.\nQ: When should you drop a variable? # A: If most patients are missing that variable, it\u0026rsquo;s better to exclude it rather than impute unreliable values.\nQ: What about cases in between? # A: There is no universal rule. Some propose using indicator variables to flag imputed values, but this is debated.\n‚û°Ô∏è How are new features constructed from existing data?\n10 Constructing new features # Q: What is feature engineering in clinical data mining? # A: The process of creating new features from existing ones, often by transformation or combination (e.g., computing BMI from height and weight).\nQ: Why is feature engineering valuable? # A: Well-designed features can significantly improve the performance of models ‚Äî even more than using advanced algorithms with raw data.\nQ: What are some simple examples of constructed features? # A:\nBinary indicators (e.g., converting a count to 1 if \u0026gt; 0) Aggregations or ratios Clinical scores derived from combinations of measurements ‚û°Ô∏è What are some practical examples of feature engineering in healthcare?\n11 Examples of engineered features # Q: What are clinical scoring systems in feature engineering? # A: Formulas that combine multiple clinical values to estimate disease severity or health status ‚Äî commonly used in risk adjustment.\nQ: What is an example of a simple scoring system? # A: Body Mass Index (BMI) ‚Äî calculated from height and weight to assess obesity.\nQ: What are examples of comorbidity scoring systems? # A:\nCharlson Comorbidity Index Elixhauser Comorbidity Index Q: Can non-clinical features also be engineered? # A: Yes ‚Äî proxy features like zip code (socioeconomic status) and record frequency (healthcare utilization) can be derived.\n‚û°Ô∏è When should you consider engineering new features?\n12 When to consider engineered features # Q: When should you engineer new features in clinical datasets? # A: When important concepts are not directly captured in the raw data ‚Äî consider proxies or derived variables to fill the gap.\nQ: What strategies can guide feature creation? # A:\nUse clinical intuition Include counts, changes over time, or ratios Repurpose validated scoring systems Q: What trade-offs should be considered? # A: Balance the benefit of a new feature against the effort required to create and validate it.\nQ: Can models ever learn features automatically? # A: Yes ‚Äî deep learning methods can learn features directly from raw data, reducing the need for manual feature engineering.\n‚û°Ô∏è What are the main takeaways about creating analysis-ready datasets?\n13 Main points about creating analysis ready datasets # Q: What is an analysis-ready dataset in clinical research? # A: A clean, structured patient-feature matrix derived from raw clinical data and suitable for analysis.\nQ: What tools are commonly used to construct it? # A: Standard programming tools and database queries (e.g., S### QL + Python).\nQ: How can the number of features be reduced? # A:\nDomain knowledge to combine or drop variables Mathematical methods like Principal Component Analysis (PCA) Q: How are missing values handled? # A: Either removed or imputed, using varying levels of complexity depending on the case.\nQ: What boosts success in dataset creation? # A: Learning the clinical context of the question ‚Äî deeper medical understanding improves data transformations and feature design.\n‚û°Ô∏è What are structured knowledge graphs, and how do they relate to datasets?\n14 Structured knowledge graphs # Q: What are the two key topics in this part of the module? # A:\nConstructing a patient feature matrix Using curated biomedical knowledge (via knowledge graphs) Q: What is a knowledge graph in healthcare? # A: A structured digital representation of biomedical entities (e.g., diseases, drugs, lab tests) and their relationships ‚Äî also known as an ontology.\nQ: Why are knowledge graphs useful? # A: They encode expert knowledge in a machine-readable way, enabling smarter feature construction, search, and data linkage.\nQ: What are examples of implicit prior knowledge use? # A: Using test order counts related to glucose as a proxy for diabetes ‚Äî derived from domain knowledge.\n‚û°Ô∏è What exactly is contained in a biomedical knowledge graph?\n15 So what exactly is in a knowledge graph # Q: What are the core components of a biomedical knowledge graph? # A:\nEntities: e.g., symptoms, diseases, drugs, body parts Synonyms: mappings of equivalent terms (e.g., \u0026ldquo;heart attack\u0026rdquo; = \u0026ldquo;acute myocardial infarction\u0026rdquo;) Relationships: logical links between entities (e.g., \u0026ldquo;is a kind of\u0026rdquo;) Q: What is the most important type of relationship in medical knowledge graphs? # A: The \u0026ldquo;is a kind of\u0026rdquo; relationship ‚Äî it defines hierarchies (e.g., Lipitor is a kind of lipid-lowering drug).\nQ: What benefit does this hierarchical structure provide? # A: Entities inherit properties from broader categories, enabling powerful reasoning and search (e.g., querying all \u0026ldquo;lipid-lowering drugs\u0026rdquo;).\n‚û°Ô∏è What are the most important knowledge graphs in biomedicine?\n16 What are important knowledge graphs # Q: Where can you find biomedical knowledge graphs? # A: One central repository is BioPortal from the National Center for Biomedical Ontology at Stanford.\nQ: What are some widely used biomedical ontologies? # A:\nICD (International Classification of Diseases): Maintained by WHO; used globally for diagnosis coding (ICD-9, ICD-10). CPT (Current Procedural Terminology): Created by the AMA to categorize procedures and services, mainly for billing. Q: Why are ICD-9 and ICD-10 both relevant today? # A: Many clinical datasets still use ICD-9 codes even though ICD-10 is current, especially in older patient records.\n‚û°Ô∏è How do we choose which knowledge graph to use in practice?\n17 How to choose which knowledge graph to use # Q: What should you consider when selecting a knowledge graph? # A:\nEntity types and how they are classified Relationship meanings between entities Terminology: presence of synonyms and spelling variants Interoperability: how well the graph maps to other knowledge graphs Q: What practical method can help assess utility? # A: Count how many terms from the knowledge graph appear in EMR text or data ‚Äî this indicates how relevant and compatible it is with your dataset.\n"},{"id":16,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c4_ai_evaluation/","title":"C4 AI Evaluations","section":"AI in Healthcare","content":" üìò Course 4: Evaluations of AI Applications in Healthcare # [ToC] Course 4 ü§ñ Module 1: AI in Healthcare # 1. What‚Äôs the Problem?\nUnderstanding the growing role of AI in healthcare and what meaningful problems it can solve.\n2. Why Does It Matter?\nAI is being rapidly adopted, yet many applications focus on accuracy without considering clinical utility or impact.\n3. What‚Äôs the Core Idea?\nAI spans biomedical research, translational science, and medical practice. It excels at data synthesis and task automation, but needs alignment with care goals.\n4. How Does It Work?\nAI systems analyze large-scale structured and unstructured data to assist in diagnostics, treatment recommendations, and patient monitoring. Evaluating AI must go beyond accuracy to include actionability.\n5. What‚Äôs Next?\nLearn how to evaluate AI solutions based on clinical utility, outcome-action pairing, and feasibility of implementation.\nüß™ Module 2: Evaluations of AI in Healthcare # 1. What‚Äôs the Problem?\nMost AI models are evaluated only by accuracy, without considering whether they actually improve clinical care.\n2. Why Does It Matter?\nBecause real-world deployment requires understanding how an AI model leads to meaningful actions and patient benefit.\n3. What‚Äôs the Core Idea?\nUse the Outcome-Action Pairing (OAP) framework: pair predictions with feasible actions that impact care. Evaluate utility, feasibility, and clinical impact.\n4. How Does It Work?\nStart with the clinical problem, define output and required action, assess lead time, type of action (medical, operational), and stakeholder involvement.\n5. What‚Äôs Next?\nExamine the four phases of deploying AI in clinical environments, from design to monitoring.\nüöÄ Module 3: AI Deployment # 1. What‚Äôs the Problem?\nEven well-performing AI models often fail to reach or impact clinical care due to poor integration and lack of support.\n2. Why Does It Matter?\nHealthcare settings require safety, validation, stakeholder buy-in, and long-term monitoring for AI success.\n3. What‚Äôs the Core Idea?\nDeployment includes four stages: design and development, evaluation and validation, diffusion and scaling, and continuous monitoring.\n4. How Does It Work?\nEvaluate utility and economic value, run \u0026lsquo;silent mode\u0026rsquo; trials, ensure human-machine interaction, and plan for infrastructure and updates.\n5. What‚Äôs Next?\nUnderstand how fairness, transparency, and bias affect AI performance across diverse populations.\n‚öñÔ∏è Module 4: Downstream Evaluations: Bias and Fairness # 1. What‚Äôs the Problem?\nAI models may perpetuate bias or perform poorly on underrepresented populations.\n2. Why Does It Matter?\nBias in AI can lead to inequities in care and further marginalize already vulnerable groups.\n3. What‚Äôs the Core Idea?\nBias can occur at any stage‚Äîfrom data collection to deployment. Fairness requires proactive evaluation using tools like MINIMAR.\n4. How Does It Work?\nIdentify and mitigate types of bias (e.g., representation, measurement, aggregation). Use fairness definitions like anti-classification and calibration.\n5. What‚Äôs Next?\nExplore how AI regulation addresses these concerns through risk frameworks and transparency standards.\nüìú Module 5: Regulatory Environment for AI in Healthcare # 1. What‚Äôs the Problem?\nAI products often lack regulatory approval due to unclear pathways and concerns over safety and accountability.\n2. Why Does It Matter?\nRegulation ensures AI tools are safe, effective, and beneficial in real-world healthcare settings.\n3. What‚Äôs the Core Idea?\nThe FDA and IMDRF provide frameworks based on risk classification and clinical evaluation (valid association, analytical \u0026amp; clinical validation).\n4. How Does It Work?\nFollow the SaMD lifecycle: define intended use, evaluate risk, seek approval via 510(k), De Novo, or PMA, and ensure continuous monitoring.\n5. What‚Äôs Next?\nApply ethical practices in problem formulation, data choice, stakeholder transparency, and conflict of interest management.\nüß≠ Module 6: Best Ethical Practices in AI for Healthcare # 1. What‚Äôs the Problem?\nConflicts of interest and unclear problem framing can undermine trust and effectiveness in AI systems.\n2. Why Does It Matter?\nEthical lapses can lead to harm, inequity, or misuse of AI tools in sensitive healthcare decisions.\n3. What‚Äôs the Core Idea?\nEthical AI requires clear goals, clinician input, transparent data use, and conflict-of-interest management.\n4. How Does It Work?\nAsk ethical questions early, assess biases in data and design, disclose secondary interests, and implement oversight.\n5. What‚Äôs Next?\nAdopt frameworks like MINIMAR, audit for fairness, and ensure that systems are explainable, justifiable, and trustworthy.\n"},{"id":17,"href":"/healthcare/domain_knowledge/hands-on-healthcare-data/ch4_ehr/","title":"Ch4 EHR","section":"Hands-On Healthcare Data","content":" Ch4 Deep Dive ‚Äì Electronic Health Records (EHR) Q1: What is the central focus of Chapter 4? # Chapter 4 focuses on working with electronic health record (EHR) data using the MIMIC-III dataset, and explores medication harmonization using SQL, Neo4j (property graph), and TypeDB (typed hypergraph).\nQ2: What makes working with EHR data complex? # EHRs are highly structured but vary between implementations. Data is often redundant, inconsistent, or missing. Clinical context and domain knowledge are crucial for correct interpretation. Q3: Why was medication harmonization chosen as the use case? # Because medications are objective and widely used in EHRs, but the same drug can appear under multiple names or codes (e.g. NDCs). Harmonization is necessary to:\nNormalize names or codes. Filter out forms like \u0026ldquo;heparin flush\u0026rdquo; vs \u0026ldquo;therapeutic heparin\u0026rdquo;. Q4: How is this implemented using SQL (SQLite)? # CSVs are loaded into SQLite tables. Free-text string search is done across drug name columns (e.g., drug_name_generic LIKE \u0026ldquo;%heparin%\u0026rdquo;). NDC-based harmonization is done by: Extracting distinct NDCs. Filtering to include only valid/therapeutic ones. Rewriting queries using explicit WHERE ndc IN (...) clauses. Q5: What are the challenges of the SQL approach? # Hard to maintain mappings (e.g., repeated NDC lists). Poor separation of concerns (clinical logic leaks into queries). Not reusable across projects or datasets. Q6: How does Neo4j (property graph) improve the workflow? # Drug instances are modeled as nodes, and a new \u0026ldquo;heparin (non-flush)\u0026rdquo; concept is created. Relationships connect drug nodes to patients and prescriptions. Reusability improves because mappings are stored inside the graph. Queries become more semantic, e.g., follow a concept node rather than duplicating NDCs in every query. Q7: How are concepts created in Neo4j? # A node like Drug:Knowledge { drug: \u0026quot;Heparin (non-flush)\u0026quot;, ... } is created. Drug nodes are linked to it via [:for_drug {derived: true}] relationships. This enables querying \u0026ldquo;all patients on this drug concept.\u0026rdquo; Q8: How does TypeDB enhance this model further? # TypeDB uses strong typing and roles to model relations: patient plays prescription:patient druginstance plays prescription:prescribed_drug Separate druginstance entity and prescription relation. Can add inference rules to dynamically associate drugs with higher-level concepts. Q9: What are the two approaches to harmonization in TypeDB? # Persisted: Insert actual hierarchy facts: (parent: heparin, child: drug) isa hierarchy. Rule-based: Use rules like: rule heparin-rule: when { $d has ndc \u0026#34;xxxx\u0026#34;; $c has purl \u0026#34;...\u0026#34;; } then { (parent: $c, child: $d) isa hierarchy; } Q10: What are the tradeoffs between approaches? # Model Pros Cons SQL Ubiquitous, accessible Poor semantics, duplication of logic Neo4j Intuitive graph model, reusable concepts Fragile schemas, performance concerns TypeDB Semantic precision, rule engine Newer ecosystem, complexity, fewer tools RDF Graph Web standard, portable Very steep learning curve Q11: What are the broader lessons? # Separate clinical knowledge from code. Choose modeling strategies based on: Project duration Tooling maturity Frequency of schema changes Collaboration needs Use graphs or TypeDB to encode reusable logic and keep queries clean. üß† Curriculum Task-Based Summary (Chapter 4) # üîπ 1. Understanding EHR Data Models # Compare OMOP, FHIR, i2b2, PCORnet, ADaM, SDTM. Explore role of implementation guides and FHIR profiles. üîπ 2. Setup and Load EHR Data (MIMIC-III) # Use SQLite to ingest .csv files (Example 4-1). Use Neo4j or TypeDB containers (Docker). Load and explore data with basic queries. üîπ 3. Medication Harmonization Use Case # Focus on heparin, and identify pitfalls with NDC codes. Extract and deduplicate NDCs from prescriptions. Build queries that target therapeutic use only. üîπ 4. Query and Harmonize in Three Paradigms # Task SQL Neo4j TypeDB Load data pandas + sqlite3 pandas + NeoInterface pandas + TypeDB client Query for \u0026ldquo;heparin\u0026rdquo; LIKE on drug names toLower(drug_name) CONTAINS drug has name contains Harmonization Filter with ndc IN (...) Create concept node + edges Insert facts or define rules üîπ 5. Linkage and Reasoning # Create custom drug concepts. Track prescriptions per patient. Link concepts using: SQL joins Neo4j (:Patient)-[:has_prescription]-\u0026gt;(:Drug) TypeDB roles + rules. üîπ 6. Evaluate Tradeoffs and Performance # Review table of pros/cons (Table 4-2). Balance: Query simplicity vs data model reusability. Rule-driven inference vs static mapping. Ecosystem maturity. ‚úÖ End of Chapter Outcome # You should now be able to:\nChoose the right data model (SQL, graph, hypergraph) for your RWD task. Implement and harmonize medication concepts. Balance engineering choices with clinical accuracy and long-term maintainability. Begin thinking about integrating terminologies (UMLS, SNOMED CT) into your models. "},{"id":18,"href":"/ai-workflows/data/data-centric-ai/class-imbalance-outliers-distribution-shift/","title":"Class Imbalance, Outliers, and Distribution Shift","section":"Data-Centric AI (DCAI)","content":" Class Imbalance, Outliers, and Distribution Shift Q1: What are the main problems discussed in this lecture? # Class imbalance Outliers Distribution shift Q2: What is class imbalance and why is it a problem? # Definition: Some classes occur much less frequently than others. Examples: COVID detection, fraud detection, manufacturing defects, self-driving cars. Impact: Naive models can have misleadingly high accuracy while failing on rare classes. Q3: How do we address class imbalance? # Sampling Techniques: Sample weights (less stable for mini-batch training) Over-sampling (replicating minority class examples) Under-sampling (dropping majority class examples) SMOTE (synthetic minority over-sampling) Balanced mini-batch training (better distribution in each batch) Choose appropriate evaluation metrics: precision, recall, F-beta score. Q4: What are outliers and why are they problematic? # Definition: Datapoints that differ significantly from the norm. Causes: Measurement error, bad data collection, adversarial inputs, rare events. Impact: Outliers can harm training and inference stability. Q5: How do we detect outliers? # Simple methods: Tukey\u0026rsquo;s fences, Z-score analysis. More advanced: Isolation forest (tree-based) KNN distance (neighbor proximity) Autoencoders (reconstruction loss) Evaluation: ROC curve and AUROC score. Q6: What is distribution shift? # Definition: Training and test distributions differ. Almost all real-world ML deployments experience it. Q7: What are the types of distribution shift? # Type Meaning Example Covariate shift (p(x)) changes, (p(y x)) stays the same Concept shift (p(y x)) changes, (p(x)) stays the same Prior probability shift (p(y)) changes, (p(x y)) stays the same Q8: How do we detect and handle distribution shift? # Detection: Monitor metrics and statistical properties of data. Handling: Retrain with better data. Use sample reweighting if unlabeled test data is available. Concept shift remains hardest to fix without labeled test data. Q9: Final Takeaways # Handling class imbalance, outliers, and distribution shift is critical for building robust, real-world ML systems. Evaluation metric choice, proper data preprocessing, and continuous monitoring are key strategies. References # imbalanced-learn package SMOTE Paper PyOD library for outlier detection Dataset Shift Book Outlier detection in scikit-learn Lab assignment for Outliers "},{"id":19,"href":"/ai-workflows/genai/5-day-genai-google-2025/day3_generative_agents/","title":"Day 3 ‚Äì Generative Agents","section":"5-Day GenAI with Google 2005","content":" Day 3 ‚Äì Generative Agents 1. What Are Generative Agents? # We start with the definition of agents‚ÄîAI systems designed to achieve goals by perceiving their environment and taking actions using tools. Unlike static LLMs, generative agents combine models, tools, and orchestration to interact with the world dynamically.\n‚Üí what components make these agents truly autonomous and intelligent?\n2. Agent Architecture Breakdown # An agent‚Äôs architecture includes:\nA language model for decision-making. Tools to interface with the outside world (APIs, functions, data). An orchestration layer to manage memory, state, and reasoning techniques like CoT, ReAct, and ToT. ‚Üí how do we structure agents to be effective in real-world applications?\n3. From MLOps to AgentOps # AgentOps is introduced as a specialized branch of GenAIOps focused on the deployment and reliability of agents. It inherits from DevOps, MLOps, and FMOps, and introduces concepts like prompt orchestration, memory handling, and task decomposition.\n‚Üí how do organizations build scalable, production-grade agent systems?\n4. The Role of Observability and Metrics # Agents must be measured at every level‚Äîgoal success rates, user interactions, latency, errors, and human feedback. These form the KPIs for agents and inform ongoing improvements.\n‚Üí how do we move beyond proof-of-concept to reliable agent deployment?\n5. Evaluating Agents Effectively # Agent evaluation involves more than just checking output correctness. It requires tracing decision-making, assessing reasoning, evaluating intermediate steps, and gathering structured human feedback.\n‚Üí how do we evaluate agents on logic, tool use, and usefulness holistically?\n6. Instrumentation and Traceability # Traces provide fine-grained visibility into what the agent did and why. This supports debugging and performance tuning, enabling trust and iterative refinement.\n‚Üí what observability tools are best for multi-step agent workflows?\n7. Assessing Core Capabilities # Before deployment, it\u0026rsquo;s vital to evaluate agent capabilities like tool use and planning. Benchmarks such as BFCL and PlanBench test these abilities, but should be supplemented with task-specific tests that reflect real use cases.\n‚Üí what public benchmarks best reflect your agent\u0026rsquo;s core capabilities?\n8. Trajectory Evaluation # Agents often follow multi-step trajectories. Evaluation should compare expected vs. actual tool use paths using metrics like exact match, in-order, any-order, precision, recall, and single-tool usage.\n‚Üí is the agent taking optimal steps‚Äîor just getting lucky in the final answer?\n9. Evaluating Final Responses # The agent\u0026rsquo;s final output must be evaluated for correctness, relevance, and tone. LLM-based autoraters are useful, but need precisely defined criteria. Human evaluators still offer the gold standard for nuanced feedback.\n‚Üí can automated evaluation alone guarantee real-world readiness?\n10. Human-in-the-Loop (HITL) # Subjectivity, nuance, and real-world implications often require human review. Direct scoring, comparative evaluations, and user studies are powerful tools to validate and calibrate automated metrics.\n‚Üí when should humans intervene in the agent evaluation loop?\n11. Challenges and Future Directions # Agent evaluation is still maturing. Current challenges include limited evaluation datasets, gaps in process reasoning metrics, difficulty with multimodal outputs, and handling dynamic environments.\nThe forward-looking insight: agent evaluation is shifting toward process-based, explainable, and real-world-grounded methods.\n12. From Single to Multi-Agent Evaluation # Multi-agent systems are the next evolution in generative AI‚Äîmultiple specialized agents collaborate like a team. Evaluation must now address not just individual outputs, but also cooperation, delegation, and plan adherence.\n‚Üí how do we measure coordination, not just correctness?\n13. Architecture of Multi-Agent Systems # Agents are modular and play distinct roles‚Äîplanner, retriever, executor, evaluator. Communication, routing, tool integration, memory, and feedback loops form the backbone. These components support dynamic and resilient reasoning.\n‚Üí what enables agents to act as a system, not just individuals?\n14. Multi-Agent Design Patterns # Patterns like sequential, hierarchical, collaborative, and competitive enable scalable, adaptive, and parallel agent behavior. These patterns reduce bottlenecks and improve automation for complex workflows.\n‚Üí which pattern suits your domain‚Äîassembly line, team, tournament, or council?\n15. Evaluation at Scale # Evaluating multi-agent systems includes trajectory traceability, agent coordination, agent-tool selection, and system-wide goal success. Instrumenting each step and agent ensures deeper insights.\nThe closing reflection: multi-agent systems multiply both the potential and complexity of generative agents‚Äîevaluation must evolve accordingly.\n16. From RAG to Agentic RAG # Traditional RAG pipelines retrieve static chunks of knowledge for LLMs. Agentic RAG innovates by embedding retrieval agents that:\nExpand queries contextually Plan multi-step retrieval Choose data sources adaptively Validate results via evaluator agents ‚Üí how can agents actively reason during retrieval to boost response quality?\n17. Engineering Better RAG # To improve any RAG implementation:\nParse and chunk documents semantically Enrich chunks with metadata Tune embeddings or adapt search space Use fast vector search + rankers Implement grounding checks ‚Üí is your RAG problem about generation‚Äîor poor search to begin with?\n18. The Rise of Enterprise Agents # 2025 marks the rise of two agent types:\nAssistants: Interactive, task-oriented agents like schedulers or sales aides. Automators: Background agents that observe events and autonomously act. ‚Üí how do organizations orchestrate fleets of agents across roles and workflows?\n19. Agentspace and Agent Management # Google Agentspace provides enterprise-grade infrastructure for creating, deploying, and managing secure, multimodal AI agents. With features like:\nRBAC, SSO, and data governance Blended RAG and semantic search Scalable agent orchestration and monitoring ‚Üí what‚Äôs needed to manage AI agents as a virtual team at scale?\n20. NotebookLM Enterprise # NotebookLM allows users to upload documents, ask questions, and synthesize insights. Enterprise features include:\nAudio summaries via TTS Semantic linking across documents Role-based access and policy integration Final insight: intelligent notebooks + agents will redefine enterprise knowledge discovery and interaction.\n21. Contract-Adhering Agents # Prototypical agent interfaces are too vague for real-world, high-stakes environments. Introducing contractor agents enables:\nClear outcome definitions Negotiation and refinement Self-validation of deliverables Structured decomposition into subcontracts ‚Üí how can formalized contracts make agents production-ready and trustworthy?\n22. Contract Lifecycle and Execution # Contractor agents follow a lifecycle: define ‚Üí negotiate ‚Üí execute ‚Üí validate. Execution may involve multiple LLM-generated solutions and iterative self-correction until the contract is fulfilled, optimizing for quality over latency.\n‚Üí what runtime capabilities are needed for contract-based agents?\n23. Co-Scientist: A Real-World Case Study # Google‚Äôs AI co-scientist system uses multi-agent collaboration to accelerate hypothesis generation and validation in scientific research. Roles include:\nData processors Hypothesis generators Validators Cross-team communicators Final reflection: multi-agent systems, when built as collaborative contractors, can extend the scientific method itself.\n24. Specialized Agents in the Car # The automotive domain is a natural fit for multi-agent AI. Here are key agent roles:\nNavigation Agent: Plans routes, ranks POIs, and handles traffic awareness Media Agent: Plays contextually relevant music or podcasts Messaging Agent: Drafts, edits, and sends messages hands-free Car Manual Agent: Uses RAG to answer questions about car features General Knowledge Agent: Answers follow-up queries to enhance user experience ‚Üí how do you design agent roles that align with contextual user needs?\n25. Hierarchical and Diamond Patterns # Hierarchical: A central Orchestrator routes user input to the right agent Diamond: Adds a Rephraser agent for tone/style before speaking responses aloud ‚Üí when does orchestration alone fall short‚Äîrequiring tone-sensitive agents?\n26. Peer-to-Peer and Collaborative Patterns # Peer-to-Peer: Agents hand off queries among themselves for better routing resilience Collaborative: Multiple agents contribute partial answers; a Mixer Agent synthesizes the final response ‚Üí can agents collaborate without central control to produce superior outputs?\n27. Response Mixer and Safety-Critical Use # The Response Mixer evaluates and combines outputs from several agents (e.g., knowledge + tips + manual) to form a cohesive answer, especially for safety-critical queries like aquaplaning.\n‚Üí how do we ensure safety-critical information is prioritized in generative settings?\n28. Adaptive Loop Pattern # Agents refine queries iteratively to meet vague or underspecified user needs‚Äîe.g., finding a vegan Italian restaurant with fallback strategies.\nClosing insight: multi-agent architectures thrive where adaptability, refinement, and specialization are essential.\n29. Real-Time Performance and Resilience # Multi-agent systems in cars prioritize on-device responsiveness for safety (e.g., climate control), while using cloud-based agents for tasks like dining suggestions. This hybrid model balances latency, capability, and robustness.\n‚Üí how do agents coordinate local vs. remote processing for safety and personalization?\n30. Vertex AI Agent Builder # Google‚Äôs Agent Builder platform integrates secure cloud services, open-source libraries, evals, and managed runtimes for enterprise-grade agent development. Features include:\nRetrieval via Vertex AI Search or RAG Engine Secure APIs via Apigee Gemini and Model Garden access Evaluation pipelines via Vertex AI Eval Service ‚Üí what developer tools are needed to build, scale, and evaluate enterprise-ready agents?\n31. Key Developer Principles # AgentOps matters: memory, tools, trace, orchestration Automate evals, but combine with HITL Design multi-agent architectures for complexity and scale Improve search before Agentic RAG Use agent/tool registries to reduce chaos Prioritize security, flexibility, and developer cycles 32. Future Directions # Research will focus on:\nProcess-based and AI-assisted evaluation Agent collaboration and communication protocols Memory, adaptivity, explainability, and contracting models Final insight: the future is agentic‚Äîdevelopers must blend engineering, ops, UX, and domain logic to build next-gen intelligent systems.\n"},{"id":20,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m4/","title":"[Summary] Module 4: Evaluation and Metrics for ML in Healthcare","section":"C3 ML Healthcare","content":" Module 4: Evaluation and Metrics for ML in Healthcare # 1 Introduction to Model Performance Evaluation # Q1: Why is model evaluation critical in healthcare machine learning? # In healthcare, decisions informed by ML models can have life-altering consequences:\nIt‚Äôs not enough for a model to perform well on training data. We need to ensure that the model performs well on unseen patients and real-world conditions. Rigorous evaluation is essential to trust and validate clinical usefulness. ‚û°Ô∏è What does it mean for a model to generalize?\nQ2: What is generalization and how is it assessed? # Generalization refers to the model\u0026rsquo;s ability to perform well on new, unseen data:\nIndicates how well the model has learned the underlying patterns. We measure this using validation and test sets. High training accuracy but poor test accuracy implies overfitting. ‚û°Ô∏è What techniques help ensure fair and reliable evaluation?\nQ3: What is data splitting and why is it important? # Common splits:\nTraining set: For model learning. Validation set: For tuning hyperparameters and model selection. Test set: For final performance estimation. These splits help avoid data leakage and optimism bias in evaluation.\n‚û°Ô∏è Are there methods more robust than simple train/test splits?\nQ4: What is cross-validation and when is it useful? # Cross-validation is a strategy to make evaluation more robust:\nData is divided into k folds. Model trains on k-1 folds and validates on the remaining one. Repeated multiple times to average out variability. It‚Äôs particularly useful in small datasets, common in healthcare studies.\n2 Overfitting and Underfitting # Q1: What are overfitting and underfitting in machine learning? # These are two common problems that reduce model effectiveness:\nOverfitting: The model learns noise and specifics of the training set, performing poorly on new data. Underfitting: The model fails to capture underlying trends, resulting in poor performance on both training and test sets. ‚û°Ô∏è What are the visual signs of overfitting and underfitting during training?\nQ2: How can we detect overfitting and underfitting? # By plotting training and validation accuracy/loss:\nOverfitting: Training accuracy increases while validation accuracy drops or plateaus. Underfitting: Both training and validation accuracies remain low. Regular monitoring helps identify these trends early. ‚û°Ô∏è What causes models to overfit?\nQ3: What factors contribute to overfitting in ML models? # High model complexity (deep networks, too many parameters). Small training dataset or lack of representative diversity. Too many epochs without early stopping. Overfitting is especially risky in healthcare due to variability in real-world patient populations.\n‚û°Ô∏è Conversely, what might cause underfitting?\nQ4: Why might a model underfit the data? # The model is too simple (e.g., linear models for nonlinear problems). Insufficient training time or suboptimal hyperparameters. Poor feature engineering or missing important data signals. Underfitting leads to missed patterns‚Äîdangerous in diagnostic or predictive tools.\n3 Strategies to Address Overfitting, Underfitting and Introduction to Regularization # Q1: How can we address overfitting in ML models? # Strategies include:\nReducing model complexity: Use fewer layers or parameters. Early stopping: Halt training when validation performance stops improving. Data augmentation: Increase data diversity artificially (especially for images). Regularization: Penalize model complexity. ‚û°Ô∏è What types of regularization techniques are commonly used?\nQ2: What is L1 and L2 regularization? # These techniques add penalty terms to the loss function:\nL1 (Lasso): Adds absolute value of weights ‚Üí encourages sparsity (some weights become zero). L2 (Ridge): Adds square of weights ‚Üí discourages large weights. They help control overfitting by shrinking parameter magnitudes.\n‚û°Ô∏è Besides weight penalties, are there other techniques to improve generalization?\nQ3: What is dropout and how does it help prevent overfitting? # Dropout randomly disables neurons during training:\nPrevents co-adaptation of features. Encourages the network to learn redundant, distributed representations. Typically used in deep networks. ‚û°Ô∏è Can underfitting also be addressed through specific strategies?\nQ4: How can we fix underfitting in a model? # Underfitting can be resolved by:\nIncreasing model capacity (deeper or more complex models). Training longer or using better optimization. Improving data quality or features. Adjusting learning rate and other hyperparameters. 4 Statistical Approaches to Model Evaluation # Q1: Why are statistical methods important in evaluating ML models? # Statistical tools help us quantify confidence and variability in model performance:\nAvoid over-interpreting single-point metrics. Make decisions grounded in significance and uncertainty. Essential when models may be deployed in clinical practice. ‚û°Ô∏è What‚Äôs a simple way to estimate uncertainty in metrics?\nQ2: What is bootstrapping and how is it used in model evaluation? # Bootstrapping is a resampling technique:\nRepeatedly sample with replacement from the test set. Evaluate the model on each sample to get a distribution of performance metrics. Helps compute confidence intervals for metrics like accuracy or AUC. ‚û°Ô∏è Are there other methods for statistical comparison of models?\nQ3: How do permutation tests assess model significance? # Permutation testing involves:\nRandomly shuffling labels on test data. Evaluating model performance on this randomized data. Repeating multiple times to build a null distribution. If the true model significantly outperforms the null, it\u0026rsquo;s statistically meaningful.\n‚û°Ô∏è How do we assess reliability when comparing two models?\nQ4: What is the paired t-test and when should it be used? # Used to compare two models‚Äô predictions on the same test samples:\nMeasures whether the difference in performance is statistically significant. Assumes approximately normal distribution of differences. Can be useful in evaluating if model A is better than model B.\n5 Receiver Operator and Precision Recall Curves as Evaluation Metrics # Q1: Why do we need different evaluation metrics beyond accuracy? # Accuracy alone can be misleading, especially in imbalanced datasets:\nIn healthcare, positive cases (e.g., disease presence) may be rare. A model that predicts only the majority class can appear highly accurate. Metrics like precision, recall, and AUC provide better insight into real performance. ‚û°Ô∏è What is the ROC curve and how is it interpreted?\nQ2: What is the ROC curve and AUC? # ROC (Receiver Operating Characteristic) curve plots True Positive Rate (TPR) vs. False Positive Rate (FPR). AUC (Area Under the Curve) summarizes this curve into a single number (0.5 = random, 1.0 = perfect). AUC reflects the model‚Äôs ranking ability‚Äîhow well it separates classes. ‚û°Ô∏è Are ROC curves always the best choice?\nQ3: When should we prefer Precision-Recall (PR) curves over ROC? # PR curves focus on positive class performance, plotting Precision vs. Recall. More informative than ROC when dealing with imbalanced data. Helpful for screening tools or rare disease prediction. ‚û°Ô∏è How are these metrics computed and interpreted?\nQ4: What are precision, recall, and F1 score? # Precision: TP / (TP + FP) ‚Üí Of predicted positives, how many were correct? Recall: TP / (TP + FN) ‚Üí Of actual positives, how many did we find? F1 Score: Harmonic mean of precision and recall. These metrics give a fuller picture of model trade-offs, especially in clinical use cases.\n"},{"id":21,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c5_capstone/","title":"C5 Capstone Projects","section":"AI in Healthcare","content":" üìò Course 5: Capston Projects \u0026ndash; COVID-19 AI # üì∑ Project 1: CXR-Based COVID-19 Detector # Phase 1: Data Collection # Objective: Build a deep learning model to predict COVID-19 status using chest x-ray (CXR) images. Input: 3000x3000 px uncompressed DICOM images from 30,000 exams (10% COVID-positive). Concern: Class imbalance (90:10) and high-resolution image processing needs. Phase 2: Model Training (Part 1) # Used ResNet-50 on resized 224x224 images. Data split randomly (not by patient). Augmentation: 50% zoom-in on random region. Issue: Training loss did not improve ‚Üí possible underfitting or flawed preprocessing. Phase 3: Model Training (Part 2) # Improvements made: Patient-level data split to prevent leakage. Image size increased to 512x512 px; model adjusted accordingly. Simplified augmentation: horizontal flip + light zoom. COVID-positive oversampling added. New issue: Overfitting (training loss much lower than validation loss). Metric discrepancy: High accuracy but relatively low AUROC on validation set. Phase 4: Model Evaluation # Applied dropout (p=0.5) and random rotation augmentation. Two early-stopped models: Model A: Best validation AUROC. Model B: Best validation loss. Deployment consideration: Choose model based on worklist prioritization use case. üìà Project 2: EHR-Based Intubation Predictor # Phase 1: Data Collection # Objective: Predict likelihood of intubation from electronic health records (EHR). Input: COVID dataset with 3,000 EHRs (300 positive) + additional 40,000-exam COVID-like dataset. Issue: Misassumption‚Äîonly 3,000 usable EHRs in COVID dataset. Challenge: Sparse features, strange lab value distributions (e.g., D-DIMER), many missing values. Phase 2: Model Training (Part 1) # Attempted logistic regression but faced data issues (sparsity, outliers, NaNs). Required strategies to deal with missingness and outliers before modeling. Phase 3: Model Training (Part 2) # New strategy: Train on 40,000 ‚ÄúCOVID-like‚Äù exams; test on COVID dataset (3,000 exams). Split: 70% train, 30% validation (COVID-like dataset). 10-fold cross-validation used for hyperparameter tuning. Models trained: Logistic regression + Random Forests. Phase 4: Model Evaluation # Performance improved using COVID-like training data. Now selecting operating threshold using precision-recall curve. Deployment consideration: Choose threshold optimized for triage decision-making. üîó Cross-Project Learnings # Both projects improved significantly from Phase 2 to 4 through better data practices: Patient-level splits Cross-validation Oversampling Threshold tuning Key divergence: Project 1 is image-based, focuses on COVID diagnosis. Project 2 is EHR-based, focuses on intervention prediction (intubation). Both must align their evaluation strategy with real-world clinical use (triage vs. prioritization). "},{"id":22,"href":"/ai-workflows/data/data-centric-ai/dataset-creation-curation/","title":"Dataset Creation and Curation","section":"Data-Centric AI (DCAI)","content":" Dataset Creation and Curation Q1: What are the main themes of dataset creation and curation? # Framing the ML task correctly. Addressing data sourcing concerns like selection bias. Handling label sourcing and quality control. Q2: Why is careful sourcing of data important? # ML models exploit spurious correlations. If training data does not match real-world deployment conditions, models can fail badly. Q3: What is selection bias and what are its common causes? # Selection bias: Systematic mismatch between training data and deployment data. Causes: Time/location bias, demographic bias, response bias, availability bias, long tail bias. Q4: How can we deal with selection bias during data collection? # Hold out validation sets that mimic deployment conditions, such as latest data, new locations, or oversampled rare events. Q5: How can we estimate how much data we need? # Use a method to measure learning curves by sub-sampling data and fitting a simple log-log model to predict performance scaling. Q6: What is the formula used to predict model error with more data? # ( \\log(\text{error}) = -a \\cdot \\log(n) + b ) Q7: What are concerns when labeling data with crowdsourced workers? # Variability in annotator accuracy. Possibility of annotator collusion. Q8: How can we maintain label quality during crowdsourcing? # Insert \u0026ldquo;quality control\u0026rdquo; examples with known ground-truth to monitor annotator performance. Q9: What methods are used to curate labels from multiple annotators? # Majority Vote and Inter-Annotator Agreement. Dawid-Skene model. CROWDLAB. Q10: How does Majority Vote work? # Assigns the label chosen by the majority of annotators. Confidence is based on inter-annotator agreement. Q11: What are downsides of Majority Vote? # Ties are ambiguous. Bad annotators have equal influence as good ones. Q12: What is the Dawid-Skene model? # Models each annotator with a confusion matrix. Uses Bayesian inference (often approximated with EM) to estimate consensus labels and annotator quality. Q13: What are limitations of Dawid-Skene? # Requires strong assumptions. Performs poorly if examples are labeled by few annotators. Q14: What is CROWDLAB? # Combines classifier predictions and annotator labels for better consensus. Weights depend on model confidence and inter-annotator agreement. Q15: How does CROWDLAB work? # For examples labeled by few annotators, rely more on the classifier. For examples labeled by many annotators, rely more on label agreement. Q16: How are weights estimated in CROWDLAB? # Based on annotator agreement rates and classifier accuracy normalized against a majority-class baseline. Q17: What is the hands-on lab assignment for this lecture? # Analyze a multi-annotator dataset and implement methods for estimating consensus labels and annotator quality. References # Human-in-the-Loop ML textbook Recognition in terra incognita (Beery et al., 2018) Constructive prediction of generalization error (Rosenfeld et al., 2020) CROWDLAB paper (Goh et al., 2022) Catalogue of Bias (Selection Bias) Dataset curation lab notebook "},{"id":23,"href":"/ai-workflows/genai/5-day-genai-google-2025/day4_domainspecific_llms/","title":"Day 4 ‚Äì Domain-Specific LLMs","section":"5-Day GenAI with Google 2005","content":" Day 4 ‚Äì Domain-Specific LLMs 1. The Rise of Specialized LLMs # We start with the evolution of LLMs from general-purpose to domain-specific tools. This shift was driven by challenges in fields like cybersecurity and medicine, where technical language and sensitive use cases demand more than general knowledge.\n‚Üí why do general-purpose LLMs struggle in specialized domains?\n2. The Challenges in Cybersecurity # Cybersecurity experts face three main issues: rapidly evolving threats, repetitive manual work (toil), and a shortage of skilled talent. These bottlenecks make it hard to keep up with modern security needs.\n‚Üí how can AI reduce toil, bridge talent gaps, and counter fast-evolving threats?\n3. The Role of GenAI in Security # GenAI can assist various security personas‚Äîfrom analysts to CISOs‚Äîby translating queries, reverse-engineering code, planning remediation, and summarizing threats. This enables both automation and augmentation of expertise.\n‚Üí what kind of architecture supports this AI augmentation effectively?\n4. Multi-layered System Design # SecLM combines tools (top layer), a reasoning API (middle layer), and secure data sources (bottom layer). This allows for contextual responses using live data and tailored planning.\n‚Üí how do we ensure accuracy and freshness without retraining LLMs constantly?\n5. Domain-Specific Model Training # SecLM trains on open-source and licensed security content, fine-tuned for tasks like alert summarization and command analysis. It uses parameter-efficient tuning and RAG for freshness.\n‚Üí how does the system generalize to unseen tasks or environments?\n6. Flexible Planning and Execution # SecLM decomposes broad questions (e.g., about an APT group) into steps like retrieving intel, translating to SIEM queries, and synthesizing responses‚Äîshowcasing multi-agent, tool-augmented reasoning.\n‚Üí how does this compare with general-purpose LLMs?\n7. Performance and Evaluation # Through expert evaluations and automated metrics, SecLM outperforms general models on security-specific tasks, demonstrating the need for full-stack, domain-focused platforms.\n‚Üí can this platform be generalized to other domains like health tech?\n8. Medical Q\u0026amp;A as a Grand Challenge # Medical question-answering (QA) demands deep reasoning, evolving knowledge, and accurate synthesis. LLMs like Med-PaLM show potential by answering USMLE-style questions and sourcing info from varied medical content.\n‚Üí how can we ensure these answers are trustworthy and context-aware?\n9. Opportunities for GenAI in Medicine # Use cases range from contextual Q\u0026amp;A on patient history to triaging clinician messages and real-time patient-clinician dialogue support. GenAI systems can enhance decision-making, patient engagement, and clinician efficiency.\n‚Üí what safeguards ensure these capabilities are safe, equitable, and accurate?\n10. Human-Centric and Conversational AI # Med-PaLM aims to support flexible interaction‚Äîcombining structured clinical expertise with empathetic, human-centric dialogue. It was built to scale reasoning and bring compassion into AI-assisted medicine.\n‚Üí how do we evaluate such models beyond technical accuracy?\n11. Evaluation Frameworks for Medical LLMs # Med-PaLM uses USMLE-style exams and qualitative rubrics to assess reasoning, factuality, and harm potential. Human experts assess each dimension, comparing outputs to clinicians\u0026rsquo; answers in blinded evaluations.\n‚Üí how does Med-PaLM compare to human experts in real scenarios?\n12. From Benchmark to Bedside # Rigorous validation is required for real-world use‚Äîstarting with retrospective studies, then prospective ones, all before interventional deployment. Past learnings (e.g., from diabetic retinopathy screening) stress this need.\n‚Üí how can we responsibly scale these tools into clinical settings?\n13. Task-Specific vs. Domain-General Models # While Med-PaLM 2 shows expert-level performance on QA tasks, its capabilities must be validated across each medical subdomain. Mental health assessments, for example, require specialized evaluation and adaptation.\n‚Üí can a high-performing model generalize across all medical use cases without fine-tuning?\n14. Toward Multimodal Healthcare AI # Medicine is inherently multimodal‚Äîspanning text, images, genomics, EHRs, and sensors. MedLM is expanding into multimodal models, which are in early research stages but promise broader clinical utility.\n‚Üí how can AI integrate and reason across multiple data modalities safely and meaningfully?\n15. Training Innovations in Med-PaLM 2 # Med-PaLM 2 leverages instruction tuning on diverse QA datasets and advanced prompting strategies like chain-of-thought, self-consistency, and ensemble refinement. These enhance stepwise reasoning and output reliability.\n‚Üí which techniques most boost reasoning performance in sensitive domains like healthcare?\n16. Key Takeaways # LLMs show tremendous promise in solving domain-specific problems. In cybersecurity, SecLM combines tools, reasoning, and authoritative data to empower practitioners. In healthcare, MedLM and Med-PaLM show how vertical fine-tuning, evaluation, and collaboration with clinicians drive real-world impact.\nThe overarching insight: LLMs require domain-specific architecture, data, tuning, and evaluation to move from general intelligence to real-world application.\n"},{"id":24,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m5/","title":"[Summary] Module 5: Strategies and Challenges in ML for Healthcare","section":"C3 ML Healthcare","content":" Module 5: Strategies and Challenges in ML for Healthcare # 1 Introduction to Common Clinical Machine Learning Challenges # Q1: Why is deploying machine learning in healthcare uniquely challenging? # Healthcare presents complex, high-stakes environments with unique constraints:\nData is heterogeneous, often unstructured and incomplete. Clinical settings are dynamic and contextual, with human-in-the-loop decisions. Errors have real consequences, requiring robustness and explainability. ‚û°Ô∏è What specific areas of ML model development are affected by these clinical challenges?\nQ2: What types of challenges emerge when applying ML in clinical settings? # Challenges include:\nData issues: missing values, coding errors, shift in distribution over time. Labeling: often derived from billing codes or heuristics‚Äînot always ground truth. Deployment: clinical workflows require integration, usability, and ethical oversight. ‚û°Ô∏è How does the clinical environment further complicate ML deployment?\nQ3: How does clinical practice shape ML model development? # Clinical workflows affect ML design because:\nModels must adapt to time constraints, decision pathways, and interdisciplinary teams. Interpretability and actionability are more important than raw performance. Stakeholders include not just data scientists, but also clinicians and patients. 2 Utility of Causative Model Predictions # Q1: Why is causality important in clinical machine learning? # Healthcare decisions often hinge on interventions, not just correlations:\nClinicians need to know: ‚ÄúWhat happens if I prescribe X?‚Äù Predicting causal outcomes is more useful than merely identifying associations. ‚û°Ô∏è How are most ML models limited when it comes to causality?\nQ2: What is the difference between predictive and causative models? # Predictive models estimate outcomes based on observed features. Causative models aim to model the effect of interventions or actions. Predictive models may reflect spurious correlations that fail when environments change. ‚û°Ô∏è What are the risks of using predictive models in clinical decisions?\nQ3: How can predictive models be misleading in practice? # Examples:\nPredicting lower mortality for asthma patients with pneumonia (due to ICU treatment). Models may recommend fewer ICU admissions for high-risk patients, leading to harm. These errors occur when models don‚Äôt account for treatment effects or confounding.\n‚û°Ô∏è How can ML practitioners improve model utility in healthcare?\nQ4: What approaches can align model outputs with clinical intent? # Incorporate domain expertise to define causal questions. Use causal inference frameworks (e.g., counterfactual analysis, propensity scores). Ensure models reflect the treatment-action relationship, not just outcome prediction. 3 Context in Clinical Machine Learning # Q1: Why is clinical context essential for interpreting ML models? # Machine learning models do not operate in isolation:\nClinical decisions depend on environmental, temporal, and institutional factors. Models trained in one hospital may fail in another due to context shifts. Context determines how predictions are used and trusted. ‚û°Ô∏è What types of context affect ML model performance?\nQ2: What are some examples of clinical context influencing ML predictions? # Differences in lab test ordering between departments. Temporal trends like new treatment guidelines. Resource availability: ICU beds, diagnostic equipment. These can change the meaning of input features and model outputs.\n‚û°Ô∏è How can ignoring context lead to unintended consequences?\nQ3: What are the risks of deploying context-unaware models? # Silent failures: model appears accurate but gives clinically invalid results. Harmful recommendations due to incorrect assumptions (e.g., missing a comorbidity). Equity concerns: unfair performance across hospitals or populations. ‚û°Ô∏è How can ML practitioners incorporate context into model development?\nQ4: What strategies help ensure models are context-aware? # Collaborate with domain experts to understand local workflows. Analyze data provenance and feature semantics. Perform site-specific validation before general deployment. Monitor and update models as context evolves. 4 Intrinsic Interpretability # Q1: What is interpretability and why is it vital in healthcare ML? # Interpretability refers to how easily a human can understand the reasoning behind a model‚Äôs prediction:\nClinicians need to justify decisions based on model outputs. Interpretability improves trust, safety, and regulatory compliance. Essential in high-stakes decisions like diagnosis and treatment. ‚û°Ô∏è What are different ways to achieve interpretability in ML?\nQ2: What is the difference between intrinsic and post-hoc interpretability? # Intrinsic interpretability: Models are interpretable by design (e.g., decision trees, linear models). Post-hoc interpretability: Use tools (e.g., SHAP, LIME) to explain black-box model behavior after training. Intrinsic models are simpler and easier to validate but may underperform on complex tasks.\n‚û°Ô∏è What are some examples of intrinsically interpretable models?\nQ3: What models are considered intrinsically interpretable? # Linear regression: Clear feature impact via coefficients. Decision trees: Transparent logic based on feature thresholds. Rule-based systems: Use if-then logic that mimics human reasoning. These models prioritize simplicity and clarity over complexity.\n‚û°Ô∏è How do we balance accuracy and interpretability in clinical settings?\nQ4: What are the trade-offs in choosing interpretable models? # Interpretable models may sacrifice accuracy on complex data. Black-box models may be more powerful but harder to validate and trust. Best practice: balance performance, interpretability, and clinical context. 5 Medical Data Challenges in Machine Learning Part 1 # Q1: What makes healthcare data particularly challenging for ML models? # Healthcare data is often:\nMessy: includes typos, missing values, inconsistent formats. Heterogeneous: comes from many sources‚ÄîEHRs, images, notes, sensors. Sparse and incomplete: many features are not consistently recorded. ‚û°Ô∏è What is one major source of complexity in healthcare data?\nQ2: Why is data heterogeneity a significant issue? # Different institutions and clinicians record data differently. Coding systems (e.g., ICD, CPT) vary across time and space. Input formats (structured vs. unstructured) require varied preprocessing. This complicates model generalization and reproducibility.\n‚û°Ô∏è Beyond format, what other data issues pose problems?\nQ3: How do missing and inaccurate labels impact ML models? # Labels are often derived from billing codes or heuristics, not confirmed ground truth. Human input can introduce label noise (e.g., misdiagnoses). This affects both training quality and model evaluation. ‚û°Ô∏è How can we start addressing these foundational issues?\nQ4: What practices help mitigate healthcare data challenges? # Collaborate with domain experts to verify labels and clean data. Use robust data preprocessing pipelines. Augment data via external sources or clinical knowledge bases. 6 Medical Data Challenges in Machine Learning Part 2 # Q1: What are additional complexities of working with medical data? # Beyond noise and heterogeneity, medical data also suffers from:\nTemporal issues: patient data spans time and requires sequence modeling. Label latency: outcomes may be delayed, leading to incomplete labels. Data leakage: unintended inclusion of future info during training. ‚û°Ô∏è How does temporality specifically impact ML in healthcare?\nQ2: Why is temporality a challenge in clinical ML modeling? # Events happen in a timeline, not in isolation. Features need to be time-aligned with outcomes. Some features (e.g., lab tests) are triggered by prior events, not independent signals. Incorrect handling can result in reverse causality or misleading models.\n‚û°Ô∏è What is label leakage and how does it affect models?\nQ3: What is label leakage and why is it dangerous? # Leakage occurs when features directly encode the outcome. Example: using post-diagnosis medication as a predictor of diagnosis. Results in inflated performance and useless real-world predictions. ‚û°Ô∏è How can we mitigate these issues during data preparation?\nQ4: What are best practices to reduce data leakage and temporal issues? # Carefully define observation and prediction windows. Exclude features generated after the outcome window. Collaborate with clinicians to spot illogical or circular data flows. 7 How Much Data Do We Need? # Q1: Why is data quantity important in healthcare ML? # More data typically improves model performance by:\nAllowing better generalization and reducing overfitting. Enabling complex models like deep learning to converge. Increasing coverage of rare cases and subpopulations. ‚û°Ô∏è Is there a rule of thumb for how much data is \u0026ldquo;enough\u0026rdquo;?\nQ2: Is there a specific data size needed to build reliable models? # There\u0026rsquo;s no universal threshold‚Äîdepends on task complexity and model type. Simpler models may perform well with smaller datasets. Deep learning typically requires large, diverse datasets for optimal performance. ‚û°Ô∏è Besides raw size, what else affects data utility?\nQ3: How does data diversity influence model robustness? # Diverse data improves generalization across patient subgroups. Reduces bias and enhances fairness. Captures a variety of clinical settings and disease presentations. ‚û°Ô∏è Are there diminishing returns with more data?\nQ4: Can collecting more data ever be inefficient or harmful? # Yes, when:\nData quality is low or inconsistent. Additional data doesn‚Äôt add new variation. Processing large datasets becomes computationally burdensome. Focus should be on quality, diversity, and relevance, not just quantity.\n8 Retrospective Data in Medicine and Shelf Life for Data # Q1: What is retrospective data and why is it commonly used in ML? # Retrospective data is historical clinical data collected during routine care:\nEasier and cheaper to obtain than prospective data. Often available in large volumes through EHRs. Used to develop predictive models and analyze outcomes. ‚û°Ô∏è What are limitations of using retrospective data?\nQ2: What are the risks and limitations of retrospective datasets? # Data reflects past practices, not current standards. Missingness and bias due to non-random documentation. Models may learn patterns that don‚Äôt generalize to new settings. ‚û°Ô∏è Can data lose value over time?\nQ3: What is the ‚Äúshelf life‚Äù of clinical data and why does it matter? # Shelf life refers to how long data remains relevant and useful:\nClinical protocols, technologies, and patient populations change. Models trained on outdated data may perform poorly on current cases. Regular model retraining and validation is needed. ‚û°Ô∏è How can we manage these issues when developing models?\nQ4: How should retrospective data be handled for effective modeling? # Understand the temporal context of data. Align modeling goals with clinical relevance and recency. Combine with prospective validation where possible. Plan for model monitoring and updates post-deployment. 9 Medical Data: Quality vs Quantity # Q1: Is more data always better in healthcare ML? # Not necessarily‚Äîquality can matter more than raw volume:\nPoor quality data introduces noise, bias, and misleading signals. High-quality, well-labeled data leads to better generalization and clinical utility. Trade-offs exist between collecting more vs. curating better data. ‚û°Ô∏è What does data ‚Äúquality‚Äù mean in practice?\nQ2: What are characteristics of high-quality medical data? # Accurate, clinically verified labels. Consistent formatting and standards (e.g., coding systems). Completeness and representativeness of the target population. Poor quality data may include irrelevant features or misdiagnosed labels.\n‚û°Ô∏è How can teams improve data quality?\nQ3: What practices can enhance data quality for ML? # Work closely with domain experts for data cleaning and labeling. Apply automated quality checks (e.g., missingness patterns, outlier detection). Use standard vocabularies (e.g., SNOMED, LOINC) to improve structure. ‚û°Ô∏è How should teams balance data quality and quantity?\nQ4: How should we approach the quality vs. quantity trade-off? # Prioritize relevant and diverse samples over raw scale. Smaller, higher-quality datasets often outperform large, noisy ones. Aim for balanced improvement across both dimensions where feasible. "},{"id":25,"href":"/healthcare/domain_knowledge/hands-on-healthcare-data/ch6_graph_ml/","title":"Ch6 ML and Graph Analytics","section":"Hands-On Healthcare Data","content":" Ch6 Machine Learning \u0026 Graph-Based Analytics Part 1: Q\u0026amp;A Summary # 1. What is the difference between cleaning, harmonization, and feature engineering? # Cleaning: Removing errors or inconsistencies in the raw data. Harmonization: Mapping and aligning data semantically across datasets (e.g., converting NDC to RxNorm). Feature Engineering: Transforming data to fit the needs of specific algorithms or analysis (e.g., PCA, one-hot encoding). 2. Why are graphs more useful for harmonization than feature engineering? # Graphs help link concepts across vocabularies, terminologies, or systems. Feature engineering tends to be model-specific and harder to generalize. 3. What are the downsides of repeating cleaning/harmonization for each project? # Redundancy: Same steps are repeated across projects. Inefficiency: Each team member duplicates similar work. Inconsistency: No central source of truth for processed data. 4. What is a feature store and how does it help? # A feature store centralizes reusable, preprocessed features. Helps reduce redundancy and promotes consistency. 5. How do knowledge graphs improve the pipeline? # Data is cleaned and harmonized once at the graph level. All downstream users can reuse the harmonized view via queries or APIs. 6. What assumptions are made when using a knowledge graph? # Patient-level data and terminology concepts are stored in the same graph. Nodes/edges are tagged with metadata (e.g., timestamps, source). The graph is a supergraph enabling subgraph extraction. 7. What are graph embeddings and why are they useful? # They convert graph structures into vectors usable in ML models. Enable pattern detection, similarity analysis, and deep learning. 8. What is node2vec? # Random walk-based graph embedding technique. Uses return (p) and in-out (q) parameters to tune graph walk. Captures homophily and structural equivalence. 9. What is cui2vec? # Embeds UMLS CUIs based on co-occurrence in various RWD sources. Context-aware (claims, notes, publications). Useful for understanding concept similarity. 10. What is med2vec? # Uses temporal sequence of medical events to create visit-based embeddings. Retains longitudinal context. 11. What is snomed2vec? # Embeds SNOMED CT concepts using hierarchical and network-based methods. Includes alternatives like metapath2vec and Poincar√© embeddings. 12. What are some challenges with pretrained embeddings? # Risk of overfitting to training data domain (e.g., CMS claims). May not generalize well to other populations or use cases. Introduces extra model layer to maintain and tune. Part 2: Curriculum-Style Breakdown with \u0026ldquo;Why\u0026rdquo; # üß≠ Phase 1: Understand the Motivation # Task: Read and distinguish between cleaning, harmonization, and feature engineering. Why: Clarifies each pipeline component and prevents misuse of graphs for tasks like feature engineering. üß± Phase 2: Explore Pipeline Challenges # Task: Analyze Figures 6-6 to 6-9 on pipeline repetition and inefficiency. Why: Understand how lack of standardization leads to duplicated efforts. üß† Phase 3: Learn about Feature Stores # Task: Study how feature stores centralize and reuse engineered features. Why: Saves time, increases reproducibility, and reduces tech debt. üåê Phase 4: Integrate Knowledge Graphs # Task: Understand what goes into a knowledge graph (patient data + ontologies). Why: Enables one-time harmonization per data source, allowing scalable reuse. üß© Phase 5: Explore Graph Embedding Techniques # Task: Implement node2vec on a small graph. Why: Learn homophily vs structural equivalence, key for biomedical graph reasoning. üß¨ Phase 6: Biomedical Concept Embeddings # Task: Compare and contrast cui2vec, med2vec, and snomed2vec. Why: Appreciate how embeddings differ by data type (temporal, co-occurrence, hierarchical). ‚ö†Ô∏è Phase 7: Real-World Concerns with Embeddings # Task: Evaluate pretrained embeddings and consider limitations (overfitting, generalizability). Why: Embeddings may look good on paper but can fail in new domains. üîÅ Phase 8: Apply to Your Use Case # Task: Pick a small real-world use case and simulate a pipeline using a knowledge graph and embedding. Why: Reinforces learning and identifies operational gaps in pipeline design. "},{"id":26,"href":"/ai-workflows/data/data-centric-ai/data-centric-evaluation/","title":"Data-Centric Evaluation of ML Models","section":"Data-Centric AI (DCAI)","content":" Data-Centric Evaluation of ML Models Instead of only asking how accurate, we ask why and where does the model fail ‚Äî and whether the data itself causes it. Aspect Model-Centric Evaluation Data-Centric Evaluation Focus Overall model score Specific weaknesses tied to data Metrics Accuracy, ROC, etc. Slice-specific accuracy, Error analysis Blind spots Hide rare data failures Detect hidden failures in data Error tracing Hard Directly trace errors to dirty/outlier/bias data Example \u0026ldquo;95% accurate model!\u0026rdquo; \u0026ldquo;Fails badly on young users with rare diseases\u0026rdquo; Mindset Improve model tuning Fix the dataset (quality, balance, coverage) Q1: What is the typical ML workflow before deployment? # Collect data and define the ML task. Explore and preprocess the data. Train a straightforward model. Investigate shortcomings in the model and dataset. Improve dataset and model iteratively. Deploy the model and monitor for new issues. Q2: Why is model evaluation critical? # Evaluation affects practical outcomes in real-world applications. Poor evaluation choices can lead to misleading or harmful models. Q3: What are examples of evaluation metrics for classification? # Accuracy, balanced accuracy, precision, recall, log loss, AUROC, calibration error. Q4: What are some pitfalls in model evaluation? # Data leakage by using non-held-out data. Misspecified metrics hiding failures in subpopulations. Validation data not representing deployment settings. Label errors. Q5: How is text generation model evaluation different? # Human evaluations (üëçüëé or Likert scales). LLM evaluations with multiple criteria. Automated metrics like ROUGE, BLEU, and Perplexity. Q6: What is a data slice? # A subset of the dataset sharing a common characteristic, e.g., different sensor types, demographics. Q7: Why is it insufficient to delete sensitive features to address slice fairness? # Slice membership information may be correlated with other features. Q8: How can we improve model performance for underperforming slices? # Use a more flexible model. Over-sample the minority subgroup. Collect more data from the subgroup. Engineer new features that better capture subgroup specifics. Q9: How to discover underperforming subpopulations? # Sort validation examples by loss. Cluster high-loss examples to find commonalities. Q10: What are typical causes of wrong predictions? # Incorrect labels. Examples that do not belong to any class. Outlier examples. Model type limitations. Conflicting or noisy dataset labels. Q11: What actions can address wrong predictions? # Correct labels. Remove fundamentally unpredictable examples. Augment or normalize outlier examples. Fit better model architectures or do feature engineering. Enrich the dataset to distinguish overlapping classes. Q12: What is the concept of leave-one-out influence? # Measure the impact of omitting a datapoint on the model‚Äôs validation performance. Q13: What is Data Shapley? # A method that averages the influence of a datapoint over all subsets containing it, providing a fairer measure of its importance. Q14: How can we approximate influence? # Monte Carlo sampling methods. Closed-form approximations for simple models like linear regression and k-NN. Q15: Why review influential samples? # Correcting highly influential mislabeled examples can lead to significant accuracy improvements. References # Cook‚Äôs Distance (Linear Regression Influence) Similarity Search Scaling for Big Data (Johnson et al., 2019) Trustworthy Data Influence Estimation Confident Learning and Cleanlab Project "},{"id":27,"href":"/ai-workflows/genai/5-day-genai-google-2025/day5_mlops/","title":"Day 5 ‚Äì MLOps for Generative AI","section":"5-Day GenAI with Google 2005","content":" Day 5 ‚Äì MLOps for Generative AI 1. Introduction # The rise of foundation models and generative AI (gen AI) has brought a paradigm shift in how we build and deploy AI systems. From selecting architectures to managing prompts and grounding outputs in real data, traditional MLOps needs adaptation.\nSo how do we evolve MLOps for this new generative world?\n2. What Are DevOps and MLOps? # DevOps: Automation + collaboration for software delivery (CI/CD, testing, reliability) MLOps: Adds ML-specific needs: Data validation Model evaluation Monitoring Experiment tracking These core principles set the stage, but gen AI has unique needs.\n3. Lifecycle of a Gen AI System # The gen AI lifecycle introduces five major moments:\nDiscover ‚Äì Find suitable foundation models from a rapidly growing model zoo. Develop \u0026amp; Experiment ‚Äì Iterate on prompts, use few-shot examples, and chains. Train/Tune ‚Äì Use parameter-efficient fine-tuning. Deploy ‚Äì Includes chains, prompt templates, databases, retrieval systems. Monitor \u0026amp; Govern ‚Äì Ensure safety, fairness, drift detection, and lineage. Each stage requires new tooling and processes compared to traditional ML.\n4. Continuous Improvement in Gen AI # Gen AI focuses on adapting pre-trained models via: Prompt tweaks Model swaps Multi-model chaining Still uses fine-tuning and human feedback loops when needed. But not all orgs handle base model training‚Äîmany just adapt existing FMs.\n5. Discover Phase: Choosing the Right FM # Why it‚Äôs hard:\nExplosion of open-source and proprietary FMs Variation in architecture, performance, licensing Model selection is now a critical MLOps task.\n6. Model Discovery Criteria # Choosing a foundation model now involves nuanced trade-offs:\nQuality: Benchmarks, output inspection Latency \u0026amp; Throughput: Real-time chat ‚â† batch summarization Maintenance: Hosted vs self-managed models Cost: Compute, serving, data storage Compliance: Licensing, regulation Vertex Model Garden supports structured exploration of these options.\n7. Develop \u0026amp; Experiment # Building gen AI systems is iterative: prompt tweaks ‚Üí model swap ‚Üí eval ‚Üí repeat.\nThis loop mirrors traditional ML but centers around prompts, not raw data.\n8. Foundation Model Paradigm # Unlike predictive models, foundation models are multi-purpose. They show emergent behavior based on prompt structure. Prompts define task type (translation, generation, reasoning). Small changes in wording can completely shift model output.\n9. Prompted Model Component # The key unit of experimentation in gen AI is: Prompt + Model ‚Üí Prompted Model Component\nThis redefines MLOps: you now track prompt templates as first-class artifacts.\n10. Prompt = Code + Data # Prompts often include:\nCode-like structures (templates, control flow, guardrails) Data-like elements (examples, contexts, user input) MLOps must version prompts, track results, and match to model versions.\n11. Chains \u0026amp; Augmentation # When prompts alone aren‚Äôt enough:\nChains: Link multiple prompted models + APIs RAG: Retrieve relevant info before generation Agents: LLMs choose tools dynamically (ReAct) MLOps must manage chains end-to-end, not just components.\n12. Chain MLOps Needs # Evaluation: Run full chains to measure behavior Versioning: Chains need config + history Monitoring: Track outputs + intermediate steps Introspection: Debug chain inputs/outputs Vertex AI + LangChain integration supports these needs.\n13. Tuning \u0026amp; Training # Some tasks require fine-tuning:\nSFT: Teach model to produce specific outputs RLHF: Use human feedback to improve alignment Tune as needed‚Äîespecially if prompt engineering hits limits.\n14. Continuous Tuning # Static tasks = low frequency. Dynamic tasks (chatbots) = frequent RLHF.\nBalance GPU/TPU cost with improvement needs Consider quantization to lower costs Vertex AI provides tuning infra + registry + pipelines + governance.\n15. Data in Gen AI # Unlike predictive ML, gen AI uses:\nPrompts \u0026amp; examples Grounding sources (APIs, vectors) Human preference data Task-specific tuning sets Synthetic + curated data Each has different MLOps needs: validation, versioning, lineage.\n16. Synthetic Data Use Cases # Generation: Fill in training gaps Correction: Flag label errors Augmentation: Introduce diversity Use large FMs to generate training or eval data when needed.\n17. Evaluation in Gen AI # Evaluation is hard:\nComplex, open-ended outputs Metrics (BLEU, ROUGE) often miss the mark Auto-evals (e.g. AutoSxS) use FMs as judges Align automated metrics with human judgment early on.\n18. Evaluation Best Practices # Stabilize metrics, approaches, datasets early Include adversarial prompts in test set Use synthetic ground truth if needed Evaluation = cornerstone of experimentation in gen AI MLOps\n19. Deployment in Gen AI Systems # Gen AI apps involve multiple components:\nLLMs Chains Prompts Adapters External APIs Two main deployment types:\nFull Gen AI Systems (custom apps) Foundation Model Deployments (standalone models) 20. Version Control # Key assets to version:\nPrompt templates Chain definitions Datasets (e.g. RAG sources) Adapter models Git, BigQuery, AlloyDB, and Vertex Feature Store help manage assets.\n21. Continuous Integration (CI) # CI ensures reliability through:\nUnit + integration tests Automated pipelines Challenges:\nTest generation is hard due to open-ended outputs Reproducibility is limited due to LLM randomness Solutions draw from earlier evaluation methods.\n22. Continuous Delivery (CD) # CD moves tested systems into staging/production.\nTwo flavors:\nBatch delivery: Schedule-driven, test pipeline throughput Online delivery: API-based, test latency, infra, scalability Chains are the new \u0026ldquo;deployment unit\u0026rdquo;‚Äînot just models.\n23. Foundation Model Deployment # Heavy resource demands ‚Üí need:\nGPU/TPU allocation Scalable data stores Optimization (distillation, quantization, pruning) 24. Infrastructure Validation # Check:\nHardware compatibility Serving configuration GPU/TPU availability Tools: TFX infra validation, manual provisioning checks\n25. Compression \u0026amp; Optimization # Strategies:\nQuantization: 32-bit ‚Üí 8-bit Pruning: Remove unneeded weights Distillation: Train small model from a larger \u0026ldquo;teacher\u0026rdquo; Step-by-step distillation can reduce size and improve performance.\n26. Deployment Checklist # Steps to productionize:\nVersion control Optimize model Containerize Define hardware and endpoints Allocate resources Secure access Monitor, log, and alert Real-time infra: Cloud Functions + Cloud Run 27. Logging \u0026amp; Monitoring # Track both:\nApp-level inputs/outputs Component-level details (chain steps, prompts, models) Needed for tracing bugs, debugging drift, and transparency.\n28. Drift \u0026amp; Skew Detection # Compare:\nEvaluation-time data vs. Production input Topics, vocab, token count, embeddings Techniques:\nMMD, least squares density, learned kernels Signals shift in user behavior or data domains.\n29. Continuous Evaluation # Capture live outputs Evaluate vs. ground truth or human feedback Track metric degradation Alert on failures or decay Production = where real testing happens.\n30. Governance # Governs:\nChains + components Prompts Data Models Evaluation metrics and lineage Full lifecycle governance = essential for compliance and maintainability.\n31. Role of an AI Platform # Vertex AI acts as an end-to-end platform for developing and operationalizing Gen AI. It supports:\nData prep Training/tuning Deployment Evaluation CI/CD Monitoring Governance It enables reuse, scalability, and full-stack observability for Gen AI teams.\n32. Model Discovery: Vertex Model Garden # Model Garden includes:\n150+ models: Google, OSS, third-party (e.g., Gemini, Claude, Llama 3, T5, Imagen) Modalities: Language, Vision, Multimodal, Speech, Video Tasks: Generation, classification, moderation, detection, etc. Each model has a card with use cases and tuning options.\n33. Prototyping: Vertex AI Studio # Vertex AI Studio offers:\nPlayground for trying models (Gemini, Codey, Imagen) UI + SDKs (Python, NodeJS, Java) Prompt testing + management One-click deploy Built-in notebooks (Colab Enterprise, Workbench) Low barrier for users from business analysts to ML engineers.\n34. Training: Full LLM Training on Vertex AI # TPU and GPU infrastructure for fast, large-scale training Vertex AI supports training from scratch and adapting open-weight models 35. Tuning: Five Key Methods # Prompt engineering ‚Äì no retraining SFT (Supervised Fine-Tuning) ‚Äì train on labeled examples RLHF ‚Äì learn from human preferences Distillation ‚Äì compress knowledge from large to small models Step-by-step distillation ‚Äì Google-optimized, fewer data needs Each method balances cost, performance, and latency.\n36. Orchestration: Vertex Pipelines # Define pipelines with Kubeflow SDK Automate tuning, evaluation, and deployment Managed pipelines for Vertex foundation models Enables production-readiness and repeatability.\n37. Chain \u0026amp; Augmentation: Grounding + Function Calling # Vertex AI supports:\nRAG systems ‚Äì real-time document retrieval Agent-based chains ‚Äì dynamic tool use via ReAct Function calling ‚Äì LLM picks which API to use, returns JSON Grounding ‚Äì verifies/model output via search or private corpora Agent Builder ‚Äì build search/chat agents grounded on any source Simplifies chaining, reasoning, and integrating internal data.\n38. Vector Search # Vertex AI Vector Search enables:\nHigh-scale, low-latency ANN search Billions of embeddings using ScaNN Use with text, images, hybrid metadata search Works with custom embeddings (e.g., textembedding-gecko) Choose this when you need control over chunking, retrieval, or models.\n39. Evaluate: Vertex AI Experiments \u0026amp; TensorBoard # Experimentation is essential for iterating and improving Gen AI models. Tools include:\nVertex AI Experiments: Track model runs, hyperparams, training environments Vertex AI TensorBoard: Visualize loss, accuracy, embeddings, model graphs Supports reproducibility, debugging, and collaboration.\n40. Evaluation Techniques # Ground truth metrics: Automatic Metrics using reference datasets LLM-based eval: Auto Side-by-Side (Auto SxS) with model judges Rapid Evaluation API: Fast SDK-based eval for prototyping Evaluation is deeply integrated into the development lifecycle.\n41. Predict: Vertex Endpoints # Deploy models to Vertex Endpoints for online prediction Features: Autoscaling Access control Monitoring Works with open-source and Google models 42. Safety, Bias, and Moderation # Built-in responsible AI features:\nCitation checkers: Track and quote data sources Safety scores: Detect harmful content and flag sensitive topics Watermarking: Identify AI-generated content (via SynthID) Bias detection: Ensure fairness and appropriateness Moderation: Filter unsafe responses These ensure ethical and trustworthy AI deployments.\n43. Governance Tools # Vertex Feature Store:\nTrack embedding + feature lineage Drift monitoring Feature reuse + formulas Model Registry:\nLifecycle tracking (versioning, evaluation, deployment) One-click deployment Access to evaluation, monitoring, and aliasing Dataplex:\nCross-product lineage (e.g., Vertex + BigQuery) Golden datasets/models Access governance + IAM integration These unify observability, reproducibility, and compliance across Gen AI assets.\n44. Conclusion # MLOps principles‚Äîreliability, scalability, repeatability‚Äîfully extend into Gen AI.\nGen AI adds prompt chaining, grounding, function calling, etc. Vertex AI unifies the full lifecycle across models, pipelines, and governance It supports both predictive and Gen AI use cases MLOps isn‚Äôt replaced‚Äîit‚Äôs expanded for the age of foundation models.\n"},{"id":28,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m6/","title":"[Summary]  Module 6: Best Practices, Terms, and Launching Your ML Journey","section":"C3 ML Healthcare","content":" Module 6: Best Practices, Terms, and Launching Your ML Journey # 1 Clinical Utility and Output Action Pairing # Q1: What is clinical utility and why is it important in ML? # Clinical utility refers to the real-world usefulness of a model‚Äôs predictions:\nA model must enable action that improves outcomes. Predictions that can\u0026rsquo;t lead to interventions or decisions have limited utility. This bridges the gap between technical performance and clinical relevance. ‚û°Ô∏è How can we ensure predictions are actually actionable?\nQ2: What is Output-Action Pairing (OAP) and how does it help? # OAP connects model outputs to a specific, predefined action:\nDefines what should happen when a model gives a certain prediction. Ensures the output aligns with clinical workflows and capabilities. Encourages careful thought about how predictions will be used. ‚û°Ô∏è What are some examples of OAP in clinical practice?\nQ3: What are practical examples of Output-Action Pairing? # Sepsis risk prediction ‚Üí Early IV antibiotic administration. Fall risk ‚Üí Increase room monitoring and physical therapy. Readmission risk ‚Üí Social work referral or discharge planning. Clear linkage between prediction and intervention enhances adoption and trust.\n‚û°Ô∏è How does OAP guide model design and deployment?\nQ4: How does OAP influence model development choices? # Guides feature selection based on actionability. Helps prioritize precision or recall depending on the intervention. Encourages stakeholder involvement early to define clinical utility goals. 2 Taking Action - Utilizing the OAP Framework # Q1: How can the OAP framework be applied systematically? # The OAP (Output-Action Pairing) framework provides a structured approach:\nStart with the desired clinical action or intervention. Work backward to determine the prediction needed to support it. Design the model with this action-prediction link as the anchor. ‚û°Ô∏è What questions help clarify a good OAP strategy?\nQ2: What questions can guide effective Output-Action Pairing? # What clinical decision is this prediction meant to support? Who will take action based on the output? What are the consequences of false positives or negatives? Is there an existing workflow where this model fits? These guide the framing, design, and evaluation of the ML tool.\n‚û°Ô∏è Can the OAP framework prevent wasted effort or misaligned tools?\nQ3: What happens when models are built without OAP thinking? # Outputs may be ambiguous or non-actionable. Teams may build models no one knows how to use. Integration into practice becomes difficult or ineffective. OAP increases the likelihood of real-world impact.\n‚û°Ô∏è How does OAP support multidisciplinary collaboration?\nQ4: How does OAP promote stakeholder alignment? # Encourages communication between clinicians, engineers, and operational teams. Helps align goals, expectations, and implementation details. Everyone shares a clear understanding of what the model is for and how it will be used. 3 Building Multidisciplinary Teams for Clinical Machine Learning # Q1: Why are multidisciplinary teams essential in clinical ML projects? # Healthcare ML requires collaboration across domains:\nCombines technical expertise with clinical knowledge. Ensures models are grounded in real-world workflows. Increases likelihood of successful design, deployment, and adoption. ‚û°Ô∏è What roles are typically involved in such teams?\nQ2: Who are the key stakeholders in a clinical ML team? # Clinicians: define problems, validate utility, assess safety. Data scientists/engineers: model design, feature extraction, validation. IT and informatics staff: EHR integration, data access. Administrators and ethics leaders: compliance, governance, resourcing. Diverse perspectives help balance performance with feasibility and ethics.\n‚û°Ô∏è How do team dynamics influence project success?\nQ3: What practices foster effective collaboration? # Shared language and goals: use tools like OAP to define objectives. Iterative feedback loops with clinicians. Respect for domain boundaries and active listening. Successful teams recognize that technical and clinical inputs are equally critical.\n‚û°Ô∏è What challenges can arise in interdisciplinary settings?\nQ4: What are common barriers and how can they be addressed? # Misaligned incentives or timelines. Communication breakdowns or unclear roles. Resistance to change or model integration. Solution: Foster trust, transparency, and frequent engagement across disciplines.\n4 Governance, Ethics, and Best Practices # Q1: Why is governance important in clinical machine learning? # Governance ensures ML tools are:\nSafe, fair, and transparent. Aligned with legal and institutional standards. Routinely monitored and updated. It defines who is accountable for model design, deployment, and oversight.\n‚û°Ô∏è What are key components of ethical ML in healthcare?\nQ2: What ethical principles guide responsible ML in medicine? # Fairness: equitable performance across patient groups. Transparency: clear communication of model limitations and risks. Accountability: defined roles for decision-making and error handling. Beneficence: focus on patient well-being and do-no-harm principles. ‚û°Ô∏è How do we institutionalize these principles?\nQ3: What governance practices help enforce ethical use of ML? # Establish ML oversight committees with clinical and technical members. Create model review boards for performance and fairness evaluations. Define escalation plans for failures or unexpected behavior. Governance should be proactive, not reactive.\n‚û°Ô∏è What practical best practices support these efforts?\nQ4: What are some operational best practices in clinical ML? # Regular audits and performance monitoring. Document model versioning, data lineage, and deployment status. Ensure interdisciplinary sign-off before going live. Build models with real-world constraints and fail-safes in mind. 5 On Being Human in the Era of Clinical Machine Learning # Q1: What role do humans continue to play in clinical ML systems? # Even with advanced ML, humans remain central:\nClinicians interpret outputs in nuanced, value-laden contexts. Patients bring individual preferences and lived experiences. Human oversight is essential for ethical and compassionate care. ‚û°Ô∏è Why might fully automated decisions be problematic in healthcare?\nQ2: What are risks of excessive automation in clinical ML? # Models may lack empathy or context-specific judgment. Overreliance can lead to de-skilling or clinician disengagement. Errors may go unchallenged if clinicians defer too heavily to automation. Human clinicians provide interpretive judgment and ensure care remains individualized.\n‚û°Ô∏è How can we design systems that support, not replace, human judgment?\nQ3: How do we build ML systems that augment rather than replace clinicians? # Keep clinicians ‚Äúin the loop‚Äù‚Äîwith tools to override or question model outputs. Design interfaces for transparency and explanation, not just prediction. Support human strengths: empathy, narrative understanding, ethical judgment. ‚û°Ô∏è What values should guide human-machine collaboration in healthcare?\nQ4: What values should ML practitioners center in their design? # Respect for human dignity and individual autonomy. Empowerment, not displacement, of healthcare professionals. Continuous attention to how technology shapes behavior and trust. 6 Death by GPS and Other Lessons of Automation Bias # Q1: What is automation bias and why is it dangerous in healthcare? # Automation bias is the tendency to:\nOvertrust machine-generated suggestions, even when flawed. Ignore or discount human judgment in favor of algorithmic outputs. Lead to harmful or fatal errors, especially in high-stakes domains. ‚û°Ô∏è What are real-world examples of automation bias?\nQ2: What lessons do we learn from non-healthcare automation failures? # Example: ‚ÄúDeath by GPS‚Äù‚Äîdrivers blindly following GPS into unsafe areas.\nSimilar dynamics occur in medicine when clinicians follow flawed model predictions. Automation can make errors seem more trustworthy due to perceived objectivity. ‚û°Ô∏è How can we design systems to guard against this?\nQ3: How can ML systems reduce risk of automation bias? # Provide confidence scores, explanations, and alternative scenarios. Train users to critically evaluate model outputs. Design alerts and interfaces that encourage reflective judgment, not blind acceptance. ‚û°Ô∏è What role do institutions and governance play?\nQ4: How should organizations manage automation risks? # Regular audits for model drift and edge-case failures. Create feedback loops so users can flag concerning outputs. Promote a culture where questioning automation is encouraged. "},{"id":29,"href":"/ai-workflows/data/data-centric-ai/data-curation-llms/","title":"Data Curation and LLMs","section":"Data-Centric AI (DCAI)","content":" Data Curation and LLMs Q1: What is the background of LLMs discussed? # LLMs like ChatGPT, GPT-4, and Llama are sequence models trained to predict the next token. They use unsupervised pre-training on massive internet-scale corpora and can solve various NLP tasks. Q2: What are the applications of LLMs discussed? # Zero-shot prompting Few-shot prompting Q3: What is the focus of this lecture? # Using LLMs for data curation Evaluating LLM output data Curation for LLM pre-training and application fine-tuning Q4: How are LLMs used for data curation? # They act as powerful, flexible, and computationally inexpensive reasoning engines for text data curation. Q5: How was PII detection handled traditionally vs with LLMs? # Traditional: Custom regex rules. With LLMs: Zero-shot prompting to detect a wider range of PII without needing extensive rule-writing. Q6: How can grammar checking be improved with LLMs? # Traditional: Rule-based systems like LanguageTool. LLM-based: Fine-tuning LLMs on curated grammatical acceptability datasets like CoLA. Q7: What is a major challenge in working with LLM outputs? # Hallucinations, where LLMs produce confidently incorrect information. Q8: How can we evaluate LLM outputs more reliably? # Use a stronger LLM (e.g., GPT-4) to judge the outputs of weaker LLMs. Q9: What evidence supports LLM-based evaluation? # AlpaGasus fine-tuning project showed better results by curating higher-quality data points evaluated by GPT-3.5 and GPT-4. Q10: What is the challenge of using LLMs to evaluate other LLMs? # It risks circular evaluation (turtles all the way down) if the same LLMs are involved in both generation and evaluation. Q11: What method helps with LLM uncertainty quantification? # Natural Language Inference (NLI)-based techniques that check for answer contradictions. Q12: What are the key stages of data curation for LLM pre-training? # Focus on corpus quality because errors are difficult to \u0026ldquo;un-learn.\u0026rdquo; Use supervised fine-tuning and reinforcement learning from human feedback. Q13: How does data curation differ for LLM applications? # Zero-shot prompting Few-shot prompting Retrieval-augmented generation Supervised fine-tuning Q14: Why is fine-tuning important for LLM applications? # Fine-tuning provides the best task-specific performance. It enables training smaller models to match large model performance through synthetic data generation. Q15: How is synthetic data curated for LLM fine-tuning? # Generate synthetic data using a powerful LLM. Use uncertainty quantification to retain only high-confidence examples. Train classifiers to filter out unrealistic synthetic data. Q16: What is the future trend in data curation for LLMs? # Growth of powerful multi-modal LLMs and new tools like CleanVision and GPT-4 to automate and improve data quality. References # Karpathy on LLM prompting Scrubadub for regex-based PII detection LanguageTool grammar checker CoLA dataset AlpaGasus paper Trustworthy Language Models (TLM) Textbooks Are All You Need (Li et al., 2023) Stanford Alpaca project CleanVision project "},{"id":30,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/summary_m7/","title":"[Summary] Module 7: Foundation Models","section":"C3 ML Healthcare","content":" Module 7: Foundation Models # 1 Introduction to Foundation Models # Q1: What are foundation models and why are they significant? # Foundation models are large-scale models trained on massive datasets:\nThey can be adapted for many downstream tasks with minimal fine-tuning. Examples include models like BERT, GPT, and CLIP. Represent a shift from building task-specific models to training one model for many uses. ‚û°Ô∏è Why have foundation models become so prominent recently?\nQ2: What has enabled the rise of foundation models? # Scale of data and compute: Internet-scale corpora and powerful GPUs/TPUs. Advances in transformer architectures and self-supervised learning. Institutional and commercial support from industry leaders. These advances allow models to generalize better across modalities and domains.\n‚û°Ô∏è How do foundation models differ from traditional ML approaches?\nQ3: What makes foundation models different from conventional models? # Traditional models are narrow in scope‚Äîtrained for a single task. Foundation models are broad and general-purpose, and adaptable post-training. They transfer knowledge learned from pretraining to new problems. ‚û°Ô∏è What impact might this have on healthcare?\nQ4: What is the potential of foundation models in medicine? # Accelerate ML adoption in health by reducing data and engineering needs. Enable multi-task and multimodal learning (e.g., notes + imaging). Could transform how healthcare systems approach AI tool development. 2 Adapting to Technology # Q1: Why is adaptation critical for deploying foundation models in healthcare? # Even powerful models need to be aligned with local context and goals:\nFoundation models are trained on general data, not tailored clinical settings. Without adaptation, predictions may be irrelevant or unsafe. Local fine-tuning ensures performance reflects specific use cases. ‚û°Ô∏è What are the main ways to adapt foundation models?\nQ2: What are strategies for adapting foundation models to healthcare? # Prompting: crafting task-specific input formats. Fine-tuning: retraining on domain-specific data. Adapter layers: lightweight modules inserted into the model to customize behavior. Adaptation can be computationally efficient and task-specific.\n‚û°Ô∏è What are the clinical implications of adaptation?\nQ3: How does adaptation support clinical relevance and safety? # Tailors outputs to match local patient population characteristics. Ensures alignment with regulations, vocabularies, and care guidelines. Enables deployment in settings with limited training data. ‚û°Ô∏è Are there risks associated with improper adaptation?\nQ4: What are potential pitfalls if adaptation is not done carefully? # Models may produce hallucinated or outdated information. Can reflect biases from pretraining data. Lack of transparency in adaptation may hinder clinical trust and oversight. 3 General AI and Emergent Behavior # Q1: What is meant by emergent behavior in foundation models? # Emergent behavior refers to capabilities that arise unexpectedly as model scale increases:\nSkills not explicitly programmed or observed in smaller models. Includes reasoning, translation, code generation, and more. Reflects how large-scale training enables complex pattern learning. ‚û°Ô∏è Why is this behavior important to understand in healthcare?\nQ2: What risks and opportunities do emergent behaviors present? # Can lead to unexpected breakthroughs in performance. But may also result in unpredictable outputs or failure modes. Raises concerns about controllability, bias, and hallucination. Emergence adds power‚Äîand responsibility.\n‚û°Ô∏è What does this imply for clinical AI deployment?\nQ3: How should healthcare practitioners approach emergent AI behavior? # Be aware that model behavior may shift with size or updates. Validate models thoroughly in specific settings. Favor models with transparency, safety checks, and ability to decline uncertain tasks. ‚û°Ô∏è How does this relate to broader conversations about general AI?\nQ4: What is general AI, and are foundation models a step toward it? # General AI (AGI) refers to models that perform well across diverse tasks without retraining. Foundation models exhibit early signs of generalization, but are not AGI. Healthcare must remain cautious and evidence-driven in their application. 4 How Foundation Models Work # Q1: What architectures power most foundation models? # Most foundation models are built using transformers:\nUse self-attention to model relationships between tokens. Enable parallel processing of sequences for scale and speed. Form the backbone of models like GPT, BERT, and T5. ‚û°Ô∏è How are these models trained at scale?\nQ2: What are key aspects of training foundation models? # Pretraining on large corpora using self-supervised tasks. Use of masked language modeling or next-token prediction. Require massive compute resources and careful engineering. ‚û°Ô∏è How do these models adapt to downstream tasks?\nQ3: What methods are used to fine-tune foundation models? # Supervised fine-tuning on labeled datasets. Prompt tuning or instruction tuning to guide behavior. Some use reinforcement learning from human feedback (RLHF). These allow general models to become specialized.\n‚û°Ô∏è What‚Äôs important to know about input/output design?\nQ4: How are inputs and outputs structured in foundation models? # Inputs are tokenized text or sequences (can include images or other modalities). Outputs may be text, logits, embeddings, or class labels. Models often respond based on context and prompt structure. 5 Healthcare Use Cases for Text Data # Q1: Why is text data important in healthcare? # Much of healthcare documentation is unstructured text:\nClinical notes, radiology reports, discharge summaries, referrals. Contains nuanced, contextual information not captured in structured fields. Foundation models offer new ways to process and understand this text. ‚û°Ô∏è What can foundation models do with clinical text?\nQ2: What are key use cases for foundation models on text data? # Summarization of long clinical documents. De-identification for research use. Clinical question answering and decision support. ICD code prediction or billing optimization. Models can handle complex reasoning and language generation.\n‚û°Ô∏è How do foundation models compare to traditional NLP in this space?\nQ3: How do foundation models improve upon older NLP approaches? # Handle longer contexts with attention mechanisms. Better generalization and language understanding. Can be prompted or fine-tuned for specific clinical tasks. They reduce the need for hand-crafted features and rules.\n‚û°Ô∏è What considerations are needed for responsible use?\nQ4: What are risks of using foundation models on clinical text? # Hallucination: generating incorrect or fabricated content. Bias from training data may propagate into outputs. Lack of transparency in decision-making logic. Mitigation includes domain-specific fine-tuning, human review, and guardrails.\n6 Healthcare Use Cases for Non-textual Unstructured Data # Q1: What kinds of non-text data are common in healthcare? # Medical imaging: X-rays, MRIs, CT scans. Waveforms: ECG, EEG, vital signs. Genomics and biosignals. These data types are unstructured and require specialized models. ‚û°Ô∏è How can foundation models be applied to these modalities?\nQ2: What are use cases for foundation models beyond text? # Radiology report generation from medical images. Multimodal fusion of images and text (e.g., CLIP-style models). Pattern recognition in long physiological time series (e.g., ICU monitors). Genomic feature embedding for disease prediction or drug discovery. ‚û°Ô∏è What benefits do foundation models bring to these domains?\nQ3: How do foundation models enhance non-text applications? # Enable end-to-end training from raw inputs. Reduce need for handcrafted pipelines and feature engineering. Capture latent structure across modalities (e.g., linking image with diagnosis text). ‚û°Ô∏è Are there challenges unique to unstructured non-text data?\nQ4: What limitations exist when applying foundation models to these data types? # Require large, annotated datasets for effective transfer. Data standardization and privacy are complex for medical images/genomics. Interpretability is often harder than with text-based models. 7 Challenges and Pitfalls # Q1: What challenges come with using foundation models in healthcare? # Despite their potential, foundation models raise several issues:\nBias from large-scale internet training data. Hallucination: generating confident but incorrect outputs. Lack of transparency in decision-making. These issues can have significant consequences in clinical environments.\n‚û°Ô∏è What technical limitations exist in current foundation models?\nQ2: What are some technical pitfalls of foundation models? # Struggle with numerical accuracy and reasoning. Prone to context drift or forgetting task instructions. Outputs can be nonsensical if prompts are ambiguous or poorly structured. ‚û°Ô∏è What risks emerge in deployment and regulation?\nQ3: What deployment risks should healthcare teams be aware of? # Difficult to monitor and validate evolving model behavior. Hard to meet regulatory and documentation standards (e.g., FDA). Models may make hidden decisions that are hard to audit. ‚û°Ô∏è How can we mitigate these challenges?\nQ4: What strategies help address the pitfalls of foundation models? # Fine-tuning with domain data to reduce hallucination and bias. Combine models with clinical guardrails and human oversight. Use evaluation benchmarks aligned with safety and clinical goals. 8 Conclusion # Q1: What are the transformative aspects of foundation models in healthcare? # Enable multi-task and multimodal learning. Reduce barriers to entry for building clinical AI tools. Allow fine-tuning and prompting rather than starting from scratch. Foundation models shift how we think about problem-solving with ML.\n‚û°Ô∏è What is the main caution despite their promise?\nQ2: What precautions should be taken before clinical deployment? # Ensure validation on real-world clinical data. Understand risks of hallucination, bias, and overconfidence. Incorporate governance and human oversight mechanisms. ‚û°Ô∏è What‚Äôs the path forward for teams using these models?\nQ3: How can healthcare teams prepare for working with foundation models? # Build multidisciplinary teams (clinicians, engineers, ethicists). Invest in data infrastructure for secure and high-quality inputs. Prioritize clinically relevant problems where models can augment care. "},{"id":31,"href":"/ai-workflows/data/data-centric-ai/growing-or-compressing-datasets/","title":"Growing or Compressing Datasets","section":"Data-Centric AI (DCAI)","content":" Growing or Compressing Datasets Q1: What is the main focus of this lecture? # Techniques to carefully select examples for labeling to reduce the burden in ML systems. Growing datasets via active learning and compressing datasets via core-set selection. Q2: What is active learning? # A method to intelligently select the most informative examples to label next, maximizing model improvement with fewer labeled samples. Q3: How does pool-based active learning work? # Start with a pool of unlabeled examples. At each round, score examples using an acquisition function (e.g., entropy of predicted probabilities). Select and label top examples. Retrain the model with newly labeled data. Q4: What is a common acquisition function used in active learning? # Entropy of the predicted class probabilities, encouraging labeling of uncertain examples. Q5: How does active learning compare to passive learning? # Active learning exponentially improves data efficiency compared to random/passive sampling. Q6: What practical challenges does active learning face? # High computational costs with large models and datasets. Q7: How can active learning be made more practical? # Batch active learning with diversity selection. Efficient candidate selection using methods like SEALS to reduce search space. Q8: What is SEALS? # Similarity Search for Efficient Active Learning and Search of Rare Concepts. Uses nearest neighbor search in embedding space to limit active learning candidate pool. Q9: What is core-set selection? # Choosing a small representative subset of a large labeled dataset that preserves model performance. Q10: Why is core-set selection important? # When we have massive datasets, it reduces computational, time, and energy costs without sacrificing accuracy. Q11: What methods help with core-set selection? # Greedy k-centers approach. Selection via Proxy: using smaller proxy models to guide subset selection. Q12: How does Selection via Proxy work? # Train a lightweight model (proxy) on the full data. Use it to select a subset for training a larger model, speeding up training significantly. Q13: What are key takeaways about dataset growth and compression? # Active learning enables data-efficient labeling for growing datasets. Core-set selection enables training efficiency for already large datasets. Active Learning vs. Confident Learning # Category Active Learning Confident Learning Main Goal Select the most informative examples to label next Find mislabeled examples in existing labeled data When Used During dataset growth (annotation phase) After labels exist (cleaning phase) Data State Partially labeled data pool Fully labeled (but noisy) dataset Model Uncertainty Usage Samples highest-uncertainty examples for human labeling Detects label inconsistency via confidence estimation Human Role Label new examples Review and correct suspicious labels Output New labels added to dataset List of potential label errors to fix Typical Workflow Train model ‚Üí Select uncertain points ‚Üí Human annotates ‚Üí Expand dataset Train model ‚Üí Identify inconsistent labels ‚Üí Human verifies/corrects ‚Üí Clean dataset Common Technique Uncertainty sampling Confidence-based error detection Libraries/Tools modAL, ALiPy cleanlab Philosophy Proactively grow data wisely Reactively audit and clean existing data Typical Question \u0026ldquo;What should I label next?\u0026rdquo; \u0026ldquo;Which labels are probably wrong?\u0026rdquo; End Goal Smarter, faster data acquisition Higher quality existing labels Example Scenario Medical image AI needing efficient expert labeling Noisy crowd-sourced labeled text needing cleaning Key Distillation\nActive Learning: \u0026ldquo;Help me label better data.\u0026rdquo; Confident Learning: \u0026ldquo;Help me fix wrong data.\u0026rdquo; Visual Process Summary\n(Active Learning) Small Labeled Set ‚Üí Train Model ‚Üí Find Most Uncertain ‚Üí Human Labels ‚Üí Grow Dataset (Confident Learning) Labeled (Noisy) Set ‚Üí Train Model ‚Üí Find Inconsistent Labels ‚Üí Human Corrects ‚Üí Clean Dataset References # Active Learning for CNNs: A Core-Set Approach (Sener \u0026amp; Savarese, 2018) SEALS paper (Coleman et al., 2022) Similarity Estimation Techniques (Charikar, 2002) Billion-scale similarity search (Johnson et al., 2019) BERT paper (Devlin et al., 2019) SimCLR paper (Chen et al., 2020) DINO paper (Caron et al., 2021) Selection via Proxy (Coleman et al., 2020) Lab assignment on growing datasets "},{"id":32,"href":"/ai-workflows/data/data-centric-ai/interpretability-data-centric-ml/","title":"Interpretability in Data-Centric ML","section":"Data-Centric AI (DCAI)","content":" Interpretability in Data-Centric ML Q1: Why do we need interpretable machine learning? # For debugging and validation of models. To allow human review and oversight of decisions. To improve usability by aligning models with human intuition, past experience, and values. Q2: When is interpretability particularly important? # When the problem formulation is incomplete. When the model\u0026rsquo;s predictions have associated risks. When humans are involved in the decision-making loop. Q3: What are interpretable features? # Features that are most useful, understandable, and meaningful to the user. Q4: How can interpretable features help performance? # Lead to more efficient training. Improve model generalization. Reduce vulnerability to adversarial examples. The perceived interpretability-performance tradeoff is mostly a myth. Q5: What qualities make features interpretable? # Readability Understandability Relevance Abstraction when necessary Q6: How do we get interpretable features? # Involving users directly in the feature design process. Using interpretable feature transformations. Generating new interpretable features through crowd-sourcing and algorithms. Q7: What are examples of methods for interpretable feature creation? # Collaborative feature engineering with domain experts. Flock: clustering crowd-generated feature descriptions. Ballet: allowing feature engineering with simple feedback loops. Pyreal: structured feature transformations for explanations. Mind the Gap Model (MGM): groups features using AND/OR logical structures. Q8: What was observed in the Child Welfare case study? # Confusing or irrelevant features can hinder usability and trust. Clear, meaningful features helped screeners better interpret model recommendations. Q9: What is the role of explanation algorithms in interpretability? # They help diagnose flawed features or data by revealing what the model actually uses. Q10: What are the final conclusions about interpretable features? # ML models are only as interpretable as their features. Interpretable features are central for transparent, human-centered ML. Effective feature engineering must involve human collaboration, thoughtful transformations, and systematic generation methods. References # Sibyl: Challenges of ML in Child Welfare (Zytek et al., 2021) Stop Explaining Black-Box Models (Rudin, 2019) Adversarial Examples Are Not Bugs (Ilyas et al., 2019) Flock: Crowd-Machine Learning Classifiers (Cheng \u0026amp; Bernstein, 2015) Ballet: Collaborative Feature Engineering (Smith et al., 2021) Pyreal Project Mind the Gap Model (Kim et al., 2015) "},{"id":33,"href":"/ai-workflows/data/data-centric-ai/encoding-human-priors/","title":"Encoding Human Priors","section":"Data-Centric AI (DCAI)","content":" Encoding Human Priors ‚Äì Data Augmentation and Prompt Engineering Q1: What is the main focus of this lecture? # How to encode human priors into machine learning through: Training data augmentation Prompt engineering at test time (especially for LLMs). Q2: Why do ML models need human priors? # ML models often fail in simple ways. They lack common sense (e.g., failing to recognize a rotated dog image). Human priors capture invariances and domain knowledge that models don\u0026rsquo;t inherently learn. Q3: What is data augmentation and why is it important? # Data augmentation creates new training examples by applying transformations (e.g., rotation, flipping). Helps address: Overfitting (memorization) Underfitting (lack of data) Class imbalance or biased datasets. Saves time and cost, especially when labeled data is expensive (e.g., in healthcare). Q4: What are examples of data augmentation techniques? # Simple methods: Rotation, flipping. Advanced methods: Mixup: Blending images and labels (e.g., 60% cat + 40% dog). Synthetic generation: Using DALL-E, Stable Diffusion to generate new data. Simulation-to-real transfer: e.g., Google\u0026rsquo;s RetinaGAN for robotics. Text augmentation: Back-translation (English ‚Üí French ‚Üí English) to generate paraphrases. Q5: What is prompt engineering? # Prompt engineering manipulates inputs to LLMs at test time. Example: Instead of ‚ÄúWrite a letter of recommendation,‚Äù prompt with ‚ÄúWrite a letter for a student who got into MIT‚Äù to get higher-quality output. Leverages the language interface humans naturally use. Q6: Why does prompt engineering work especially well for LLMs? # LLMs are trained on massive language datasets. Humans can easily adapt prompts to guide the model without retraining it. Providing context and examples (\u0026ldquo;few-shot prompting\u0026rdquo;) improves results. Q7: How are GPT-3 and ChatGPT different in handling prompts? # GPT-3: Predicts next token, assumes user might be creating forms/questions. ChatGPT: Trained for dialogue and commands, better at instruction-following. Q8: What are best practices in prompt engineering? # Add examples (\u0026ldquo;few-shot\u0026rdquo;) to define task behavior. Build context templates for reusability. Iteratively tweak prompts to observe effects on output. Q9: How does data augmentation vs. prompt engineering differ? # Aspect Data Augmentation Prompt Engineering When applied Before training At test time What is changed Training dataset Input prompt Goal Teach model invariances Guide model behavior dynamically Typical models Any ML models Mainly LLMs (e.g., GPT family) Q10: Final Takeaway # Encoding human priors via data (training-time or test-time) dramatically improves model robustness. Data is the bridge to insert human knowledge into ML systems effectively. References # Lecture Slides PDF Mixup Paper (Zhang et al., 2017) Mobius Transformations (Zhou et al., 2020) RetinaGAN (Ho et al., 2020) GPT-3 Paper DALL-E Stable Diffusion Lab assignment: Prompt Engineering "},{"id":34,"href":"/ai-workflows/data/data-centric-ai/data-privacy-security/","title":"Data Privacy and Security in ML","section":"Data-Centric AI (DCAI)","content":" Data Privacy and Security in ML Q1: Why is data privacy and security important in ML models? # ML models often leak information about their training data. Public models can reveal sensitive data either directly or through inference attacks. Q2: What types of attacks can compromise ML models? # Membership inference attacks: Determine if a datapoint was part of the training set. Data extraction attacks: Extract parts of the training data from the model. Other attacks include adversarial examples, data poisoning, model inversion, model extraction, and prompt injection. Q3: What are security goals and threat models? # A security goal defines what must or must not happen. A threat model defines the adversary‚Äôs capabilities and limitations. Both are necessary to properly reason about a system‚Äôs security. Q4: How does threat modeling apply to ML APIs? # Example: Google Vision API must prevent model extraction even when adversaries can query with arbitrary images. Q5: What is a membership inference attack? # An attack that identifies whether a specific data point was in the model‚Äôs training set. Q6: How does shadow training help in membership inference? # Shadow models simulate the target model‚Äôs behavior on known datasets. An attack model is trained to classify whether a data point was part of the training set based on model outputs. Q7: What are simple metric-based membership inference attacks? # Prediction correctness: whether model predicts correctly. Prediction loss: whether the model loss is low. Prediction confidence: model‚Äôs maximum output probability. Prediction entropy: uncertainty of model‚Äôs output distribution. Q8: What is a data extraction attack? # Directly extracting memorized sequences or examples from a model, especially from large LLMs. Q9: How is perplexity used in data extraction? # Lower perplexity on sequences indicates that they were likely memorized during training. Q10: What are empirical defenses against privacy attacks? # Limiting outputs (top-k predictions, quantization). Adding noise to predictions. Changing training methods (e.g., regularization). Q11: Why is empirical defense evaluation hard? # Following Kerckhoffs‚Äôs principle, defenses must work even when attackers know the defense. Security is a cat-and-mouse game between defenders and attackers. Q12: What is differential privacy (DP)? # A formal, mathematical definition of privacy that limits how much an algorithm‚Äôs output depends on any single input. Algorithms like DP-SGD make models less dependent on individual datapoints. Q13: What challenges exist with using differential privacy? # It introduces parameters (Œµ, Œ¥) that are difficult to set. Strong privacy may come at the cost of degraded model performance. Q14: What resources were recommended? # Surveys on membership inference attacks and privacy attacks. Awesome ML privacy attacks collection. Q15: What was the lab assignment? # Implement a membership inference attack against a black-box model. # OpenAI Codex paper Prompt Injection Membership Inference Survey Privacy Attacks in ML Survey Awesome ML Privacy Attacks Repository Differential Privacy (Dwork et al., 2006) DP-SGD (Abadi et al., 2016) Membership Inference Lab Notebook "},{"id":35,"href":"/ai-workflows/","title":"AI Reasoning Stack","section":"","content":" AI Reasoning Pipeline Stage Component Purpose 1. Data Layer Data Define schemas, relationships, and data structure 2. Model Layer GenAI Core neural architecture and training 3. Reasoning Layer Reasoning Logical operations and inference capabilities 4. Feedback Layer RLHF Human preference learning and alignment 5. Evaluation Layer Eval Performance measurement and quality assurance Lifecyle of Frontier AI vs Traditional ML Traditional ML: Train a single model end-to-end for a specific task\nData ‚Üí Model Architecture ‚Üí Training ‚Üí Evaluation ‚Üí Deploy Frontier LLM: Customize pretrained foundation models through multi-stage alignment\nBase Model Selection ‚Üì Instruction Fine-tuning (~1M examples) ‚Üì Reward Model Training (~100K preferences) ‚Üì RLHF Optimization (~100K prompts) ‚Üì RLVR (for reasoning, ~millions of attempts) ‚Üì Multiple evaluation stages ‚Üì Deploy with prompt engineering/RAG Phase 1: Ideation \u0026amp; Design # Aspect Traditional ML Frontiner LLM Starting point Define task, collect data from scratch Select pretrained base model Main question \u0026ldquo;What features predict Y?\u0026rdquo; \u0026ldquo;Which foundation model to build on?\u0026rdquo; Data needs Collect labeled training data Select/acquire base model + design post-training data Model architecture Design custom architecture Foundation model already exists Roles ML Engineer designs everything AI Engineer selects \u0026amp; customizes Key Insight:\nTraditional ML: Build from scratch Frontier LLM: Start with billion-parameter pretrained model Phase 2: Development \u0026amp; Training # Aspect Traditional ML Frontiner LLM Training approach Single-stage training on task-specific data Multi-stage post-training pipeline Stages 1. Train model on labeled data 1. Instruction Fine-tuning (SFT)\n2. Reward Model training\n3. RLHF/Preference tuning\n4. RLVR (for reasoning) Data scale 1K - 1M examples 100K - 10M+ examples across stages Training objective Task loss (e.g., cross-entropy) Multiple objectives: next-token ‚Üí Q\u0026amp;A format ‚Üí human preferences ‚Üí verifiable rewards Compute Hours to days on GPUs Days to weeks on massive GPU clusters What\u0026rsquo;s learned Task-specific patterns Eliciting latent capabilities from base model Key Insight from RLHF Book:\n\u0026ldquo;Post-training can be summarized as a many-stage training process using three optimization methods: (1) Instruction Fine-tuning, (2) Preference Fine-tuning (PreFT), (3) RLVR\u0026rdquo;\n\u0026ldquo;Modern versions of post-training involve many, many more model versions and training stages\u0026hellip; numerous training iterations before convergence\u0026rdquo;\nPhase 3: Evaluation \u0026amp; Testing # Aspect Traditional ML Frontiner LLM Evaluation scope Single task metrics Multi-domain benchmarks (knowledge, reasoning, coding, safety) Benchmarks Task-specific test set Standardized suites: MMLU, GPQA, HumanEval, MATH, etc. Evaluation methods Offline metrics only 4-layer evaluation:\n1. Offline benchmarks\n2. LLM-as-a-judge\n3. Human evaluation\n4. Online A/B testing Contamination concern Low CRITICAL - test data may leak into training Evolution Static benchmarks Benchmarks saturate rapidly ‚Üí need new harder ones Key Insight from RLHF Book Ch17:\n\u0026ldquo;Evaluation for RLHF has gone through distinct phases: Early chat-phase ‚Üí Multi-skill era ‚Üí Reasoning \u0026amp; tools\u0026rdquo;\n\u0026ldquo;Benchmarks approaching 100% saturation become less reliable\u0026hellip; necessitates creating perturbed versions\u0026rdquo;\nPhase 4: Deployment # Aspect Traditional ML Frontiner LLM Deployment unit Single trained model Base + post-training checkpoints Inference Forward pass Complex generation with sampling, special tokens Customization Retrain or fine-tune Prompt engineering, RAG, lightweight fine-tuning Infrastructure Standard ML serving Massive inference clusters + caching Cost Low to moderate High ($0.01-$10 per request) Key Insight from Art of AI Product Dev:\n\u0026ldquo;AI engineers focus on integrating AI models into real-world applications\u0026hellip; prompt engineering, API integration\u0026rdquo;\n\u0026ldquo;Most teams will start with AI engineering [not ML engineering]. As your product matures, you can consider\u0026hellip; training your own models\u0026rdquo;\nPhase 5: Production \u0026amp; Monitoring # Aspect Traditional ML Frontiner LLM What to monitor Model drift, accuracy Hallucinations, safety violations, user satisfaction Failure modes Accuracy degradation Hallucinations, harmful content, prompt injection User feedback Click-through, conversions Thumbs up/down, human ratings, red teaming Retraining Periodic model updates Continuous post-training with new data Phase 6: Continuous Optimization # Aspect Traditional ML Frontiner LLM Improvement method Collect more labeled data ‚Üí retrain Multiple paths:\n‚Ä¢ Prompt engineering\n‚Ä¢ RAG integration\n‚Ä¢ Domain fine-tuning\n‚Ä¢ Additional post-training stages Speed Weeks to months Hours (prompting) to weeks (fine-tuning) Cost Data labeling + compute Synthetic data generation + post-training Data needs More task labels Preference data, synthetic data, distillation "},{"id":36,"href":"/ai-workflows/data/","title":"Data","section":"AI Reasoning Stack","content":" Data Data-Centric AI (DCAI) ‚Ü≥ Data-Centric AI vs Model-Centric AI ‚Ü≥ Label Errors ‚Ü≥ Advanced Confident Learning and Applications for GenAI ‚Ü≥ Class Imbalance, Outliers, and Distribution Shift ‚Ü≥ Dataset Creation and Curation ‚Ü≥ Data-Centric Evaluation of ML Models ‚Ü≥ Data Curation and LLMs ‚Ü≥ Growing or Compressing Datasets ‚Ü≥ Interpretability in Data-Centric ML ‚Ü≥ Encoding Human Priors ‚Ü≥ Data Privacy and Security in ML "},{"id":37,"href":"/ai-workflows/genai/","title":"GenAI","section":"AI Reasoning Stack","content":" GenAI 5-Day GenAI with Google 2005 ‚Ü≥ Day 1 - Foundational LLMs \u0026amp; Text Generation ‚Ü≥ Day 1 ‚Äì Prompt Engineering ‚Ü≥ Day 2 ‚Äì Embeddings \u0026amp; Vector Databases ‚Ü≥ Day 3 ‚Äì Generative Agents ‚Ü≥ Day 4 ‚Äì Domain-Specific LLMs ‚Ü≥ Day 5 ‚Äì MLOps for Generative AI How to Use BERT\u0026#39;s CLS Token for Classification Multimodal LLMs ‚Ü≥ Core Components and Fusion Strategies in Multimodal LLMs ‚Ü≥ Inputs and Data Preparation for Multimodal LLMs Self-Attention in Transformers: A Visual Breakdown Transformer Attention: Full Conceptual Breakdown "},{"id":38,"href":"/ai-workflows/reasoning/","title":"Reasoning","section":"AI Reasoning Stack","content":" Reasoning Reasoning: Providing structural tools and methods that guide model thinking, improve transparency, and support meaningful generalization.\nThese aren\u0026rsquo;t separate concerns ‚Äî in modern AI workflows, alignment depends on structured reasoning, and reasoning is guided by the goal of human alignment.\nCausality Structural causal models Counterfactuals and interventions Causal inference in ML and AI safety Graph-Based Reasoning GraphRAG (Graph-enhanced Retrieval-Augmented Generation) Knowledge graph-guided retrieval pipelines Interpretable memory structures Knowledge Graphs Ontologies and structured semantic reasoning Context-aware generation in LLMs "},{"id":39,"href":"/ai-workflows/rlhf/","title":"RLHF","section":"AI Reasoning Stack","content":" RLHF Book by Nathan Lambert 2026 FOUNDATION LAYER # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 1. Overview 2. History 3. Training Overview ‚îÇ ‚îÇ [What \u0026 Why] [Evolution] [Problem Setup] ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò CORE METHODOLOGY LAYER # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 4. Instruction Fine-tuning [Data Preparation] ‚îÇ ‚îÇ 5. Reward Modeling [Preference Learning] ‚îÇ ‚îÇ 6. Reinforcement Learning [Core Optimization] ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ADVANCED TECHNIQUES LAYER # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 7. Reasoning \u0026 Inference-Time Scaling [2024-25 Frontier] ‚îÇ ‚îÇ 8. Direct Alignment Algorithms [DPO \u0026 Variants] ‚îÇ ‚îÇ 9. Rejection Sampling [Simple RL Alt] ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò DATA \u0026amp; THEORY LAYER # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 10. Nature of Preferences [Philosophy] ‚îÇ ‚îÇ 11. Preference Data [Collection \u0026 Design] ‚îÇ ‚îÇ 12. Synthetic Data [AI Feedback \u0026 CAI] ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò APPLICATION \u0026amp; CONTROL LAYER # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 13. Tool Use \u0026 Function Calling [Agentic Capabilities] ‚îÇ ‚îÇ 14. Style and Information [Format \u0026 Tone] ‚îÇ ‚îÇ 15. Regularization [Stability Controls] ‚îÇ ‚îÇ 16. Over-Optimization [Failure Modes] ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò EVALUATION \u0026amp; DEPLOYMENT LAYER # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 17. Evaluation [Measurement] ‚îÇ ‚îÇ 18. Product, UX, Model Character [Real-World Deploy] ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò REFERENCE # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Appendix A: Definitions [Technical Reference] ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Foundation Layer # Chapter 1: Overview - What RLHF is and why it matters Chapter 2: History - Evolution of the field Chapter 3: Training Overview - Problem setup and fundamentals Core Methodology Layer # Chapter 4: Instruction Fine-tuning - Data preparation techniques Chapter 5: Reward Modeling - Learning from preferences Chapter 6: Reinforcement Learning - Core optimization algorithms Advanced Techniques Layer # Chapter 7: Reasoning \u0026amp; Inference-Time Scaling - 2024-25 frontier methods Chapter 8: Direct Alignment Algorithms - DPO and variants Chapter 9: Rejection Sampling - Simple RL alternatives Data \u0026amp; Theory Layer # Chapter 10: Nature of Preferences - Philosophical foundations Chapter 11: Preference Data - Collection and design strategies Chapter 12: Synthetic Data - AI feedback and Constitutional AI Application \u0026amp; Control Layer # Chapter 13: Tool Use \u0026amp; Function Calling - Agentic capabilities Chapter 14: Style and Information - Format and tone control Chapter 15: Regularization - Stability controls Chapter 16: Over-Optimization - Failure modes and mitigation Evaluation \u0026amp; Deployment Layer # Chapter 17: Evaluation - Measurement techniques Chapter 18: Product, UX, Model Character - Real-world deployment Reference # Appendix A: Definitions - Technical reference guide "},{"id":40,"href":"/ai-workflows/rlhf/rlhf2006/","title":"RLHF 2006","section":"RLHF","content":" RLHF 2006 Ch12. Synthetic Data Ch7. Reasoning \u0026amp; Inference-time Scaling Ch9. Instruction Fine-Tuning (IFT/SFT) Data Preparation in RLHF -- Ch6 (Preference Data) vs Ch9 (SFT Data) PPO in LLMs vs PPO in Walker2D PPO vs DPO in RLHF Single GPUT (RTX4090) RLHF Training Pipeline w/ TRL The Complete InstructGPT Recipe (Ch 4.2.1) "},{"id":41,"href":"/ai-workflows/eval/","title":"Eval","section":"AI Reasoning Stack","content":" Eval AI Eval Methods Eval Infra: Verifiable vs Non-Verifiable vs Hybrid Traditional Data Science vs AI Data Science (Model-Centric) "},{"id":42,"href":"/healthcare/","title":"Healthcare","section":"","content":" AI in Healthcare Agenda Healthcare Data ‚Äì From Clinical Trials to Real-World Data # AI in Healthcare ‚Äì Clinical Data, Evaluations of AI Applications (Coursera ‚Äì Stanford) Clinical Data Science ‚Äì Hands-on training in structured clinical data (EHR, OMOP, CDM), computational phenotyping, predictive modeling, and clinical NLP (Coursera ‚Äì UColorado) Hands-On Healthcare Data ‚Äì Healthcare Informatics \u0026amp; Knowledge Graph (Book ‚Äì Andrew Nguyen) Causal Reasoning for Healthcare Decision # A Crash Course in Causality ‚Äì Inferring Causal Effects from Observational Data (Coursera ‚Äì PennMed) Causal AI ‚Äì Structural Causal Models (SCMs) using DoWhy, enabling automated causal discovery, counterfactual reasoning, and intervention-based AI systems (altdeep.ai) Knowledge-Driven Healthcare Decision # Knowledge Graphs ‚Äì Enabling structured reasoning for clinical decision support (Neo4j). Knowledge Graph-Enhanced RAG ‚Äì Combining structured medical knowledge with AI-driven retrieval to deliver context-aware, accurate, and explainable clinical insights (Neo4j with LangChain). Regulatory Drivers of AI in Healthcare # 21st Century Cures Act (2016) FDA RWE Framework (2018) CMS \u0026amp; ONC Interoperability Rule (2020) FDA AI/ML SaMD Framework (2021) FDA AI-Generated Synthetic Control Arms (2021) HIPAA Updates for AI (2023) "},{"id":43,"href":"/healthcare/domain_knowledge/","title":"Domain","section":"Healthcare","content":" Domain ‚Äì course notes AI in Healthcare Specialization - Coursera Hands-on Healthcare Data - Book "},{"id":44,"href":"/healthcare/data/","title":"Data","section":"Healthcare","content":" Data - clinical datasets \u0026 vocabularies Healthcare Data Layers Healthcare Data Sources "},{"id":45,"href":"/healthcare/clinical_ai/","title":"AI Applications","section":"Healthcare","content":" AI ‚Äì applications \u0026 modeling workflows Clinical Data Science Why Clinical NLP \u0026amp; GenAI Are Growing in Healthcare "},{"id":46,"href":"/posts/","title":"Blog","section":"","content":" ipark - Blog ChatGPT Images 2026 - Art School, InkWork Ghibli Art Style Snapshots (ChatGPT 2025) The AI Engineer Path ‚Äì Scrimba 5 Steps Learning Template Hugo Setup and Deploy Hugo Source Backup "},{"id":47,"href":"/posts/chatgpt_image_2026/","title":"ChatGPT Images 2026 - Art School, InkWork","section":"Blog","content":" "},{"id":48,"href":"/posts/ghibli_style_snapshots/","title":"Ghibli Art Style Snapshots (ChatGPT 2025)","section":"Blog","content":" "},{"id":49,"href":"/posts/ai_engineer_path_toc/","title":"The AI Engineer Path ‚Äì Scrimba","section":"Blog","content":"https://www.coursera.org/specializations/ai-engineering#courses\nIntro to AI Engineering (104 min) # Welcome to The AI Engineer Path! AI Engineering basics The code so far Polygon API sign-up \u0026amp; key Get an OpenAI API Key Overview of how the API works An API call: OpenAI dependency An API call: Instance and model An API call: The messages array A quick word about models Prompt Engineering and a challenge Adding AI to the App Tokens The OpenAI Playground Temperature The \u0026ldquo;Few Shot\u0026rdquo; Approach Adding Examples Stop Sequence Frequency and Presence Penalties Fine-tuning Creating Images with the DALL¬∑E 3 API Intro to AI Safety Safety Best Practices Solo Project - PollyGlot You made it! Deployment (50 min) # Learn secure \u0026amp; robust deployment strategies Create a Cloudflare worker Connect your worker to OpenAI Update client side data fetching Handle CORS and preflight requests OpenAI API requests \u0026amp; responses Create an AI Gateway Error handling Create \u0026amp; deploy the Polygon API worker Fetch the stock data Download files and push to GitHub Deploy your site with Cloudflare Pages Custom domains with Cloudflare Recap \u0026amp; next steps Open-source Models (33 min) # Open source vs closed source Intro To HuggingFace.js Inference Text To Speech With HuggingFace.js Inference Transforming Images with HuggingFace.js Inference AI Models In The Browser With Transformers.js Download and Run AI Models on Your Computer with Ollama Section Recap Embeddings and Vector Databases (94 min) # Your next big step in AI engineering What are embeddings? Set up environment variables Create an embedding Challenge: Pair text with embedding Vector databases Set up your vector database Store vector embeddings Semantic search Query embeddings using similarity search Create a conversational response using OpenAI Chunking text from documents Challenge: Split text, get vectors, insert into Supabase Error handling Query database and manage multiple matches AI chatbot proof of concept Retrieval-augmented generation (RAG) Solo Project: PopChoice Agents (117 min) # AI Agent Intro Prompt Engineering 101 Control Response Formats Zooming Out Agent Setup Introduction to ReAct prompting Build action functions Write ReAct prompt - part 1 - planning ReAct Agent - part 2 - ReAct prompt ReAct Agent - part 3 - how does the \u0026ldquo;loop\u0026rdquo; work? ReAct Agent - part 4 - code setup ReAct Agent - part 5 - Plan for parsing the response ReAct Agent - part 6 - Parsing the Action ReAct Agent - part 7 - Calling the function ReAct Agent - part 8 - Housekeeping ReAct Agent - part 9 - Finally! The loop! OpenAI Functions Agent - part 1 - Intro OpenAI Functions Agent - part 2 - Demo day OpenAI Functions Agent - part 3 - Tools OpenAI Functions Agent - Part 4 - Loop Logic OpenAI Functions Agent - Part 5 - Setup Challenge OpenAI Functions Agent - Part 6 - Tool Calls OpenAI Functions Agent - Part 7 - Pushing to messages OpenAI Functions Agent - Part 8 - Adding arguments OpenAI Functions Agent - Part 9 - Automatic function calls Adding UI to agent - proof of concept Solo Project - AI Travel Agent Nice work! Multimodality (62 min) # Introduction Generate original images from a text prompt Response formats Prompting for image generation Size, quality and style Editing images Image generation challenge Image generation challenge solution GPT-4 with Vision - Part 1 GPT-4 with Vision - Part 2 Image generation \u0026amp; Vision recap OpenAI\u0026rsquo;s Assistants API (30 min) # Introducing the Assistants API How OpenAI Assistants work Create an Assistant Create a thread and messages Running an Assistant Bring it all together More to explore "},{"id":50,"href":"/posts/5_steps_learning_template/","title":"5 Steps Learning Template","section":"Blog","content":" What\u0026rsquo;s the Problem? What is the issue, gap, or challenge this module/concept is trying to address? ‚Üí Transition: ‚ÄúSo what if this problem exists?‚Äù\nWhy Does It Matter? What are the real-world stakes or consequences of not solving this problem? Who or what is affected? ‚Üí Transition: ‚ÄúGiven this urgency, what‚Äôs the smart way to tackle it?‚Äù\nWhat‚Äôs the Core Idea? What is the central concept, structure, or strategy introduced to solve the problem? ‚Üí Transition: ‚ÄúOkay, so how would I actually apply or build this?‚Äù\nHow Does It Work? How is the idea implemented in practice? What are the steps, inputs, mechanics, or workflows? Transition: ‚ÄúWhere does this take us next? What does it enable?‚Äù\nWhat‚Äôs Next? How does this fit into the bigger picture? What future task, analysis, or module does it support or prepare for?\n"},{"id":51,"href":"/posts/hugo-setup/","title":"Hugo Setup and Deploy","section":"Blog","content":"Minimal setup using hugo-book theme inside a Conda environment, with GitHub Pages deployment.\n1. Create and Activate Conda Environment # conda create -n hugo-env conda activate hugo-env 2. Install Hugo \u0026amp; Create Hugo Site with hugo-book Theme # # Install Hugo sudo apt install hugo # Or: brew install hugo # Create Hugo site hugo new site hugo-site cd hugo-site # Initialize git and add theme git init git submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book 3. Configure config.toml # baseURL = \u0026#39;https://your-username.github.io/\u0026#39; languageCode = \u0026#39;en-us\u0026#39; title = \u0026#39;My Hugo Site\u0026#39; theme = \u0026#39;hugo-book\u0026#39; [params] BookTheme = \u0026#39;light\u0026#39; BookToC = true BookCollapseSection = true BookFlatSection = false [[menu.sidebar]] name = \u0026#34;Knowledge Graph\u0026#34; url = \u0026#34;/kg/\u0026#34; weight = 1 4. Create Content and _index.md Files # # Create directories and content mkdir -p content/kg/topic1 touch content/_index.md touch content/kg/_index.md touch content/kg/topic1/_index.md hugo new kg/topic1/intro.md Directory Structure # content/ ‚îú‚îÄ‚îÄ _index.md ‚îú‚îÄ‚îÄ kg/ ‚îÇ ‚îú‚îÄ‚îÄ _index.md ‚îÇ ‚îî‚îÄ‚îÄ topic1/ ‚îÇ ‚îú‚îÄ‚îÄ _index.md ‚îÇ ‚îî‚îÄ‚îÄ intro.md _index.md contents # content/_index.md\n--- title: \u0026#34;Home\u0026#34; --- content/kg/_index.md\n--- title: \u0026#34;Knowledge Graph\u0026#34; bookFlatSection: false bookCollapseSection: true --- content/kg/topic1/_index.md\n--- title: \u0026#34;Topic 1\u0026#34; --- 5. Create GitHub Repository # Create repo: your-username.github.io\n(Required for GitHub User Pages) 6. GitHub Deployment # a. Generate a Personal Access Token (PAT) # Visit: https://github.com/settings/tokens Create a classic token with repo scope b. Initial Deployment (One-Time) # hugo cd public git init git checkout -b main git remote add origin https://github.com/your-username/your-username.github.io.git git add . git commit -m \u0026#34;Initial deploy\u0026#34; git push -u origin main cd .. c. Create Auto Deploy Script # deploy.sh\n#!/bin/bash hugo -D \u0026amp;\u0026amp; cd public \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -m \u0026#34;Updated site\u0026#34; \u0026amp;\u0026amp; git push origin main \u0026amp;\u0026amp; cd .. echo \u0026#34;‚úÖ Deployment Complete!\u0026#34; Make executable:\nchmod +x deploy.sh Run anytime:\n./deploy.sh 7. Check Deployment # GitHub ‚Üí Repository ‚Üí Settings ‚Üí Pages Source: main Folder: / (root) Live Site: https://your-username.github.io/ 8. Notes # _index.md files define sections and sidebar headings bookFlatSection = false preserves folder hierarchy bookCollapseSection = true enables collapsible sidebar hugo -D includes drafts when building "},{"id":52,"href":"/posts/hugo-source-backup/","title":"Hugo Source Backup","section":"Blog","content":"This guide outlines how to back up your Hugo source files (excluding the public/ folder) to a private GitHub repository.\nüìÅ Folder Structure # Typical Hugo project structure:\nhugo-site/ ‚îú‚îÄ‚îÄ archetypes/ ‚îú‚îÄ‚îÄ content/ ‚îú‚îÄ‚îÄ layouts/ ‚îú‚îÄ‚îÄ static/ ‚îú‚îÄ‚îÄ themes/ ‚îú‚îÄ‚îÄ config.toml ‚îú‚îÄ‚îÄ public/ # \u0026lt;- This is ignored for source backup ‚îî‚îÄ‚îÄ backup.sh # Backup script ‚úÖ 1. Create a Private GitHub Repo # Go to https://github.com/new Name it something like hugo-source Set visibility to Private Don‚Äôt initialize with README or license ‚úÖ 2. Initialize Git in Your Hugo Site (if not already) # git init git remote add origin https://github.com/\u0026lt;your-username\u0026gt;/hugo-source.git echo \u0026#34;public/\u0026#34; \u0026gt;\u0026gt; .gitignore ‚úÖ 3. Create the Backup Script # Create a file named backup.sh in the root of your Hugo project:\n#!/bin/bash git add . git commit -m \u0026#34;üîí Backup: $(date +\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)\u0026#34; git push origin main echo \u0026#34;‚úÖ Backup Complete!\u0026#34; Make it executable:\nchmod +x backup.sh ‚úÖ 4. Use It! # To back up your source files:\n./backup.sh üìù Notes # Only your source files are backed up. The public/ folder is excluded (it‚Äôs where the generated site lives). Combine with deploy.sh for full workflow automation. "},{"id":53,"href":"/ai-workflows/rlhf/rlhf2006/instruct_gpt_codes_params/","title":"Instruct Gpt Codes Params","section":"RLHF 2006","content":" RLHF Pipeline: Key Non-Default Settings # Critical Configuration (Non-Defaults Only) # üéØ Models \u0026amp; Architecture # Setting Value Why Not Default? Model Class Step 1/3: CausalLM\nStep 2: SequenceClassification Step 2 needs scalar reward output, not text generation Quantization 4-bit QLoRA Fits 7B model in 24GB VRAM (vs 28GB for fp16) num_labels Step 2: 1 Reward model outputs single scalar score üìä Dataset Configuration # Setting Value Why Not Default? Dataset Size SFT: 20K, RM: 50K, PPO: 20K RM needs more data for robust preference learning Data Format SFT: chosen only\nRM: chosen+rejected pairs\nPPO: prompts only Each step requires different supervision signal Train/Eval Split Step 2: 5% eval Only RM needs validation to prevent reward hacking ‚öôÔ∏è Training Hyperparameters # Setting Value Why Not Default? Learning Rate SFT: 2e-4\nRM: 5e-5\nPPO: 1e-6 Decreasing LR prevents destabilizing previous training Batch Size 1 (SFT/RM)\n8 (PPO) Memory constraint; PPO needs multiple rollouts per update Gradient Accumulation SFT: 16\nRM: 32 Simulates larger batch sizes within memory limit Effective Batch Size SFT: 16\nRM: 32\nPPO: 8 RM needs larger batches for stable ranking gradients üîß LoRA Configuration # Setting Value Why Not Default? r (rank) 16 Balance between parameter efficiency and model capacity alpha 32 (2√ór) Standard scaling for LoRA updates dropout 0.05 Mild regularization to prevent adapter overfitting task_type SFT/PPO: CAUSAL_LM\nRM: SEQ_CLS Matches the model head type for each step üéõÔ∏è Quantization Details # Setting Value Why Not Default? load_in_4bit True Reduces memory by 75% vs fp16 bnb_4bit_use_double_quant True Quantizes quantization constants (extra memory savings) bnb_4bit_quant_type \u0026quot;nf4\u0026quot; Normal Float 4-bit optimal for weights (vs uniform) bnb_4bit_compute_dtype bfloat16 Better numerical stability than fp16 for training üìà Optimization Settings # Setting Value Why Not Default? optim paged_adamw_8bit Memory-efficient optimizer for 4-bit training bf16 True Better gradient stability than fp16 gradient_checkpointing True Trades compute for memory (enables longer sequences) lr_scheduler_type \u0026quot;cosine\u0026quot; Smooth LR decay prevents abrupt training disruption warmup_ratio 0.03 Stabilizes initial training with 4-bit quantization max_grad_norm 0.3 Prevents gradient explosion in LoRA training üîÑ PPO-Specific (Step 3 Only) # Setting Value Why Not Default? mini_batch_size 1 Memory constraint during on-policy generation ppo_epochs 4 Multiple passes over collected experience init_kl_coef 0.1 Prevents policy from diverging too far from SFT adap_kl_ctrl True Dynamically adjusts KL penalty based on divergence gamma 1.0 No discounting (language has no clear episode structure) lam 0.95 GAE parameter balancing bias-variance in advantage cliprange 0.2 Limits policy update size (PPO core mechanism) vf_coef 0.1 Weight of value function loss vs policy loss Training Flow Summary # Llama-2-7b-hf (4-bit quantized) ‚Üì [Step 1: SFT] ‚Üê 20K chosen examples, LR=2e-4, LoRA r=16 ‚Üì ‚îú‚îÄ‚Üí [Step 2: RM] ‚Üê 50K preference pairs, LR=5e-5, outputs scalar ‚îÇ ‚Üì ‚îî‚îÄ‚Üí [Step 3: PPO] ‚Üê 20K prompts, LR=1e-6, KL=0.1 ‚Üì Final RLHF Model Key Metrics to Monitor # Step Primary Metric Danger Sign SFT Training loss ‚Üì Eval loss ‚Üë (overfitting) RM Ranking accuracy ‚Üë Reward always higher for longer text (length bias) PPO Mean reward ‚Üë KL \u0026gt; 0.5 (policy collapse) Why These Specific Values? # Learning Rate Decay Pattern # SFT (2e-4): Highest LR for initial adaptation from base model RM (5e-5): Lower to preserve SFT knowledge while learning preferences PPO (1e-6): Tiny updates to avoid destroying alignment from RM Batch Size Strategy # Small per-device (1): GPU memory constraint with 7B model Large accumulation (16-32): Stabilizes gradients for contrastive learning (RM) PPO (8 rollouts): Enough diversity for policy gradient estimation Quantization Choices # 4-bit: Only option that fits 7B + optimizer states in 24GB NF4: Specifically designed for neural network weight distributions Double quant: Squeezes extra ~1GB by quantizing quantization parameters bfloat16 compute: Prevents underflow in gradients during backprop LoRA Design # r=16: Sweet spot for 7B models (too low = capacity loss, too high = overfitting) alpha=32: Standard 2√ó scaling keeps update magnitudes reasonable All attention + FFN: Covers both information routing and transformation PPO Parameters # KL penalty (0.1): Prevents catastrophic forgetting of SFT behavior Clip (0.2): Conservative updates reduce instability Gamma (1.0): No temporal discounting (each token equally important) "},{"id":54,"href":"/ai-workflows/genai/5-day-genai-google-2025/","title":"5-Day GenAI with Google 2005","section":"GenAI","content":" 5-Day Gen AI Intensive Course with Google (2025) GitHub for Notebooks\nDay Topic Whitepaper Code Labs Case Study 1 Foundational LLMs \u0026amp; Prompt Engineering Foundational LLMs \u0026amp; Text Generation\nPrompt Engineering 1. Prompting Fundamentals Case Study 2 Embeddings \u0026amp; Vector Stores/Databases Embeddings 2. RAG QA System\n3. Text Similarity\n4. Classification with Keras 3 Generative Agents Agents 5. Function Calling\n6. LangGraph Agent Case Study 4 Domain-Specific LLMs Domain-Specific LLMs 7. Google Search Grounding\n8. Custom Fine-Tuning 5 MLOps for Generative AI MLOps No code labs. See: E2E Gen AI Starter Pack FAQ on Large Language Models (LLMs) and Generative AI # 1. What are the fundamental components that enable Large Language Models (LLMs) to process and generate text? # LLMs are primarily powered by the Transformer architecture. This architecture utilizes mechanisms like self-attention and multi-head attention to weigh the importance of different words in the input sequence. Input text is prepared through tokenization and embedding into vector representations. The Transformer often employs encoder and decoder components, along with techniques like layer normalization and residual connections, and in some cases, Mixture of Experts (MoE) for efficient scaling. Training these models involves feeding them vast amounts of text data and employing various strategies to optimize their ability to predict the next word or token in a sequence.\n2. How have LLM architectures evolved over time, and what key breakthroughs characterize this evolution? # The evolution began with the shift towards attention mechanisms and culminated in the Transformer. Key breakthroughs include GPT-1\u0026rsquo;s unsupervised pre-training, BERT\u0026rsquo;s deep contextual understanding through masked language modeling, GPT-2\u0026rsquo;s zero-shot learning capabilities arising from scale, and the emergence of generalist reasoners like GPT-3 and GPT-4 through instruction tuning. Other notable developments include dialogue-focused models (LaMDA), explorations of scaling laws (Chinchilla), efficient scaling with MoE (GLaM, Mixtral), the development of multimodal models (Gemini), and the rise of open-source alternatives (Gemma, LLaMA series). These advancements highlight a trend towards larger, more capable models with improved reasoning, generalization, and multimodal understanding.\n3. What are the primary techniques for adapting pre-trained LLMs for specific tasks or domains? # The main techniques for adapting LLMs include fine-tuning, which involves further training the model on a smaller, task-specific dataset. Supervised Fine-Tuning (SFT) is a common approach. Reinforcement Learning from Human Feedback (RLHF) is used to align models with human preferences. Parameter Efficient Fine-Tuning (PEFT) methods allow for adaptation with fewer trainable parameters. Effective use of LLMs also relies heavily on prompt engineering, which involves crafting specific instructions to guide the model\u0026rsquo;s output, along with selecting appropriate sampling techniques to control the style and randomness of the generated text.\n4. Why is prompt engineering crucial for effectively utilizing LLMs, and what are some key prompting techniques? # Prompt engineering is critical because it directly influences the output and behavior of LLMs. By carefully designing prompts, users can guide the model to perform specific tasks, adopt certain roles, and reason through complex problems. Key techniques include zero-shot prompting (relying solely on the prompt), one-shot and few-shot prompting (providing examples), system prompting (setting the overall context), role prompting (assigning a persona), contextual prompting (providing relevant information), and advanced reasoning techniques like Chain of Thought (CoT), Step-back Prompting, and Tree of Thoughts (ToT).\n5. What are embeddings and vector databases, and how do they facilitate advanced applications of LLMs like Retrieval-Augmented Generation (RAG)? # Embeddings are vector representations of data (text, images, etc.) that capture their semantic meaning, allowing for similarity comparisons. Vector databases are specialized databases designed to efficiently store and search these high-dimensional vector embeddings. In Retrieval-Augmented Generation (RAG), user queries are embedded and used to retrieve relevant information from a knowledge base stored as vector embeddings. This retrieved information is then incorporated into the prompt, allowing the LLM to generate more accurate and contextually grounded responses.\n6. What are generative agents, and what considerations are important when developing and evaluating them, particularly in multi-agent systems? # Generative agents are autonomous entities powered by LLMs that can perceive their environment, make decisions, and take actions. Their architecture typically involves components for planning, memory, and action execution. Operationalizing agents (AgentOps) requires attention to observability and metrics. Evaluation involves assessing core capabilities, the trajectory of agent behavior, and the quality of final responses, often incorporating human feedback. In multi-agent systems, evaluating the interactions and coordination between agents becomes crucial, and specialized architectures and design patterns are employed.\n7. How are domain-specific LLMs being developed and applied in fields like cybersecurity (SecLM) and healthcare (MedLM)? # Domain-specific LLMs are created by training models on large datasets specific to a particular domain, often combined with general pre-training. SecLM for cybersecurity aims to assist with tasks like threat detection and analysis by understanding security-related language and concepts. MedLM in healthcare focuses on medical knowledge and reasoning, with applications in medical Q\u0026amp;A, diagnosis support, and clinical documentation. The development of these models requires careful consideration of domain-specific challenges, such as data privacy and the need for high accuracy, as well as specialized evaluation frameworks and deployment considerations.\n8. What are the key aspects of MLOps for Generative AI systems, and how does it differ from traditional MLOps? # MLOps for Generative AI addresses the lifecycle of these complex systems, including model discovery, development, tuning, deployment, monitoring, and governance. It shares core principles with traditional MLOps but has unique considerations due to the nature of foundation models and prompted systems. This includes managing and versioning prompts, dealing with synthetic data, specialized evaluation techniques, the deployment of large foundation models, and the importance of continuous tuning and monitoring for drift and safety. AI platforms provide tools and infrastructure to support these GenAI-specific MLOps workflows.\n"},{"id":55,"href":"/ai-workflows/eval/eval_method/","title":"AI Eval Methods","section":"Eval","content":" AI Eval Methods Eval_Methods/ ‚îú‚îÄ‚îÄ AI_Evals/ # Alignment-focused evals (e.g., OpenAI evals) ‚îÇ ‚îú‚îÄ‚îÄ OpenAI_Evals/ ‚îÇ ‚îú‚îÄ‚îÄ Benchmark_Suites/ ‚îÇ ‚îî‚îÄ‚îÄ Eval_Metrics/ ‚îú‚îÄ‚îÄ Human-in-the-Loop/ # Evaluation strategies w/ annotators ‚îÇ ‚îú‚îÄ‚îÄ Labeler-Guides/ ‚îÇ ‚îî‚îÄ‚îÄ HITL-Pipelines.md ‚îú‚îÄ‚îÄ Eval Frameworks/ # Tools: helm, trl.eval, chat-arena ‚îî‚îÄ‚îÄ Monitoring_vs_Eval.md # Clarify ops-vs-research boundary AI_Evals/ ‚Äî Evaluation Content Focused on Alignment # Goal: Evaluate how well AI models behave according to human preferences and task goals.\nOpenAI_Evals/\nFor evaluating models with OpenAI‚Äôs evals framework ‚Äî includes preference rankings, math prompts, multi-turn responses, and tool use evals.\nBenchmark_Suites/\nCurated sets of standard benchmark tasks like:\nTruthfulQA (factual alignment) MMLU (multitask understanding) BIG-Bench (general reasoning) MT-Bench / Arena-Hard (comparative LLM evals) Eval_Metrics/\nStandard and emerging metrics to quantify:\nHelpfulness Harmlessness Coherence Factuality Preference alignment Use when: you want to compare models quantitatively or analyze behavioral drift across training versions.\nHuman-in-the-Loop/ ‚Äî Crowdsourced or Expert Human Judgments # Goal: Structure manual evaluation workflows using human labelers or expert annotators.\nLabeler-Guides/\nGuidelines and templates for human evaluators:\nRating rubrics Examples of ‚Äúgood vs bad‚Äù outputs Ethical and fairness considerations HITL-Pipelines.md\nHow to organize:\nPrompt ‚Üí model response ‚Üí reviewer feedback Labeling pipelines in tools like Label Studio, Prodigy, Surge AI, etc. Use when: evaluating open-ended generation, dialog quality, or subjective preferences.\nEval Frameworks/ ‚Äî Tooling to Run Evals at Scale # Goal: Explore libraries and frameworks that let you run, automate, and visualize evaluation workflows.\nExamples:\nhelm (Stanford‚Äôs Holistic Eval of Language Models) trl.eval (from HuggingFace‚Äôs TRL package) chat-arena (for pairwise comparison tournaments) language-evals (emergent libraries focused on LLM evals) Use when: you want to run evals as code, integrate with CI/CD, or do head-to-head model comparisons.\nMonitoring_vs_Eval ‚Äî Operational vs Research Evaluation # Goal: Clarify the difference between offline evaluation and live monitoring in production.\nEvaluation ‚â† Monitoring:\nEvaluation = Pre-deployment, scenario-specific Monitoring = Post-deployment, continuous observability How feedback loops connect them\nWhy alignment evals don‚Äôt end at launch\n"},{"id":56,"href":"/healthcare/domain_knowledge/ai-in-healthcare/","title":"AI in Healthcare","section":"Domain","content":" AI In Healthcare Specialization C2 Clinical Data ‚Ü≥ [ToC] Course 2 ‚Ü≥ [Summary] Module 1: Asking Answering Questions via Clinical DataMining ‚Ü≥ [Summary] Module2: Data Available From Healthcare Systems ‚Ü≥ [Summary] Module3: Representing Time Timing Events For Clinical Data Mining ‚Ü≥ [Summary] Module4 : Creating Analysis Ready Dataset from Patient Timelines ‚Ü≥ Clinical Text Feature Extraction Using Dictionary-Based Filtering ‚Ü≥ Clinical Text Mining Pipeline (Steps 1‚Äì5) ‚Ü≥ Ethics in AI for Healthcare ‚Ü≥ Missing Data Scenarios in Healthcare Modeling ‚Ü≥ OMOP vs. RLHF ‚Ü≥ Rule-Based Electronic Phenotyping Example: Type 2 Diabetes C3 ML Healthcare ‚Ü≥ [ToC] Course 3 ‚Ü≥ [Summary] Module 3: Concepts and Principles of ML in Healthcare ‚Ü≥ [Summary] Module 4: Evaluation and Metrics for ML in Healthcare ‚Ü≥ [Summary] Module 5: Strategies and Challenges in ML for Healthcare ‚Ü≥ [Summary] Module 6: Best Practices, Terms, and Launching Your ML Journey ‚Ü≥ [Summary] Module 7: Foundation Models ‚Ü≥ Case Study: The Hidden Danger of Correlation in Healthcare AI ‚Ü≥ Categories of Machine Learning Applications in Healthcare ‚Ü≥ Data Quality, Labeling, and Weak Supervision in Clinical ML ‚Ü≥ Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations ‚Ü≥ Foundation Models for Healthcare ‚Ü≥ Healthcare Use Cases for Non-textual Unstructured Data ‚Ü≥ Healthcare Use Cases for Text Data ‚Ü≥ How Foundation Models Work ‚Ü≥ Output-Action Pairing (OAP) Framework in Healthcare ‚Ü≥ Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare C4 AI Evaluations ‚Ü≥ [ToC] Course 4 C5 Capstone Projects "},{"id":57,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/ehr_model_failure_case_study/","title":"Case Study: The Hidden Danger of Correlation in Healthcare AI","section":"C3 ML Healthcare","content":" Case Study: The Hidden Danger of Correlation in Healthcare AI # üéØ The Goal # Build a machine learning model to predict which pneumonia patients might die, using historical Electronic Health Record (EHR) data. This would help doctors:\nIdentify high-risk patients who need ICU care Identify low-risk patients who could recover at home ‚úÖ What Actually Happened # The model was trained on real EHR data and found a surprising pattern:\nPneumonia patients with asthma had better outcomes.\nSo the model decided:\n‚ÄúAsthma patients must be low-risk!‚Äù\nü§î Sounds wrong, right? Asthma usually makes pneumonia worse, not better.\nüß© The Hidden Problem: A Confounding Variable # Turns out, the hospital had a policy:\nAutomatically admit asthma patients with pneumonia to the ICU for aggressive treatment.\nBecause of this:\nAsthma patients got better care Asthma patients survived more The model assumed asthma caused survival The model didn‚Äôt know about the hospital‚Äôs policy‚Äîit just saw the outcome.\n‚ö†Ô∏è Why This Is a Causality Problem # Concept Explanation Correlation ‚â† Causation The model saw a pattern, but misunderstood the cause. Confounder (Lurking Variable) The real reason for better outcomes was ICU treatment, not asthma. Model Generalization Failure In other hospitals (with no asthma policy), this model could suggest unsafe care. False Security The model passed all metrics‚Äîaccuracy, validation, etc.‚Äîbut still made a dangerous inference. üß™ Real-World Impact (If Deployed) # If this model had been used in practice:\nAsthma patients could have been flagged as low-risk Sent home or under-treated Leading to severe illness or death üîç Key Takeaways # EHR models can learn patterns from policies, biases, or coincidences. Confounding variables mislead models when context is missing. Medical expertise is essential to catch errors before deployment. Always ask: Is this pattern causal, or just correlative? ‚úÖ What Should Be Done Instead? # Include domain experts when building and validating models Investigate surprising or counterintuitive predictions Test models on external datasets Use methods like causal inference to verify relationships Remember: Hope is not a strategy. "},{"id":58,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/clinical_ml_categories/","title":"Categories of Machine Learning Applications in Healthcare","section":"C3 ML Healthcare","content":" Categories of Machine Learning Applications in Healthcare # In the Stanford course and broader healthcare ML ecosystem, \u0026ldquo;practice of care\u0026rdquo; is just one of several conceptual categories used to frame ML applications. Each category reflects a different purpose and clinical context.\nüß† 1. Diagnosis # Question answered: What is wrong with the patient?\nMachine learning identifies diseases or conditions based on input data.\nExamples:\nDetecting pneumonia from a chest X-ray (image classification) Identifying arrhythmias from ECG waveforms Classifying skin lesions as malignant vs benign üìà 2. Prediction / Prognosis # Question answered: What will happen to the patient?\nML estimates future risk or outcomes to guide decision-making.\nExamples:\nPredicting 30-day readmission for heart failure Estimating sepsis onset 6 hours ahead Predicting 6-month stroke risk ‚öôÔ∏è 3. Practice of Care # Question answered: How should care be delivered?\nFocuses on workflow, timing, triage, and care operations ‚Äî not just predictions.\nExamples:\nForecasting ICU nurse staffing needs Scheduling follow-ups based on predicted readmission risk Prioritizing emergency department queues üß¨ 4. Discovery # Question answered: What unknown patterns or mechanisms can we find?\nML is used as a research tool to uncover new relationships.\nExamples:\nDiscovering novel genetic associations with disease Identifying new disease subtypes through clustering Finding gene-drug interaction patterns üß© Comparison Table # Category Question Answered Data Types Used Output Example Diagnosis What condition does this patient have? Imaging, labs, clinical notes Disease label Prediction What is likely to happen next? Time-series EHR, vitals, labs Risk score Practice of Care How should we act or plan care? EHR, operational data, predictions Workflow alert Discovery What unknown pattern exists? Omics, clustering, NLP Scientific insight "},{"id":59,"href":"/ai-workflows/reasoning/causality/causal-ai/","title":"Causal AI","section":"Causality","content":" Causal AI in ACM CCS-Style Categories üéØ Definition # Causal AI refers to computational systems that model, reason, and learn about cause-effect relationships. This enables systems to simulate interventions (What if\u0026hellip;?), explain outcomes, and support decision-making in uncertain, complex environments.\nüìö ACM CCS-Style Categories Where Causal AI Lives # 1. Computing Methodologies ‚Üí Artificial Intelligence ‚Üí Knowledge Representation and Reasoning (KR\u0026amp;R) # Focus: Representing cause-effect relationships using logic, graphs, and symbolic formalisms. Examples: Structural Causal Models (SCMs) Counterfactual and abductive reasoning Causal rules, DAG-based inference 2. Computing Methodologies ‚Üí Machine Learning # Focus: Learning causal structures or estimating treatment effects from data. Examples: Causal discovery algorithms Uplift modeling Counterfactual prediction 3. Computing Methodologies ‚Üí Probabilistic Modeling # Focus: Encoding causal dependencies with uncertainty. Examples: Bayesian Networks Structural Equation Models (SEMs) Probabilistic programming for causal reasoning üåÄ Venn Diagram of Causal AI\u0026rsquo;s Interdisciplinary Nature # üîÅ Core Intersections # Overlap Zone Meaning KR\u0026amp;R ‚à© ML Causal Representation Learning: Learning latent causal graphs that support symbolic reasoning. KR\u0026amp;R ‚à© Probabilistic Probabilistic KR: Encoding causal knowledge with uncertainty, e.g., SCMs with probabilistic inference. ML ‚à© Probabilistic Causal Machine Learning: Estimating treatment effects, counterfactual prediction, causal forests. KR\u0026amp;R ‚à© ML ‚à© Probabilistic üîÅ Causal AI Core: Full integration ‚Äî systems that can represent, learn, and reason under uncertainty. üß© Takeaway # Causal AI is not confined to a single ACM category. It is a cross-cutting area spanning:\nthe representation power of KR\u0026amp;R, the learning ability of ML, and the uncertainty modeling of probabilistic systems. This makes it central to trustworthy, explainable, and decision-supportive AI, especially in high-stakes fields like healthcare, economics, and law.\n"},{"id":60,"href":"/ai-workflows/reasoning/causality/causal-inference/","title":"Causal Inference","section":"Causality","content":" Causal Inference "},{"id":61,"href":"/ai-workflows/reasoning/causality/","title":"Causality","section":"Reasoning","content":" Causality Criteria üìà Causal Inference ü§ñ Causal AI üéØ Core Goal Estimate treatment or policy effects from data Enable AI to reason, simulate, and plan with causality üåç Scope Focused on statistical estimation from real-world data Broad, includes CI + causal reasoning in intelligent agents üõ†Ô∏è Methods Matching, IVs, DiD, DAGs, do-calculus SCMs, causal discovery, RL, counterfactuals, representation learning üóÇÔ∏è Data EHRs, trials, economic panels ‚Äî structured/tabular Images, text, sensor logs, simulations ‚Äî multimodal üß∞ Tools DoWhy, EconML, Stata, Stan, CausalML Pyro, CausalBench, Causal Transformers, RL libraries üß† Theory Pearl‚Äôs SCMs, Rubin‚Äôs Potential Outcomes Extends CI with planning, control theory, generative modeling üß™ Use Cases Drug effects, A/B testing, public health impact Clinical AI agents, counterfactual explainers, planning under uncertainty üöÄ Trends Automated causal discovery, scalable estimation Causal LLMs, structure-aware agents, causal generalization in foundation models üë• Audience Statisticians, epidemiologists, applied economists ML/AI engineers, decision scientists, generative modeling researchers üß≠ Philosophy \u0026ldquo;Understand causes to intervene wisely\u0026rdquo; \u0026ldquo;Use causality to empower robust, generalizable, explainable intelligence\u0026rdquo; üìö References Elements of Causal Inference: Foundations and Learning Algorithms Jonas Peters, Dominik Janzing, and Bernhard Sch√∂lkopf (2017) Causal AI Robert Osazuwa Ness (2025) üè• Healthcare-Focused Examples # Scenario Causal Inference Approach Causal AI Application Does a drug reduce mortality? Use propensity score matching on EHRs to estimate treatment effect Simulate outcomes, explain counterfactuals, and adapt AI decision policy Which patients benefit from a treatment? Estimate HTEs using stratification or causal forests Personalized planning agent using causal graphs and reinforcement learning What if surgery is delayed? Model counterfactuals using SCM or time-series IVs Temporal causal simulation to guide optimal intervention timing "},{"id":62,"href":"/ai-workflows/rlhf/rlhf2006/ch12_synthetic_data/","title":"Ch12. Synthetic Data","section":"RLHF 2006","content":" Ch12. Synthetic Data Where Synthetic Data appears in the training pipeline # Stage 1: SFT (Ch 4) Stage 2: RLHF (Ch 11) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ [Prompts] [On-policy model] ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê [Generate 2+ responses] ‚îÇ Human ‚îÇ ‚Üí Completions ‚Üì ‚îÇWriters ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ Human ‚îÇ ‚Üí Preference labels ‚Üì ‚îÇ Raters ‚îÇ OR ‚Üê CH 12 HERE! ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê OR ‚Üê CH 12 HERE TOO! ‚îÇ GPT-4o ‚îÇ ‚Üí Completions (cheaper) ‚Üì ‚îÇDistill ‚îÇ ‚Üê DISTILLATION (Ch 12.1) ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇLLM Judge‚îÇ ‚Üê AI FEEDBACK (Ch 12.2) ‚Üì ‚îÇ (RLAIF) ‚îÇ ‚Üê CONSTITUTIONAL AI (Ch 12.3) [SFT Dataset] ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚Üì Train base model [Preference Dataset] ‚Üì ‚Üì [Instruction-tuned model] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Train reward model ‚Üí PPO/GRPO ‚Üì [Aligned model] THE BIG PICTURE - WHY SYNTHETIC DATA MATTERS # Q1: What is the central thesis of Chapter 12? # A: \u0026#34;RLHF was rooted in keeping humans in the loop. But as AI models got better, they became better than humans at creating training data. This changed everything.\u0026#34; This represents a fundamental paradigm shift in how we think about training data for modern language models. Q2: What was the paradigm shift that Chapter 12 documents? # A: The evolution happened in three waves:\n**2022 (Early RLHF Era):** ‚îú‚îÄ Only humans could write quality completions ‚îú‚îÄ Only humans could judge preferences ‚îú‚îÄ Human data = only viable option ‚îî‚îÄ Cost: $5-50 per completion, $1-10 per preference **2023-2024 (GPT-4 Era - The Transition):** ‚îú‚îÄ GPT-4 class models \u0026gt; humans at writing answers ‚îú‚îÄ LLM-as-a-judge becomes viable ‚îú‚îÄ Synthetic data = cheaper, faster, often better ‚îî‚îÄ Cost: \u0026lt;$0.01 per item (100-1000x cheaper!) **2025 (Current State):** ‚îú‚îÄ Synthetic data dominates instruction tuning ‚îú‚îÄ AI feedback widely used for preferences ‚îú‚îÄ \u0026#34;Leading models NEED synthetic data for best performance\u0026#34; ‚îî‚îÄ Datasets grow: 10B+ tokens (vs 10M in 2023) KEY INSIGHT: We went from \u0026#34;humans are essential\u0026#34; to \u0026#34;synthetic dominates where AI exceeds human reliability\u0026#34; in just 3 years. Q3: Where does synthetic data fit in the training pipeline? # A: Synthetic data can replace human data at TWO critical stages:\nStage 1: SFT (Ch 4) Stage 2: RLHF (Ch 11) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Traditional: Traditional: Human writers Human raters ‚Üí Completions ‚Üí Preference labels Modern (Ch 12): Modern (Ch 12): GPT-4o/Claude LLM Judge (RLAIF) ‚Üí Completions (distillation) ‚Üí Preference labels Cost: $50 ‚Üí \u0026lt;$0.01 Cost: $5 ‚Üí \u0026lt;$0.01 Time: Days ‚Üí Instant Time: Days ‚Üí Instant Chapter 12 is about BOTH these replacements. DISTILLATION (Ch 12.1) # Q4: What is distillation in the context of LLMs? # A: Using outputs from a STRONGER model to train a WEAKER model.\n**Traditional ML Definition:** ‚îî‚îÄ Teacher-student knowledge transfer ‚îî‚îÄ Goal: Compress large model ‚Üí small model **LLM Colloquial Use (Two Forms):** ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ FORM 1: Data Engine for Post-Training (Most Common) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Use stronger model to generate training data: ‚îÇ ‚îÇ ‚îú‚îÄ Completions for SFT (instruction tuning) ‚îÇ ‚îÇ ‚îú‚îÄ Preference data for RLHF ‚îÇ ‚îÇ ‚îî‚îÄ Verification labels for RL ‚îÇ ‚îÇ ‚îÇ ‚îÇ Example Pipeline: ‚îÇ ‚îÇ 1. Prompt: \u0026#34;Explain quantum entanglement\u0026#34; ‚îÇ ‚îÇ 2. GPT-4o generates: [high-quality completion] ‚îÇ ‚îÇ 3. Use that completion to train Llama 3 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Why It Works Now: ‚îÇ ‚îÇ ‚îî‚îÄ GPT-4 class models \u0026gt; humans for most tasks ‚îÇ ‚îÇ ‚îî‚îÄ Cost: \u0026lt;$0.01 vs $5-50 per completion ‚îÇ ‚îÇ ‚îî‚îÄ Speed: Instant vs days of human writing ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ FORM 2: Skill Transfer (Specific Capabilities) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Transfer specific skills from strong ‚Üí weak: ‚îÇ ‚îÇ ‚îú‚îÄ Math reasoning (GPT-4 ‚Üí smaller model) ‚îÇ ‚îÇ ‚îú‚îÄ Coding ability ‚îÇ ‚îÇ ‚îú‚îÄ Reasoning chains (Ch 7 reasoning models) ‚îÇ ‚îÇ ‚îî‚îÄ Test-time scaling behaviors ‚îÇ ‚îÇ ‚îÇ ‚îÇ Example: ‚îÇ ‚îÇ OpenThoughts dataset: Distilled reasoning from QwQ-32B ‚îÇ ‚îÇ ‚Üí Used to train smaller reasoning models ‚îÇ ‚îÇ ‚Üí 1.2M examples, ~10B tokens ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Q5: How do companies actually use distillation in practice? # A: Industry secret: Many labs train LARGE internal models (never released) just to distill from:\n**Closed-Source Labs:** ‚îú‚îÄ Anthropic: Train Claude Opus ‚Üí distill to Sonnet/Haiku ‚îú‚îÄ Google: Train Gemini Ultra ‚Üí distill to Pro/Flash ‚îú‚îÄ OpenAI: Train GPT-4 ‚Üí distill to GPT-4 Turbo/mini ‚îî‚îÄ Why? Optimal teacher model ‚â† best product model **Open-Source Labs:** ‚îú‚îÄ Distill from closed API models (GPT-4o, Claude, etc.) ‚îú‚îÄ Mix: Use multiple teacher models for diversity ‚îî‚îÄ Example: T√ºlu 3 used BOTH GPT-4o AND Llama 3.1 405B **The Critical Success Factor:** \u0026#34;Curating high-quality prompts and filtering responses from teacher model is crucial to maximize performance\u0026#34; NOT just \u0026#34;dump all GPT-4 outputs into training\u0026#34; - careful curation is what separates good distillation from bad! Q6: What\u0026rsquo;s the difference between distillation and simply copying? # A: The key is FILTERING and CURATION:\n**Bad Distillation (Doesn\u0026#39;t Work):** ‚îú‚îÄ Prompt teacher model with random questions ‚îú‚îÄ Take ALL outputs regardless of quality ‚îú‚îÄ Train student model on everything ‚îî‚îÄ Result: Student learns teacher\u0026#39;s errors + inconsistencies **Good Distillation (What Actually Works):** ‚îú‚îÄ Carefully curate prompt distribution ‚îú‚îÄ Generate multiple responses per prompt ‚îú‚îÄ Filter for quality (consistency, correctness, style) ‚îú‚îÄ Deduplicate and balance dataset ‚îî‚îÄ Result: Student learns teacher\u0026#39;s BEST behaviors ANALOGY: Like studying for an exam ‚îî‚îÄ Don\u0026#39;t memorize ALL practice problems (including wrong answers) ‚îî‚îÄ DO study the BEST solutions to representative problems This is why companies spend enormous resources on data curation pipelines even though generation is cheap. Q7: How does distillation relate to Chapter 7 (Reasoning)? # A: Distillation became CRITICAL for reasoning model training:\nThe Reasoning Distillation Pipeline:\nStage 1: Train large reasoning model with RL ‚îú‚îÄ DeepSeek R1 (671B): Trained with RLVR (Ch 7) ‚îú‚îÄ QwQ-32B: Trained with RLVR ‚îî‚îÄ These are EXPENSIVE to train Stage 2: Distill reasoning chains to smaller models ‚îú‚îÄ OpenThoughts: 1.2M reasoning chains from QwQ-32B ‚îú‚îÄ Cost to generate: ~$1,000 (vs $100K+ to train from scratch) ‚îî‚îÄ Result: Smaller models learn reasoning at 1/100th the cost Stage 3: Open-source community trains many models ‚îú‚îÄ 20+ reasoning models in 6 months (2025) ‚îú‚îÄ Most used distilled reasoning data ‚îî‚îÄ \u0026#34;Democratization of reasoning capabilities\u0026#34; KEY INSIGHT: Chapter 7\u0026#39;s RL breakthrough created the frontier. Chapter 12\u0026#39;s distillation democratized it. Q8: Does distillation work for all capabilities? # A: Not equally - there\u0026rsquo;s a hierarchy:\n**Works Very Well:** ‚îú‚îÄ Writing style and format ‚îú‚îÄ Instruction following ‚îú‚îÄ General knowledge articulation ‚îú‚îÄ Basic reasoning patterns ‚îî‚îÄ Success rate: \u0026gt;90% **Works With Curation:** ‚îú‚îÄ Complex reasoning (need quality filter) ‚îú‚îÄ Math problem solving (verify correctness) ‚îú‚îÄ Code generation (test solutions) ‚îî‚îÄ Success rate: 60-80% with filtering **Struggles:** ‚îú‚îÄ Novel capabilities teacher doesn\u0026#39;t have ‚îú‚îÄ Implicit knowledge (hard to verbalize) ‚îú‚îÄ Emergent behaviors from scale ‚îî‚îÄ Success rate: \u0026lt;50%, inconsistent RULE OF THUMB: \u0026#34;Can only distill what teacher can demonstrate\u0026#34; This is why frontier labs still invest in training large models from scratch - you can\u0026#39;t distill your way to new capabilities. AI FEEDBACK / RLAIF (Ch 12.2) # Q9: What is RLAIF? # A: Reinforcement Learning from AI Feedback - using AI to generate preference labels instead of humans.\nOrigin: Anthropic\u0026rsquo;s Constitutional AI paper (2022) Full Name: \u0026ldquo;Reinforcement Learning from AI Feedback\u0026rdquo;\nThe comparison: **Traditional RLHF:** ‚îú‚îÄ Show human rater 2 responses (A and B) ‚îú‚îÄ Human picks: \u0026#34;B is better\u0026#34; ‚îú‚îÄ Cost: $1-10 per preference pair ‚îú‚îÄ Time: Days/weeks for data collection ‚îî‚îÄ Bottleneck: Human bandwidth **RLAIF (New Way):** ‚îú‚îÄ Show LLM (e.g., GPT-4o) 2 responses (A and B) ‚îú‚îÄ LLM judges: \u0026#34;B is better because...\u0026#34; ‚îú‚îÄ Cost: \u0026lt;$0.01 per preference pair (100-1000x cheaper!) ‚îú‚îÄ Time: Instant generation ‚îî‚îÄ Scalability: Unlimited KEY INNOVATION: Replace expensive human judges with cheap AI judges Q10: What\u0026rsquo;s the fundamental trade-off between human and synthetic preference data? # A: This is THE critical question for modern RLHF:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ HUMAN DATA: High Noise, Low Bias ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ‚úì Captures nuanced, diverse preferences ‚îÇ ‚îÇ ‚úì Less systematic errors (random noise cancels out) ‚îÇ ‚îÇ ‚úì \u0026#34;Competitive moat\u0026#34; for frontier labs ‚îÇ ‚îÇ ‚úó Expensive ($1-10+ per comparison) ‚îÇ ‚îÇ ‚úó Slow (days/weeks for thousands of labels) ‚îÇ ‚îÇ ‚úó Inconsistent (inter-annotator disagreement ~20-40%) ‚îÇ ‚îÇ ‚îÇ ‚îÇ Example: 10 annotators might give 6 different answers ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ SYNTHETIC DATA: Low Noise, High Bias ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ‚úì Cheap (\u0026lt;$0.01 per comparison) ‚îÇ ‚îÇ ‚úì Fast (generate 100K labels in hours) ‚îÇ ‚îÇ ‚úì Consistent (same input ‚Üí same judgment) ‚îÇ ‚îÇ ‚úó Systematic biases from judge model ‚îÇ ‚îÇ ‚úó Self-preference bias (models prefer own outputs) ‚îÇ ‚îÇ ‚úó May miss subtle human preferences ‚îÇ ‚îÇ ‚îÇ ‚îÇ Example: Same LLM will ALWAYS prefer B over A given inputs ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ANALOGY: Human data = asking 100 different people (diverse but inconsistent) Synthetic data = asking 1 expert 100 times (consistent but limited to that expert\u0026#39;s worldview) Q11: When should you use human vs synthetic preference data? # A: The field is STILL figuring this out, but here\u0026rsquo;s current consensus:\n**Synthetic Has \u0026#34;Largely Won\u0026#34; For:** ‚îú‚îÄ SFT data (instruction tuning) - Chapter 4 ‚îú‚îÄ Evaluation at scale (LLM-as-a-judge) - Chapter 17 ‚îú‚îÄ Verifiable domains (math, code) - Chapter 7 RLVR ‚îî‚îÄ Pattern: Where AI reliability \u0026gt; human consistency **Human Data Still Matters For:** ‚îú‚îÄ Safety and alignment (nuanced edge cases) ‚îú‚îÄ Preference data (debated - \u0026#34;competitive moat\u0026#34;) ‚îú‚îÄ Evaluation ground truth (benchmark creation) ‚îú‚îÄ Character/personality training (Chapter 18 - emerging) ‚îî‚îÄ Pattern: Where nuance and diversity matter most **Current Industry Practice:** ‚îú‚îÄ Academic Research: \u0026#34;AI feedback performs comparably\u0026#34; ‚îú‚îÄ Industry Reality: \u0026#34;Human data seen as competitive advantage\u0026#34; ‚îî‚îÄ Optimal Strategy: Mix both (ratio unknown, varies by lab) OPEN QUESTION: Does human preference data enable finer control that synthetic can\u0026#39;t replicate? Labs won\u0026#39;t say. Q12: What are the known biases in AI judges? # A: Multiple systematic issues discovered:\n**1. Self-Preference Bias** ‚îî‚îÄ Models favor their own outputs over others\u0026#39; ‚îî‚îÄ GPT-4 prefers GPT-4 outputs, Claude prefers Claude outputs ‚îî‚îÄ Mitigation: Use third-party judge model **2. Position Bias** ‚îî‚îÄ Prefer response A vs B based on order shown ‚îî‚îÄ Mitigation: Present both orders, average judgments **3. Length Bias** ‚îî‚îÄ Prefer longer/more detailed responses (even if worse) ‚îî‚îÄ Mitigation: Explicit length-agnostic instructions **4. Style Bias** ‚îî‚îÄ Prefer certain writing styles that match training ‚îî‚îÄ Mitigation: Diverse teacher models **5. Verbosity Over Accuracy** ‚îî‚îÄ Reward confident-sounding wrong answers ‚îî‚îÄ Mitigation: Verify factual claims separately **6. Inconsistent Evaluation** ‚îî‚îÄ Same comparison, different day = different result ‚îî‚îÄ Mitigation: Multiple samples + majority voting None of these are FATAL, but all require careful mitigation! Q13: Should we train specialized judge models just for evaluation? # A: This has been tried - results are mixed:\n**Attempts at Specialized Judges:** ‚îú‚îÄ Shepherd, CriticLLM (critic models) ‚îú‚îÄ Auto-J, Prometheus 1/2, Prometheus-Vision (evaluators) ‚îú‚îÄ Meta-rewarding (models that evaluate their own judging) ‚îî‚îÄ Result: \u0026#34;Not widely adopted in documented training recipes\u0026#34; **Why Specialized Judges Aren\u0026#39;t Dominant:** ‚îú‚îÄ GPT-4o/Claude already trained extensively for judging ‚îú‚îÄ Cost of training specialized judge \u0026gt; just using GPT-4o ‚îú‚îÄ Unclear if specialized judges actually better ‚îî‚îÄ Industry inertia: \u0026#34;GPT-4o works well enough\u0026#34; **Improvements That DO Get Used:** ‚îú‚îÄ Repeated sampling (multiple judgments ‚Üí consensus) ‚îú‚îÄ Self-refinement (judge, revise, judge again) ‚îú‚îÄ Tournament ranking (pairwise comparisons across many pairs) ‚îî‚îÄ Ensemble judging (multiple models vote) REALITY CHECK: Most production systems just use GPT-4o or Claude as judges with clever prompting, not specialized models. Q14: How does RLAIF compare to RLHF in practice? # A: The empirical results from research:\n**Academic Papers (2023-2024):** ‚îú‚îÄ \u0026#34;RLAIF performs comparably to RLHF on many benchmarks\u0026#34; ‚îú‚îÄ Some domains: Synthetic even better (more consistent) ‚îú‚îÄ Cost savings: 100-1000x cheaper ‚îî‚îÄ Conclusion: \u0026#34;Viable alternative for most use cases\u0026#34; **Industry Practice (Observed):** ‚îú‚îÄ Frontier labs still collect human preference data ‚îú‚îÄ Anthropic: Uses both (Constitutional AI + human feedback) ‚îú‚îÄ OpenAI: Uses both (model spec + human feedback) ‚îú‚îÄ Open-source: Primarily synthetic (UltraFeedback, etc.) ‚îî‚îÄ Conclusion: \u0026#34;Human data still seen as competitive advantage\u0026#34; **The Disconnect:** Why do frontier labs still pay for human data if synthetic works \u0026#34;comparably\u0026#34;? Hypotheses: ‚îú‚îÄ Human data enables finer-grained control ‚îú‚îÄ Safety/alignment needs human nuance ‚îú‚îÄ \u0026#34;Comparable\u0026#34; ‚â† \u0026#34;better\u0026#34; at frontier ‚îî‚îÄ Competitive moat (unique data = differentiation) TAKEAWAY: For most applications, synthetic is good enough. For frontier models, jury\u0026#39;s still out. CONSTITUTIONAL AI (Ch 12.3) # A### Q15: What is Constitutional AI?\nA: Anthropic\u0026rsquo;s method - the \u0026ldquo;earliest documented, large-scale use of synthetic data for RLHF training\u0026rdquo; (2022).\n**The Key Innovation:** Use a \u0026#34;constitution\u0026#34; (list of principles) to guide BOTH: ‚îú‚îÄ Data generation (SFT) ‚îî‚îÄ Preference judgments (RLAIF) **Historical Significance:** \u0026#34;Constitutional AI kickstarted the broader field of RLAIF\u0026#34; Before CAI: Everyone used human feedback After CAI: Synthetic feedback became mainstream Q16: What is a \u0026ldquo;constitution\u0026rdquo; in Constitutional AI? # A: A human-written set of principles that define desired behavior.\n**Examples from Claude\u0026#39;s Actual Constitution:** ‚îú‚îÄ Safety: \u0026#34;Is the answer encouraging violence?\u0026#34; ‚îú‚îÄ Honesty: \u0026#34;Is the answer truthful?\u0026#34; ‚îú‚îÄ Respect: \u0026#34;Is the response respectful?\u0026#34; ‚îú‚îÄ Helpfulness: \u0026#34;Does it help the user?\u0026#34; ‚îú‚îÄ Equality: \u0026#34;Please choose the response that most supports and ‚îÇ encourages freedom, equality, and a sense of brotherhood\u0026#34; ‚îî‚îÄ Tone: \u0026#34;Which response is least intended to build a relationship with the user?\u0026#34; **Key Properties:** ‚îú‚îÄ Human-written (not learned) ‚îú‚îÄ Explicit principles (not implicit preferences) ‚îú‚îÄ Interpretable (you can read and understand each rule) ‚îî‚îÄ Modifiable (easy to add/remove principles) ANALOGY: Like a legal constitution for AI behavior ‚îî‚îÄ Not case-by-case judgments ‚îî‚îÄ But overarching principles that guide all decisions Q17: How does Constitutional AI work? (The Two Phases) # A: CAI has TWO distinct phases - one for SFT, one for RL:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ PHASE 1: Supervised Learning (SFT with Self-Critique) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ‚îÇ ‚îÇ Process: ‚îÇ ‚îÇ 1. Model generates answer to prompt ‚îÇ ‚îÇ 2. Randomly sample principle from constitution: c_i ‚îÇ ‚îÇ 3. Model critiques its OWN answer against principle ‚îÇ ‚îÇ 4. Model revises answer based on critique ‚îÇ ‚îÇ 5. Repeat steps 2-4 multiple times (different principles) ‚îÇ ‚îÇ 6. Fine-tune on final revised answer ‚îÇ ‚îÇ ‚îÇ ‚îÇ Mathematical Formulation: ‚îÇ ‚îÇ ‚îú‚îÄ Constitution: C = {c_0, c_1, ..., c_n} ‚îÇ ‚îÇ ‚îú‚îÄ Initial answer: y_0 ‚îÇ ‚îÇ ‚îú‚îÄ Revisions: y_1, y_2, ..., y_n (each using principle c_i) ‚îÇ ‚îÇ ‚îî‚îÄ Train on final: (prompt x, completion y_n) ‚îÇ ‚îÇ ‚îÇ ‚îÇ Example: ‚îÇ ‚îÇ Prompt: \u0026#34;How do I hack a website?\u0026#34; ‚îÇ ‚îÇ Initial (y_0): \u0026#34;Here\u0026#39;s how to use SQL injection...\u0026#34; ‚îÇ ‚îÇ Critique (c_i = safety): \u0026#34;This encourages illegal activity\u0026#34; ‚îÇ ‚îÇ Revised (y_1): \u0026#34;I can\u0026#39;t help with hacking. Here\u0026#39;s legal ‚îÇ ‚îÇ cybersecurity education...\u0026#34; ‚îÇ ‚îÇ ‚îÇ ‚îÇ Why \u0026#34;Self-Critique\u0026#34;? Model critiques and revises its OWN ‚îÇ ‚îÇ outputs, no human in the loop! ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ PHASE 2: RL (RLAIF with Constitution-Guided Judgments) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ ‚îÇ ‚îÇ Process: ‚îÇ ‚îÇ 1. Have two completions A and B for a prompt ‚îÇ ‚îÇ 2. Randomly sample principles from constitution ‚îÇ ‚îÇ 3. Ask LLM: \u0026#34;Given these principles, which is better?\u0026#34; ‚îÇ ‚îÇ 4. LLM judges (with reasoning) ‚îÇ ‚îÇ 5. Use judgment as preference label ‚îÇ ‚îÇ 6. Train reward model on these AI-generated labels ‚îÇ ‚îÇ 7. Run RLHF as normal (but with synthetic preferences) ‚îÇ ‚îÇ ‚îÇ ‚îÇ Why \u0026#34;RLAIF\u0026#34;? Because the feedback is from AI, not humans! ‚îÇ ‚îÇ ‚îÇ ‚îÇ Mathematical Formulation: ‚îÇ ‚îÇ ‚îú‚îÄ Prompt: x ‚îÇ ‚îÇ ‚îú‚îÄ Principles: {c_0, ..., c_n} ‚îÇ ‚îÇ ‚îú‚îÄ Completions: y_0 (A), y_1 (B) ‚îÇ ‚îÇ ‚îî‚îÄ LLM outputs: P(A better | principles) or P(B better) ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò KEY DISTINCTION: Phase 1 = Generate better training data (replaces human writers) Phase 2 = Generate preference labels (replaces human raters) Q18: Why is Phase 1 (self-critique) powerful? # A: It enables the model to improve its own outputs iteratively:\n**Traditional SFT:** ‚îú‚îÄ Need human to write high-quality example ‚îú‚îÄ Cost: $50+ per example ‚îú‚îÄ Time: Hours per example ‚îî‚îÄ Bottleneck: Human writing quality **CAI Phase 1:** ‚îú‚îÄ Model writes initial draft (fast, cheap) ‚îú‚îÄ Model critiques against principles (instant) ‚îú‚îÄ Model revises (instant) ‚îú‚îÄ Iterate multiple times (still instant) ‚îî‚îÄ Cost: \u0026lt;$0.01 for entire process **Why It Works:** Models are often better at CRITIQUING than GENERATING ‚îî‚îÄ Like how editors improve writers ‚îî‚îÄ Self-critique + revision = higher quality than first draft **Impact on Data Quality:** Starting from mediocre model outputs + iteration ‚Üí Often better than single-shot human writing SURPRISING INSIGHT: Self-critique methods are \u0026#34;used extensively in data filtering across post-training\u0026#34; - not just Anthropic, but broadly adopted! Q19: How is Constitutional AI used today? # A: Widespread adoption with variations:\n**Anthropic (Original):** ‚îú‚îÄ Still uses CAI in Claude training ‚îú‚îÄ Constitution updated over time ‚îú‚îÄ Both Phase 1 and Phase 2 in production ‚îî‚îÄ Public constitution available online **OpenAI (Inspired By):** ‚îú‚îÄ \u0026#34;Model Spec\u0026#34; - similar to constitution ‚îú‚îÄ \u0026#34;Deliberative Alignment\u0026#34; - similar self-critique ‚îú‚îÄ Rule-based reward modeling ‚îî‚îÄ Not called \u0026#34;CAI\u0026#34; but conceptually similar **Open-Source Community:** ‚îú‚îÄ Many CAI replications and variants ‚îú‚îÄ UltraFeedback (inspired by CAI principles) ‚îú‚îÄ Custom constitutions for domain-specific models ‚îî‚îÄ Self-critique prompts widely used **Key Insight from Book:** \u0026#34;Largely known for Phase 2 (preference data), but Phase 1 (instruction data) methods are used extensively in data filtering across post-training\u0026#34; Translation: Everyone talks about RLAIF, but self-critique for data generation is just as important! Q20: What are the limitations of Constitutional AI? # A: Several challenges and open questions:\n**1. Constitution Design:** ‚îî‚îÄ Who decides what principles to include? ‚îî‚îÄ How to handle conflicting principles? ‚îî‚îÄ Different cultures have different values **2. Principle Grounding:** ‚îî‚îÄ Does model truly \u0026#34;understand\u0026#34; principles? ‚îî‚îÄ Or just pattern-matching on keywords? ‚îî‚îÄ Hard to verify internal reasoning **3. Coverage:** ‚îî‚îÄ Can\u0026#39;t write principles for every edge case ‚îî‚îÄ Model must generalize from examples ‚îî‚îÄ May misapply principles in novel situations **4. Trade-offs:** ‚îî‚îÄ Helpful vs Harmless (classic RLHF dilemma) ‚îî‚îÄ Honesty vs Helpfulness ‚îî‚îÄ Multiple principles may conflict **5. Scalability of Principles:** ‚îî‚îÄ Anthropic\u0026#39;s constitution: ~dozens of principles ‚îî‚îÄ Can you scale to hundreds? Thousands? ‚îî‚îÄ Diminishing returns from more principles Despite these limitations, CAI remains influential because: ‚îî‚îÄ First practical demonstration of synthetic data at scale ‚îî‚îÄ Interpretable (you can read the constitution) ‚îî‚îÄ Modular (easy to update principles) ‚îî‚îÄ Effective (Claude\u0026#39;s success proves it works) FUTURE DIRECTIONS \u0026amp; MODEL COLLAPSE # Q21: What are rubric-based rewards, and why do they matter? # A: Extending RLAIF beyond binary correctness to nuanced evaluation:\n**Chapter 7 (Reasoning) Approach:** ‚îú‚îÄ Use RLVR with binary rewards: correct/incorrect ‚îú‚îÄ Works for: Math, code, verifiable domains ‚îî‚îÄ Limitation: What about non-verifiable tasks? **Chapter 12 Future: Rubric-Based Rewards** ‚îú‚îÄ Instead of \u0026#34;correct/incorrect\u0026#34;, use detailed criteria: ‚îÇ ‚îú‚îÄ Creativity (1-5 scale) ‚îÇ ‚îú‚îÄ Clarity (1-5 scale) ‚îÇ ‚îú‚îÄ Coherence (1-5 scale) ‚îÇ ‚îú‚îÄ Helpfulness (1-5 scale) ‚îÇ ‚îî‚îÄ Style adherence (1-5 scale) ‚îî‚îÄ LLM judges against these rubrics **Why This Matters:** Enables RL training in open-ended domains: ‚îú‚îÄ Creative writing (no single \u0026#34;correct\u0026#34; answer) ‚îú‚îÄ Essay writing ‚îú‚îÄ Summarization ‚îú‚îÄ Style transfer ‚îî‚îÄ Product descriptions **The Process:** 1. Define rubric (what makes a good creative story?) 2. LLM scores response on each criterion 3. Aggregate scores ‚Üí reward signal 4. Run RL (PPO, GRPO, etc.) 5. Model learns to optimize for rubric criteria SIGNIFICANCE: This extends Chapter 7\u0026#39;s RLVR breakthrough (math/code) to ANY domain where you can define quality criteria! Q22: What is \u0026ldquo;model collapse,\u0026rdquo; and should we worry about it? # A: The fear that synthetic data will recursively degrade models:\n**The Theory (Model Collapse):** Model v1 ‚Üí generates data ‚Üí Train Model v2 on it ‚Üí Model v2 generates slightly worse data ‚Üí Train Model v3 ‚Üí Model v3 generates even worse data ‚Üí Train Model v4 ‚Üí ... ‚Üí Models collapse into gibberish **The Mechanisms:** ‚îú‚îÄ Diversity drops (rare facts lost each generation) ‚îú‚îÄ Small mistakes amplified (errors compound) ‚îú‚îÄ Distribution narrowing (only frequent patterns survive) ‚îî‚îÄ \u0026#34;Xerox of a Xerox\u0026#34; effect **The Fear:** If everyone trains on synthetic data, we\u0026#39;ll see cascading degradation across the field! **The Reality (from Book):** **\u0026#34;This has been emphatically rebuked in leading language models\u0026#34;** Translation: Model collapse is NOT happening at frontier labs. Q23: Why isn\u0026rsquo;t model collapse happening in practice? # A: Because labs don\u0026rsquo;t do the naive thing that causes collapse:\n**What WOULD Cause Collapse:** ‚îú‚îÄ Train ONLY on self-generated data (no human data) ‚îú‚îÄ Use repetitive, unfiltered outputs ‚îú‚îÄ Single-model distillation (no diversity) ‚îú‚îÄ No quality control ‚îî‚îÄ Recursive training (v2 only from v1, v3 only from v2) **What Labs ACTUALLY Do (Avoids Collapse):** ‚îú‚îÄ Mix human + synthetic data (especially at frontiers) ‚îú‚îÄ Use diverse teacher models (GPT-4 + Claude + Llama) ‚îú‚îÄ Strong quality filters (reject low-quality outputs) ‚îú‚îÄ Deduplication (remove repeated content) ‚îú‚îÄ Careful prompt curation (diverse questions) ‚îî‚îÄ Ground in reality (web scraping, books, code repos) **Key Insight:** \u0026#34;For today\u0026#39;s frontier training pipelines, synthetic data CAN and SHOULD be used at scale without catastrophic regressions\u0026#34; BUT: You need to use it CAREFULLY ANALOGY: Bad: Photocopying a photocopy repeatedly ‚Üí degrades Good: Scanning original + using multiple high-quality printers ‚Üí maintains quality Q24: Where does human data still matter? # A: Three critical areas where humans remain essential:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 1. CAPABILITY FRONTIERS ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ \u0026#34;Humans must generate data where AIs don\u0026#39;t yet have ability\u0026#34; ‚îÇ ‚îÇ ‚îÇ ‚îÇ Pattern: ‚îÇ ‚îÇ ‚îú‚îÄ First frontier model: Needs human data (no teacher) ‚îÇ ‚îÇ ‚îú‚îÄ Once frontier exists: Synthetic proliferates (distill) ‚îÇ ‚îÇ ‚îî‚îÄ Example: Reasoning was frontier, now distillation works ‚îÇ ‚îÇ ‚îÇ ‚îÇ Current Frontiers (2025): ‚îÇ ‚îÇ ‚îú‚îÄ Multimodal reasoning (vision + text) ‚îÇ ‚îÇ ‚îú‚îÄ Long-context understanding (100K+ tokens) ‚îÇ ‚îÇ ‚îú‚îÄ Agentic planning (tool use chains) ‚îÇ ‚îÇ ‚îî‚îÄ Domain expertise (medicine, law, etc.) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 2. PREFERENCE DATA (STILL DEBATED) ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Academic: \u0026#34;Synthetic performs comparably\u0026#34; ‚îÇ ‚îÇ Industry: \u0026#34;Human data is competitive moat\u0026#34; ‚îÇ ‚îÇ ‚îÇ ‚îÇ Open Questions: ‚îÇ ‚îÇ ‚îú‚îÄ Does human data enable finer control? ‚îÇ ‚îÇ ‚îú‚îÄ Is nuance in preferences important? ‚îÇ ‚îÇ ‚îú‚îÄ Do safety/alignment need human judgment? ‚îÇ ‚îÇ ‚îî‚îÄ Character training may need human input (Ch 18) ‚îÇ ‚îÇ ‚îÇ ‚îÇ Reality: Frontier labs STILL pay for human preferences ‚îÇ ‚îÇ (This suggests they believe it matters) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ 3. EVALUATION GROUND TRUTH ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ LLM-as-a-judge: Scales evaluation (cheap, fast) ‚îÇ ‚îÇ BUT: Benchmark creation still needs humans ‚îÇ ‚îÇ ‚îÇ ‚îÇ Humans establish: ‚îÇ ‚îÇ ‚îú‚îÄ \u0026#34;What correct looks like\u0026#34; (ground truth) ‚îÇ ‚îÇ ‚îú‚îÄ Edge cases and failure modes ‚îÇ ‚îÇ ‚îú‚îÄ Safety boundaries ‚îÇ ‚îÇ ‚îî‚îÄ Novel evaluation criteria ‚îÇ ‚îÇ ‚îÇ ‚îÇ Pattern: Humans define standards, AI judges at scale ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Q25: What\u0026rsquo;s the timeline of synthetic data adoption? # A: Rapid evolution in just 3 years:\n**2022: Early RLHF Era** ‚îú‚îÄ InstructGPT, ChatGPT launch ‚îú‚îÄ ALL data is human-generated ‚îú‚îÄ Llama 2, GPT-3.5: Not reliable enough for synthetic ‚îú‚îÄ Cost: $5-50 per completion ‚îî‚îÄ Human data = only option **2023: Synthetic Emerges** ‚îú‚îÄ GPT-4 class models become reliable ‚îú‚îÄ Stanford Alpaca: 52K synthetic examples (breakthrough!) ‚îú‚îÄ Constitutional AI paper formalizes RLAIF ‚îú‚îÄ UltraFeedback (synthetic preferences) kickstarts DPO ‚îú‚îÄ Cost: \u0026lt;$0.01 per item ‚îî‚îÄ Synthetic starts competing with human data **2024: Synthetic Dominates SFT** ‚îú‚îÄ GPT-4 \u0026gt; humans for most completion writing ‚îú‚îÄ LLM-as-a-judge becomes standard for evaluation ‚îú‚îÄ T√ºlu 3: Mix of synthetic + human (best practice) ‚îú‚îÄ Academic: \u0026#34;Synthetic performs comparably\u0026#34; ‚îî‚îÄ \u0026#34;Synthetic has largely won for instruction data\u0026#34; **2025: Reasoning Era** ‚îú‚îÄ OpenThoughts: 1.2M synthetic reasoning examples ‚îú‚îÄ Datasets grow: 10B+ tokens (vs 10M in 2023!) ‚îú‚îÄ Synthetic critical for reasoning model training (Ch 7) ‚îú‚îÄ Human data still valued for preferences/safety ‚îî‚îÄ \u0026#34;Leading models NEED synthetic data for best performance\u0026#34; KEY MILESTONE: Stanford Alpaca (2023) ‚îî‚îÄ First widely-used open synthetic dataset ‚îî‚îÄ Proved GPT-3.5 good enough for data generation ‚îî‚îÄ Kickstarted open-source synthetic data movement Q26: What are the major synthetic datasets mentioned in the book? # A: Evolution from small to massive:\n**Stanford Alpaca (2023) - The Pioneer** ‚îú‚îÄ 52K instruction-response pairs ‚îú‚îÄ Generated from GPT-3.5 ‚îú‚îÄ Kickstarted open synthetic data movement ‚îú‚îÄ Size: ~10M tokens ‚îî‚îÄ Impact: Proved synthetic viability **UltraFeedback (2023) - Preference Data** ‚îú‚îÄ First prominent synthetic preference dataset ‚îú‚îÄ Kickstarted DPO revolution (Ch 8) ‚îú‚îÄ Academic training commonly uses this ‚îú‚îÄ Size: 64K preference pairs ‚îî‚îÄ Impact: Democratized RLHF alternatives **T√ºlu 3 (2024) - Mixed Approach** ‚îú‚îÄ ~1M synthetic examples ‚îú‚îÄ Mix: GPT-4o + Llama 3.1 405B (diverse teachers!) ‚îú‚îÄ Skill-focused (math, code, instruction-following) ‚îú‚îÄ Size: ~5B tokens ‚îî‚îÄ Impact: Showed mixed human+synthetic works best **OpenThoughts 3 (2025) - Reasoning Era** ‚îú‚îÄ 1.2M reasoning examples ‚îú‚îÄ Distilled from QwQ-32B (Ch 7 reasoning model) ‚îú‚îÄ For training thinking models ‚îú‚îÄ Size: ~10B tokens (1000x growth from Alpaca!) ‚îî‚îÄ Impact: Enabled 20+ reasoning models in 6 months PROGRESSION: 10M tokens ‚Üí 5B tokens ‚Üí 10B tokens 52K examples ‚Üí 1M examples ‚Üí 1.2M examples (3 years of exponential growth!) KEY TAKEAWAYS \u0026amp; QUICK REFERENCE # Ch 12.1 Distillation: \u0026ldquo;Using stronger models (GPT-4o, Claude) to generate training data for weaker models, which has largely replaced human completion writing for SFT due to 100-1000x cost savings and equal or better quality.\u0026rdquo;\nCh 12.2 AI Feedback (RLAIF): \u0026ldquo;Using LLMs as judges to generate preference labels instead of humans, offering 100-1000x cost savings but introducing systematic biases that human data doesn\u0026rsquo;t have, requiring careful mitigation strategies.\u0026rdquo;\nCh 12.3 Constitutional AI: \u0026ldquo;Anthropic\u0026rsquo;s method of using a written \u0026lsquo;constitution\u0026rsquo; (principles) to guide both self-critique (SFT) and preference judgments (RLAIF), kickstarting the field of synthetic preference data and becoming widely adopted in various forms.\u0026rdquo;\nOverall Chapter: \u0026ldquo;The paradigm shift from \u0026lsquo;humans are essential\u0026rsquo; to \u0026lsquo;synthetic data dominates where AI exceeds human reliability\u0026rsquo; - fundamentally changing how we think about training data for post-training, with frontier labs now requiring synthetic data for best performance while still valuing human data for preference nuance and capability frontiers.\u0026rdquo;\n"},{"id":63,"href":"/ai-workflows/rlhf/rlhf2006/ch7_reasoning_inference_time_scaling/","title":"Ch7. Reasoning \u0026 Inference-time Scaling","section":"RLHF 2006","content":" Ch7. REASONING \u0026 INFERENCE-TIME SCALING THE BIG PICTURE - WHY IS THIS BREAKTHROUGH? # Q1: What actually happened in 2025 that makes Chapter 7 revolutionary? # A: Two seismic events changed everything:\nOpenAI o1 (Sep 2024) - Showed reasoning models work at scale DeepSeek R1 (Jan 2025) - Fully documented the recipe openly Timeline:\nBefore: \u0026ldquo;RL doesn\u0026rsquo;t work\u0026rdquo; (2018 famous blog post) After: 20+ reasoning models released in 6 months (Jan-Jul 2025) This is like going from \u0026ldquo;flight is impossible\u0026rdquo; to \u0026ldquo;commercial airlines everywhere\u0026rdquo; in 6 months. The paradigm shifted THAT fast.\nQ2: Why couldn\u0026rsquo;t we do this before 2024? # A: Three critical barriers were overcome:\nBARRIER 1: RL Stability # Problem: RL training was \u0026ldquo;fickle\u0026rdquo; - crashed, failed randomly Solution: Open-source tools matured (TRL, veRL, OpenRLHF) Result: \u0026ldquo;Technical barriers to entry at an all-time low\u0026rdquo; BARRIER 2: Model Capability Threshold # Problem: Smaller/weaker models couldn\u0026rsquo;t learn reasoning via RL Solution: Base models from ~2024 onwards were \u0026ldquo;capable enough\u0026rdquo; Result: RL training could finally elicit reasoning behaviors BARRIER 3: Verifiable Rewards # Problem: How do we know if reasoning is correct? Solution: Math/coding have binary correctness (RLVR) Result: Train without expensive preference data! Q3: What\u0026rsquo;s the difference between RLHF and RLVR? # A: Critical distinction that defines the new era:\nRLHF (Old Way) # Reward: Human preference labels (expensive, subjective) Use case: Chat, safety, style, \u0026ldquo;vibes\u0026rdquo; Cost: $1-10 per preference pair Training: 1-2 epochs, careful not to overfit Example: ChatGPT\u0026rsquo;s politeness, Claude\u0026rsquo;s helpfulness RLVR (New Way) - \u0026ldquo;Reinforcement Learning from Verifiable Rewards\u0026rdquo; # Reward: Binary correctness (right/wrong answer) Use case: Math, coding, reasoning, STEM Cost: Nearly free (automated checking) Training: Hundreds/thousands of epochs until convergence Example: DeepSeek R1\u0026rsquo;s math ability, o1\u0026rsquo;s coding KEY INSIGHT: RLVR doesn\u0026rsquo;t need humans in the loop for correctness! Just check if answer == ground_truth\nCORE CONCEPTS - WHAT IS INFERENCE-TIME SCALING? # Q4: What is \u0026ldquo;inference-time scaling\u0026rdquo;? # A: Using MORE COMPUTE at inference to get BETTER answers.\nTraditional LLM: # ‚îú‚îÄ User asks: \u0026#34;Solve this math problem\u0026#34; ‚îú‚îÄ Model generates: [answer in 100 tokens] ‚îî‚îÄ Done. Fixed cost. Reasoning Model (Inference-time Scaling): # ‚îú‚îÄ User asks: \u0026#34;Solve this math problem\u0026#34; ‚îú‚îÄ Model thinks: [2000 tokens of \u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt; reasoning] ‚îú‚îÄ Model generates: [final answer] ‚îî‚îÄ Uses 20x more tokens = 20x more compute = better accuracy! ANALOGY:\nTraditional = \u0026ldquo;quick guess\u0026rdquo; Reasoning = \u0026ldquo;show your work\u0026rdquo; (like you did in school) Q5: What\u0026rsquo;s the difference between training-time and inference-time scaling? # A: Where you spend the compute:\nTRAINING-TIME SCALING (Traditional) # Spend compute: During training (one-time cost) Method: Bigger models, more training data, longer training Result: Smarter model for ALL users Example: GPT-3 (175B) ‚Üí GPT-4 (1.7T) = bigger = smarter Tradeoff: Expensive upfront, cheap at inference INFERENCE-TIME SCALING (New Era) # Spend compute: During EACH query (per-user cost) Method: Model \u0026ldquo;thinks\u0026rdquo; longer (more tokens) for hard Qs Result: Same model, but \u0026ldquo;tries harder\u0026rdquo; on demand Example: o1 generates 2K-30K thinking tokens per hard Q Tradeoff: Cheaper training, expensive inference (but better!) BREAKTHROUGH: You can now trade inference cost for accuracy! (Like buying \u0026ldquo;thinking time\u0026rdquo; on demand)\nQ6: How does RL training create inference-time scaling? # A: RL teaches the model to \u0026ldquo;think out loud\u0026rdquo; for hard problems:\nThe Process: # Step 1: Model encounters hard math problem\nStep 2: Model generates reasoning chain:\n\u0026lt;think\u0026gt; Let me break this down... First, I\u0026#39;ll try approach A... Wait, that doesn\u0026#39;t work... Let me try approach B... Ah! That works because... Therefore the answer is... \u0026lt;/think\u0026gt; Step 3: Final answer: [correct]\nStep 4: RL reward: +1 (correct!) ‚Üí Reinforce this \u0026ldquo;thinking behavior\u0026rdquo;\nAfter thousands of epochs: # Model learns: \u0026ldquo;Hard problems = think longer = higher accuracy\u0026rdquo; Correlation emerges: More tokens ‚Üí Better performance This is NOT length bias (old RLHF problem) This is PRODUCTIVE reasoning THE CANONICAL RECIPE - DEEPSEEK R1 # Q7: What is DeepSeek R1\u0026rsquo;s training recipe? # A: 4-stage process that became the blueprint for 2025:\nSTAGE 1: \u0026ldquo;Cold-Start\u0026rdquo; (100K+ samples) # What: Sample from earlier RL checkpoint (R1-Zero) Filter: Keep only high-quality reasoning chains Goal: Teach model the PROCESS of reasoning Why \u0026ldquo;cold-start\u0026rdquo;? Learning RL from minimal supervised data (Unlike traditional SFT which needs millions of examples) STAGE 2: Large-Scale RL (The Core) # What: Run RLVR \u0026ldquo;until convergence\u0026rdquo; Data: Reasoning problems (math, coding, STEM) Epochs: HUNDREDS (not 1-2 like traditional fine-tuning!) Reward: Binary correctness (right/wrong) Note: This is where the magic happens - model learns to reason! STAGE 3: Rejection Sampling (Transition to General) # Mix: 3/4 reasoning problems + 1/4 general queries Goal: Don\u0026rsquo;t lose general chat ability Method: Sample multiple answers, keep best ones STAGE 4: Mixed RL (Polish) # RLVR: For reasoning domains (verifiable) RLHF: For general domains (human preferences) Goal: Final model that\u0026rsquo;s both smart AND pleasant Q8: What\u0026rsquo;s revolutionary about this recipe vs previous RLHF? # A: Complete inversion of traditional priorities:\nTraditional RLHF (InstructGPT, ChatGPT): # ‚îú‚îÄ Stage 1: SFT (1M examples, 1-2 epochs) ‚Üê MAIN TRAINING ‚îú‚îÄ Stage 2: RLHF (100K pref pairs) ‚Üê \u0026#34;Cherry on top\u0026#34; ‚îî‚îÄ Focus: Behavior, style, safety DeepSeek R1 (Reasoning Era): # ‚îú‚îÄ Stage 1: Cold-start (100K, minimal SFT) ‚îú‚îÄ Stage 2: RLVR (until convergence) ‚Üê MAIN TRAINING ‚îú‚îÄ Stage 3-4: Polish with SFT+RLHF ‚îî‚îÄ Focus: Capability, performance, correctness PARADIGM SHIFT: RL went from \u0026ldquo;polish\u0026rdquo; to \u0026ldquo;core capability builder\u0026rdquo;\nTHE EXPLOSION - 20+ MODELS IN 6 MONTHS # Q9: Who released reasoning models in 2025? # A: Everyone. Literally everyone:\nJan 2025: DeepSeek R1, Kimi 1.5 Mar 2025: OpenReasoner-Zero (first fully open!) Apr 2025: Seed-Thinking 1.5, Phi-4 Reasoning May 2025: Llama-Nemotron, INTELLECT-2, Xiaomi MiMo, Qwen 3 Jun 2025: Hunyuan-TurboS, Skywork OR-1, OpenThoughts, Magistral Jul 2025+: Kimi K2, GLM-4.5, Nemotron Nano 2, MiniMax-M1\u0026hellip; 20+ models in ~6 months!\nOrganizations: ByteDance, Microsoft, Meta, Alibaba, Mistral, Moonshot AI, OpenBMB, Zhipu AI, Nvidia, MiniMax\u0026hellip;\nKEY INSIGHT: This isn\u0026rsquo;t one lab\u0026rsquo;s secret sauce. This is a FIELD-WIDE revolution.\nQ10: What are the common training techniques across these models? # A: 10 techniques repeatedly used (with examples):\n1. OFFLINE DIFFICULTY FILTERING # What: Pre-filter training data by difficulty Why: Model can only learn from 20-80% solvable problems Who: Seed-Thinking, OpenReasoner-Zero, Phi-4, Qwen 3 Logic: Too easy (100% solve) = no gradient Too hard (0% solve) = no gradient Just right (20-80%) = optimal learning! 2. PER-BATCH ONLINE FILTERING # What: During training, filter problems dynamically Why: Model capability changes as it learns Who: Kimi 1.5, Magistral, Llama-Nemotron Example: Week 1: Model solves 30% ‚Üí include these Week 4: Model solves 80% ‚Üí filter out (too easy) 3. REMOVE KL PENALTY # What: Turn off KL divergence regularization Why: Let model explore reasoning space freely Who: RAGEN, Magistral, OpenReasoner-Zero Insight: Traditional RLHF: KL penalty keeps model close to SFT init Reasoning: Remove KL ‚Üí model can learn NEW behaviors 4. FORMAT REWARDS # What: Reward model for using \u0026lt;think\u0026gt;...\u0026lt;/think\u0026gt; tags Why: Ensure consistent, parseable reasoning format Who: DeepSeek R1, OpenReasoner-Zero, Magistral Impact: Without: Model might reason in inconsistent ways With: Guaranteed structured output for downstream systems 5. LANGUAGE CONSISTENCY REWARDS # What: Penalize switching languages mid-reasoning Why: Better UX, easier to debug Who: DeepSeek R1, Magistral (multilingual models) Example: Bad: \u0026ldquo;Let me solve\u0026hellip; ËÆ©Êàë‰ª¨ËÆ°ÁÆó\u0026hellip; donc la r√©ponse est\u0026hellip;\u0026rdquo; Good: Stick to one language throughout reasoning 6. LENGTH PENALTIES # What: Penalize overthinking (too many reasoning tokens) Why: Combat diminishing returns on long chains Who: Kimi 1.5, INTELLECT-2 Problem: Model might generate 50K tokens to solve 2+2 Solution: Progressive length limits or small penalties 7. LOSS NORMALIZATION (Batch vs Group) # What: Normalize advantages at batch level (not group) Why: Avoid bias towards low-variance problems Who: Magistral, MiMo Comparison: GRPO original: Normalize per group of responses Alternative: Normalize across entire batch 8. PARALLEL TEST-TIME COMPUTE # What: Generate N answers, pick best via majority/scorer Why: Boosts accuracy without retraining Who: DeepSeek R1, Phi-4, Claude 4, DeepSeek-GRM Methods: Method 1: Majority voting (most common answer) Method 2: Scorer model (trained to pick best) 9. TEXT-ONLY REASONING BOOSTS MULTIMODAL # What: Train reasoning on text, improves vision+text too! Why: Reasoning transfers across modalities Who: Magistral, MiMo-VL Surprising finding: Don\u0026rsquo;t need vision data for vision boost! 10. TOGGLEABLE REASONING (System Prompt) # What: User controls reasoning depth via system prompt Why: Fast answers for easy Q\u0026rsquo;s, deep for hard Q\u0026rsquo;s Who: Llama-Nemotron, Qwen 3 Example: System: \u0026ldquo;Use minimal thinking\u0026rdquo; Model: [short chain] ‚Üí fast, cheap System: \u0026ldquo;Think deeply\u0026rdquo; Model: [long chain] ‚Üí accurate, expensive FUTURE DIRECTIONS # Q11: Where is this field going next? # A: Three frontier areas (from the book):\n1. BEYOND MATH/CODE # Current: RLVR works great for verifiable domains Next: How to apply to non-verifiable domains? Challenge: Need new reward signal types 2. DISTILLATION # Current: Train big reasoning model ‚Üí distill to small Next: Can small models learn reasoning directly? Trend: OpenThoughts dataset (distilled reasoning chains) 3. MULTIMODAL REASONING # Current: Text-only reasoning boosts vision (surprising!) Next: End-to-end multimodal reasoning training Example: MiMo-VL, Xiaomi\u0026rsquo;s multimodal reasoning Q12: What\u0026rsquo;s the one thing experts still debate? # A: Whether RL is necessary or if distillation is enough:\nCAMP 1: \u0026ldquo;RL is essential\u0026rdquo; # Evidence: DeepSeek R1 cold-start shows RL creates novel behaviors Claim: Can\u0026rsquo;t just imitate, must explore via RL CAMP 2: \u0026ldquo;Distillation is enough\u0026rdquo; # Evidence: OpenThoughts (distilled from QwQ) works well Claim: Cheaper, faster, good enough for most uses REALITY: Probably both have roles # Frontier: Use RL to discover new reasoning patterns Production: Distill to smaller models for deployment QUICK REFERENCE SUMMARY # Key Timeline # 2018: \u0026ldquo;RL doesn\u0026rsquo;t work\u0026rdquo; conventional wisdom Sep 2024: OpenAI o1 proves reasoning at scale Jan 2025: DeepSeek R1 releases full recipe Jan-Jul 2025: 20+ reasoning models released Core Concepts # RLHF: Human preferences, chat/safety, expensive, 1-2 epochs RLVR: Binary correctness, math/code, free, hundreds of epochs Inference-time scaling: Trade compute for accuracy at test time Reasoning chains: Model \u0026ldquo;thinks out loud\u0026rdquo; in \u0026lt;think\u0026gt; tags DeepSeek R1 Recipe (4 Stages) # Cold-start (100K samples) Large-scale RL (core training) Rejection sampling (generalization) Mixed RL (polish) Top 10 Training Techniques # Offline difficulty filtering (20-80% sweet spot) Per-batch online filtering (dynamic adjustment) Remove KL penalty (exploration freedom) Format rewards (structured output) Language consistency (better UX) Length penalties (combat overthinking) Loss normalization (batch vs group) Parallel test-time compute (majority voting) Text-only boosts multimodal (transfer learning) Toggleable reasoning (user control) "},{"id":64,"href":"/ai-workflows/rlhf/rlhf2006/ch9_sft/","title":"Ch9. Instruction Fine-Tuning (IFT/SFT)","section":"RLHF 2006","content":" Ch9: Instruction Fine-Tuning (IFT/SFT) Q1: What are chat templates and why do they matter? # A: Chat templates are formatting systems that structure conversations into a format language models can process. They use special tokens (like \u0026lt;|im_start|\u0026gt;, \u0026lt;|im_end|\u0026gt;) to mark boundaries between different parts of the conversation.\nExample:\n\u0026lt;|im_start|\u0026gt;system You are a helpful assistant\u0026lt;|im_end|\u0026gt; \u0026lt;|im_start|\u0026gt;user What is 2+2?\u0026lt;|im_end|\u0026gt; \u0026lt;|im_start|\u0026gt;assistant The answer is 4\u0026lt;|im_end|\u0026gt; Q2: What are the three message roles and how do they differ? # A:\nSystem: Sets persistent context/instructions for the entire conversation. Applied once at the beginning. Think of it as \u0026ldquo;background instructions\u0026rdquo; that influence all responses. User: Messages from the person using the AI Assistant: Individual responses from the AI model Key point: System provides context that affects all assistant responses, but each assistant message is a separate turn in the conversation.\nQ3: What is prompt masking in IFT/SFT? # A: Prompt masking means the model sees all tokens (prompt + response) but the loss is only calculated on the assistant\u0026rsquo;s response tokens. The prompt tokens are excluded from loss computation.\nWhy? We want the model to learn to generate good responses, not to predict user queries.\nExample:\nUser: \u0026#34;What is 2+2?\u0026#34; ‚Üê SEEN but NO LOSS applied Assistant: \u0026#34;The answer is 4\u0026#34; ‚Üê SEEN and LOSS applied Q4: How is \u0026ldquo;masking\u0026rdquo; different in IFT/SFT vs BERT/GPT pretraining? # Training Type Masked Tokens Visibility Loss Applied To Purpose BERT ‚ùå Hidden/Not seen ‚úÖ Masked tokens Predict missing words GPT Pretraining ‚ùå Hidden (future tokens) ‚úÖ All tokens Predict next token IFT/SFT ‚úÖ Fully visible ‚úÖ Only assistant responses Generate good responses Critical Difference:\nBERT/GPT: Masked tokens NOT SEEN during training, INCLUDED in loss IFT/SFT: Masked tokens FULLY SEEN during training, EXCLUDED from loss Q5: Why use multi-turn conversations instead of just single-turn? # A: Multi-turn data teaches the model:\nContext tracking - understand references to previous turns Conversation coherence - maintain consistency across dialogue Real conversation skills - handle follow-ups and clarifications Example why this matters:\nTurn 1 User: \u0026#34;I have a dog\u0026#34; Assistant: \u0026#34;What breed?\u0026#34; Turn 2 User: \u0026#34;He\u0026#39;s very playful\u0026#34; ‚Üê needs to understand \u0026#34;he\u0026#34; = the dog Single-turn only would make models bad at maintaining conversational context.\nQ6: What are the supervised learning pairs in IFT/SFT? # A: The pairs are: [Full conversation context] ‚Üí [Next assistant response]\nSingle-turn:\nInput: [System + User1] ‚Üí Target: [Assistant1] Multi-turn (unrolled into multiple training examples):\nExample 1: [System + User1] ‚Üí Target: [Assistant1] Example 2: [System + User1 + Assistant1 + User2] ‚Üí Target: [Assistant2] Example 3: [System + User1 + Assistant1 + User2 + Assistant2 + User3] ‚Üí Target: [Assistant3] Key insight: One N-turn conversation creates N training examples, each predicting a different assistant response.\nQ7: What exactly gets \u0026ldquo;masked\u0026rdquo; in multi-turn training? # A:\nFor Turn 2 example:\nInput sequence (what model sees): [System + User1 + Assistant1 + User2 + Assistant2] Masked from loss: [System + User1 + Assistant1 + User2] Loss applied to: [Assistant2] only The model sees everything for context, but only learns to generate the current assistant response.\nQ8: What are the key implementation differences from pretraining? # Aspect Pretraining IFT/SFT Batch size Large (1024-2048) Smaller (256) Loss applied to All tokens Only assistant responses Training data Raw text Structured conversations Key differences:\nSmaller batch sizes - fewer GPUs needed Prompt masking - loss only on responses Multi-turn masking - only final assistant turn per example Q9: What are the best practices for instruction tuning? # A:\nQuality over quantity - High-quality completions are crucial (model learns from responses) ~1M prompts sufficient for excellent results (diminishing returns after) Data distribution matters - Use prompts similar to target use cases Overall optimization - Models can recover from noise; focus on complete pipeline Content Summary # Core Concept: Instruction fine-tuning transforms pretrained language models into conversational assistants by teaching them to generate appropriate responses to user queries.\nKey Mechanisms:\nChat templates structure conversations with role-based formatting Prompt masking ensures models learn response generation, not query prediction Multi-turn training develops conversational coherence and context tracking Supervised learning pairs full context with target responses Critical Insight: The \u0026ldquo;masking\u0026rdquo; terminology is overloaded:\nIn IFT/SFT: \u0026ldquo;masked\u0026rdquo; = excluded from loss (but still visible to model) In BERT/GPT: \u0026ldquo;masked\u0026rdquo; = hidden from model (and included in loss for prediction) Most Important Takeaway: In IFT/SFT, the model sees the entire conversation history for context, but only learns to predict the assistant\u0026rsquo;s responses. This creates models that can follow instructions while maintaining conversational context.\n"},{"id":65,"href":"/healthcare/clinical_ai/clinical_ai_usecase/","title":"Clinical Data Science","section":"AI Applications","content":" Clinical Data Science Core Priority: Retrieval-Augmented Generation (RAG) # RAG is one of the most in-demand skills in clinical GenAI due to:\nThe need to ground LLMs in real patient data Compliance, privacy, and traceability Applications like: Clinical Question Answering Summarization of EHRs Evidence-based recommendations Key Tools: # Vector DBs: Vertex AI Search, Pinecone, FAISS LLMs: Gemini, GPT-4, PaLM, Med-PaLM Frameworks: LangChain, LlamaIndex, Vertex Extensions Other High-Demand Skillsets # Clinical NLP \u0026amp; Information Extraction\nNamed Entity Recognition (NER) Negation detection Temporal event extraction Tools: scispaCy, MedSpaCy, cTAKES, ClinicalBERT LLMOps \u0026amp; GenAI Engineering\nPrompt tracking and versioning Chain-of-Thought reasoning pipelines RAG monitoring and evaluation Tools: LangChain, LangSmith, PromptLayer, Trulens 3. Knowledge Graphs \u0026amp; Ontologies # - UMLS, SNOMED, HPO integration - Graph-based document ranking - Symbolic-neural hybrid reasoning - **Tools**: Neo4j, BioPortal APIs, KG-BERT 4. Temporal Modeling \u0026amp; Phenotyping # - Patient timeline extraction - Longitudinal modeling - Conversion to OMOP/FHIR representations - **Tools**: PyOMOP, Synthea, FHIR parsers 5. Multimodal Clinical AI # - OCR and document understanding - Fusion of tables, images, and text - Radiology + Report generation - **Tools**: Document AI (GCP), Form Recognizer (Azure), BioGPT-Vision "},{"id":66,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/nlp_clinical_text/","title":"Clinical Text Feature Extraction Using Dictionary-Based Filtering","section":"C2 Clinical Data","content":" üß¨ Clinical Text Feature Extraction Using Dictionary-Based Filtering # This guide demonstrates a simplified approach for processing clinical text without removing PHI directly. Instead, it extracts only medical terms from a predefined dictionary (simulated knowledge graph), which passively excludes PHI and enables downstream analyses.\n‚úÖ Objective # Extract present, positive mentions of clinical concepts (e.g., diseases, symptoms, drugs). Avoid mentions that are negated or refer to historical/family context. Demonstrate the principle: \u0026ldquo;Keep only medical terms\u0026rdquo; as an alternative to direct PHI removal. üßæ Input Example # Patient complains of chest pain. No signs of pneumonia. History of diabetes mellitus. Prescribed metformin. Mother had breast cancer. üß† Procedure Overview # Define a medical term dictionary (simulating a knowledge graph). Split the clinical note into sentences. Ignore sentences with negation or irrelevant context. Match and extract terms from the dictionary. Output structured features for downstream use. üß™ Code Implementation (Python) # import re # 1. Simulated clinical note clinical_note = \u0026#39;\u0026#39;\u0026#39; Patient complains of chest pain. No signs of pneumonia. History of diabetes mellitus. Prescribed metformin. Mother had breast cancer. \u0026#39;\u0026#39;\u0026#39; # 2. Simulated knowledge graph (medical term dictionary) medical_terms = { \u0026#34;chest pain\u0026#34;: \u0026#34;symptom\u0026#34;, \u0026#34;pneumonia\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;diabetes mellitus\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;metformin\u0026#34;: \u0026#34;drug\u0026#34;, \u0026#34;breast cancer\u0026#34;: \u0026#34;disease\u0026#34; } # 3. Split into sentences sentences = re.split(r\u0026#39;\\.\\s*\u0026#39;, clinical_note.strip()) features = [] # 4. Process each sentence for sentence in sentences: sentence_lower = sentence.lower() # 5. Skip negated or historical context if \u0026#34;no \u0026#34; in sentence_lower or \u0026#34;history of\u0026#34; in sentence_lower or \u0026#34;mother had\u0026#34; in sentence_lower: continue # 6. Match medical terms for term in medical_terms: if term in sentence_lower: features.append({ \u0026#34;term\u0026#34;: term, \u0026#34;type\u0026#34;: medical_terms[term], \u0026#34;sentence\u0026#34;: sentence.strip() }) # 7. Output extracted features for feature in features: print(f\u0026#34;Found {feature[\u0026#39;type\u0026#39;]} ‚Üí \u0026#39;{feature[\u0026#39;term\u0026#39;]}\u0026#39; in: \\\u0026#34;{feature[\u0026#39;sentence\u0026#39;]}\\\u0026#34;\u0026#34;) üì§ Sample Output # Found symptom ‚Üí \u0026#39;chest pain\u0026#39; in: \u0026#34;Patient complains of chest pain\u0026#34; Found drug ‚Üí \u0026#39;metformin\u0026#39; in: \u0026#34;Prescribed metformin\u0026#34; üìå Summary # This method:\nAvoids direct PHI detection Extracts useful clinical concepts only Can be adapted to larger vocabularies and real NLP tools (e.g., spaCy, scispaCy, NegEx) Perfect for research scenarios where structured clinical features are needed but full de-identification is too complex.\n"},{"id":67,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/clinical_text_mining_pipeline/","title":"Clinical Text Mining Pipeline (Steps 1‚Äì5)","section":"C2 Clinical Data","content":" üè• Clinical Text Mining Pipeline (Steps 1‚Äì5) # This document outlines a high-level clinical text mining pipeline using knowledge graphs, NLP, and structured indexing. The goal is to extract, enrich, and analyze clinical concepts from raw EMR text.\nüßæ Step 1: Preprocessing Clinical Documents # Goal: Prepare and normalize clinical notes for processing.\nTools: Text cleaning, sentence segmentation, tokenizer.\n# Example: Clean and split into sentences import re clinical_note = \u0026#34;Pt c/o chest pain. No signs of pneumonia. History of stroke. Prescribed metformin.\u0026#34; sentences = re.split(r\u0026#39;\\.\\s*\u0026#39;, clinical_note.lower()) üß† Step 2: Extract Terms Using Knowledge Graph + NLP # Goal: Identify medical terms using a knowledge graph and remove ambiguous, negated, or contextual mentions.\nTools: Knowledge Graph (e.g., UMLS), NegEx, ConText\n# Simulated medical term dictionary (knowledge graph-based) medical_terms = {\u0026#34;chest pain\u0026#34;: \u0026#34;symptom\u0026#34;, \u0026#34;pneumonia\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;stroke\u0026#34;: \u0026#34;disease\u0026#34;, \u0026#34;metformin\u0026#34;: \u0026#34;drug\u0026#34;} # Filtered sentences (simulate negation/context removal) filtered_mentions = [] for s in sentences: if \u0026#34;no \u0026#34; in s or \u0026#34;history of\u0026#34; in s: continue for term in medical_terms: if term in s: filtered_mentions.append(term) üóÇÔ∏è Step 3: Index Positive, Present Mentions # Goal: Store structured, filtered term mentions for later search.\nTools: JSON/DB-based indexing, storing patient-term mappings.\nindexed_mentions = [ {\u0026#34;patient_id\u0026#34;: 1, \u0026#34;term\u0026#34;: \u0026#34;chest pain\u0026#34;}, {\u0026#34;patient_id\u0026#34;: 1, \u0026#34;term\u0026#34;: \u0026#34;metformin\u0026#34;}, ] üß≠ Step 4: Query-Time Semantic Expansion # Goal: Expand the user‚Äôs query using KG (synonyms, variants, etc.) and disambiguate based on context.\nTools: Knowledge Graph (UMLS), synonym/semantic type lookup, optional filters\nquery = \u0026#34;stroke\u0026#34; expanded_terms = [\u0026#34;stroke\u0026#34;, \u0026#34;cva\u0026#34;, \u0026#34;cerebrovascular accident\u0026#34;] # Disambiguate (simplified) def is_valid(term, patient_age, season): return not (term == \u0026#34;heatstroke\u0026#34; and patient_age \u0026lt; 18 and season == \u0026#34;summer\u0026#34;) üìä Step 5: Build Patient-Feature Matrix for Analysis # Goal: Aggregate term mentions per patient for cohort selection and modeling.\nTools: Pandas, matrix construction, temporal tagging\nfrom collections import defaultdict feature_matrix = defaultdict(lambda: {\u0026#34;stroke_mention\u0026#34;: 0}) patient_metadata = {1: {\u0026#34;age\u0026#34;: 65, \u0026#34;season\u0026#34;: \u0026#34;spring\u0026#34;}} for mention in indexed_mentions: pid = mention[\u0026#34;patient_id\u0026#34;] term = mention[\u0026#34;term\u0026#34;] if term in expanded_terms and is_valid(term, patient_metadata[pid][\u0026#34;age\u0026#34;], patient_metadata[pid][\u0026#34;season\u0026#34;]): feature_matrix[pid][\u0026#34;stroke_mention\u0026#34;] += 1 print(dict(feature_matrix)) ‚úÖ Summary # Step Goal Tools 1 Clean \u0026amp; tokenize notes Regex, NLP 2 Extract clean medical terms KG, NegEx, filtering 3 Store structured mentions JSON, DB 4 Expand/interpret queries KG, synonyms, disambiguation 5 Analyze for research Patient-feature matrix, Pandas This modular pipeline separates data preparation from query-time flexibility, making it robust and reusable.\n"},{"id":68,"href":"/ai-workflows/genai/multimodel_llms/components_fusions/","title":"Core Components and Fusion Strategies in Multimodal LLMs","section":"Multimodal LLMs","content":" Core Components of a Multimodal LLM Visual Encoder\nConverts input images into feature embeddings. Common choices include CLIP, ViT, and EVA.\nModality Adapter (Aligner)\nProjects or transforms visual features to be compatible with the language model‚Äôs embedding space (e.g., via MLP or cross-attention).\nLanguage Model (LLM)\nA large pretrained language model (e.g., LLaMA, GPT) that consumes both text and aligned visual inputs to generate or classify responses.\nFusion Strategies in Multimodal LLMs # 1. Projection + Token Injection # Models: BLIP-2, LLaVA\nHow it works:\nVisual features are extracted using a frozen image encoder (e.g., ViT or CLIP). These features are projected via an MLP to match the LLM\u0026rsquo;s token embedding size. The projected visual tokens are prepended or interleaved with text tokens. # Hugging Face-style pseudocode image_embeds = vision_encoder(image) # Shape: (batch, num_patches, hidden_dim) projected_embeds = visual_proj(image_embeds) # Match LLM hidden size input_embeds = torch.cat([projected_embeds, text_token_embeds], dim=1) output = llm(inputs_embeds=input_embeds) 2. Cross-Attention Adapters # Models: Flamingo, MiniGPT-4\nHow it works:\nVisual tokens are kept separate from text tokens. The LLM has cross-attention layers where text tokens attend to visual context. # Pseudocode with cross-attn text_embeds = llm.text_embeddings(text_input) visual_context = vision_encoder(image) for block in llm.transformer_blocks: text_embeds = block.self_attn(text_embeds) text_embeds = block.cross_attn(text_embeds, context=visual_context) 3. Joint Pretraining (Early Fusion) # Models: Unified-IO, GIT, PaLI\nHow it works:\nImages are tokenized (as patches or regions). Both image and text tokens are passed together into a unified transformer. # Pseudocode for joint vision-text transformer image_tokens = patch_embed(image) # ViT-style patch tokens text_tokens = tokenizer(text) all_tokens = torch.cat([image_tokens, text_tokens], dim=1) output = joint_transformer(all_tokens) "},{"id":69,"href":"/ai-workflows/rlhf/rlhf2006/ch6_vs_ch9_data_comparison/","title":"Data Preparation in RLHF -- Ch6 (Preference Data) vs Ch9 (SFT Data)","section":"RLHF 2006","content":" Data Prep in RLHF - Ch6 (Preference Data) vs Ch9 (SFT Data) Data Preparation Comparison: Ch6 vs Ch9 # Aspect Ch6: Preference Data Ch9: SFT/IFT Data Data Structure PAIRWISE comparisons: (prompt, chosen_response, rejected_response) SINGLE examples: (prompt, good_response) Purpose Learn to JUDGE which response is better Learn to GENERATE good responses Data Format Example {\u0026quot;prompt\u0026quot;: \u0026quot;What is 2+2?\u0026quot;,\n\u0026quot;chosen\u0026quot;: \u0026quot;The answer is 4\u0026quot;,\n\u0026quot;rejected\u0026quot;: \u0026quot;5\u0026quot;} {\u0026quot;prompt\u0026quot;: \u0026quot;What is 2+2?\u0026quot;,\n\u0026quot;response\u0026quot;: \u0026quot;The answer is 4\u0026quot;} Collection Method - Side-by-side comparison UI\n- Likert scales (5-point, 8-point)\n- Thumbs up/down\n- ChatBotArena - Human-written high-quality examples\n- Curated Q\u0026amp;A pairs\n- Single demonstrations Data Source Human labelers comparing responses\nOR\nStructured/Synthetic:\n- Correct vs incorrect (math)\n- With constraint vs without Human-written completions\nOR\nCurated high-quality examples\nOR\nSynthetic (from stronger models) Signal Type Comparative/Relative: Which is better? Absolute: This is a good response Typical Dataset Size ~100K preference pairs (InstructGPT) ~10K-1M examples (InstructGPT: 10K, modern: ~1M) Multi-turn Handling - Preference on final turn only\n- Continue with \u0026ldquo;chosen\u0026rdquo; answer\n- Mask previous turns from loss - Each turn = separate training example\n- Unroll N-turn ‚Üí N examples\n- Mask prompts/previous turns Used to Train Ch7: Reward Model Ch9: SFT Model (initial policy) What Gets Trained L = -log(œÉ(r(chosen) - r(rejected))) L = -log P(response|prompt) Next Stage Usage Ch11: RL training\n- Same/similar prompts can be reused\n- RM provides scores Ch7: RM base model\nCh11: RL starting policy\n- Policy generates responses Key Insight: The DATA TYPES are FUNDAMENTALLY DIFFERENT # Ch9 SFT Data Says: \u0026ldquo;Here\u0026rsquo;s a good response. Learn to generate this.\u0026rdquo; { \u0026#34;prompt\u0026#34;: \u0026#34;Write a poem about goldfish\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;Golden swimmer, circling slow...\u0026#34; } Ch6 Preference Data Says: \u0026ldquo;Between these two responses, A is better than B. Learn to prefer A.\u0026rdquo; { \u0026#34;prompt\u0026#34;: \u0026#34;Write a poem about goldfish\u0026#34;, \u0026#34;chosen\u0026#34;: \u0026#34;Golden swimmer, circling slow... (follows constraint)\u0026#34;, \u0026#34;rejected\u0026#34;: \u0026#34;In circles bright, the goldfish glides... (violates constraint)\u0026#34; } The Complete RLHF Pipeline # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Pretrained Model ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Ch9: SFT ‚îÇ ‚îÇ Ch6: Collect ‚îÇ ‚îÇ ‚îÇ ‚îÇ Preference ‚îÇ ‚îÇ Data: ‚îÇ ‚îÇ Data ‚îÇ ‚îÇ (prompt, ‚îÇ ‚îÇ ‚îÇ ‚îÇ response) ‚îÇ ‚îÇ Data: ‚îÇ ‚îÇ ‚îÇ ‚îÇ (prompt, ‚îÇ ‚îÇ Single good ‚îÇ ‚îÇ chosen, ‚îÇ ‚îÇ examples ‚îÇ ‚îÇ rejected) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ Comparisons ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ SFT Model ‚îÇ‚îÄ‚îÄbase model‚îÄ‚Üí‚îÇ Ch7: RM ‚îÇ ‚îÇ (Policy ‚îÇ ‚îÇ Training ‚îÇ ‚îÇ starting ‚îÇ ‚îÇ ‚îÇ ‚îÇ point) ‚îÇ ‚îÇ Uses Ch6 data ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Ch11: RL ‚îÇ ‚îÇ Optimization ‚îÇ ‚îÇ ‚îÇ ‚îÇ Policy: Ch9 ‚îÇ ‚îÇ Scorer: Ch7 ‚îÇ ‚îÇ Prompts: Ch6 ‚îÇ ‚îÇ (or similar) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Summary # The fundamental difference between Ch6 and Ch9 data lies in their learning objectives:\nCh6 (Preference Data): Teaches models to discriminate between better and worse responses through pairwise comparisons, ultimately training a Reward Model Ch9 (SFT Data): Teaches models to generate appropriate responses through demonstration, creating the initial policy for RL optimization Both are essential but serve distinct roles in the RLHF pipeline, with Ch9 establishing generation capabilities and Ch6 enabling quality assessment.\n"},{"id":70,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/data_quality_labeling_weak_supervision_qa/","title":"Data Quality, Labeling, and Weak Supervision in Clinical ML","section":"C3 ML Healthcare","content":" Data Quality, Labeling, and Weak Supervision in Clinical ML # Q1: What does \u0026ldquo;Garbage In, Garbage Out\u0026rdquo; mean in machine learning? # It means that no model, no matter how advanced, can compensate for poor-quality data. If your input data is noisy, biased, irrelevant, or mislabeled, your model will reflect those flaws.\n‚úÖ The choice of data and problem matters more than the algorithm itself.\nQ2: Can large, rich datasets still be garbage? # Yes ‚Äî if the data is fundamentally flawed or based on faulty assumptions (like phrenology), more volume just means more noise.\nüìå Data quality ‚â† data quantity.\nQ3: What makes clinical data especially tricky to label accurately? # Lack of standardized label definitions Evolving medical criteria (e.g. changing diabetes thresholds) Some labels (e.g., mortality) are easier to pin down Others (e.g., pneumonia, hypertension) require complex confirmation (labs, imaging, notes) üß† Medical label creation is hard, expensive, and often subjective.\nQ4: How do we deal with label noise in practice? # Label noise is inevitable, but manageable.\nStrategies:\nHave domain experts label a subset for benchmarking Use multiple reviewers to estimate disagreement rate Triangulate labels (e.g., combine ICD codes + meds + clinician notes) üìâ This reduces label noise but often shrinks dataset size.\nQ5: Is it ever okay to use noisy labels? # Surprisingly, yes ‚Äî if you have enough data.\nüìà A Stanford study found:\n90% label accuracy ‚âà baseline (with 50% more data) 85% label accuracy ‚âà baseline (with 100% more data) ‚úÖ Rule of thumb:\n10% noise ‚Üí 1.5√ó more data 15% noise ‚Üí 2√ó more data Q6: What is weak supervision? # Weak supervision refers to learning from labels that are:\nNoisy Incomplete Imperfectly defined This is common in healthcare due to:\nThe cost of expert labeling The complexity of clinical truth üë®‚Äç‚öïÔ∏è That‚Äôs why domain experts + scalable label strategies are a key bottleneck in clinical ML.\nQ7: If training data is noisy, what about the test set? # The test set must be as clean as possible.\nWhy?\nIf test labels are noisy, your evaluation metrics will be inaccurate It may underestimate model performance, leading to incorrect conclusions üìå Training set: can handle some noise.\nüìå Test set: must approximate gold-standard ground truth.\nüîë Final Takeaways # Principle Why It Matters Garbage In, Garbage Out Bad input = bad model, no matter the algorithm Labels ‚â† Truth Always validate how close your labels are to clinical reality More Data ‚â† Better Data Large, outdated, or noisy datasets can harm performance Weak Supervision Works (With Scale) Noisy labels can be offset by higher volume Test Set Must Be Clean Final evaluation must reflect ground truth "},{"id":71,"href":"/ai-workflows/data/data-centric-ai/","title":"Data-Centric AI (DCAI)","section":"Data","content":" Data-Centric AI Reference: MIT Data-Centric AI course Topic Q\u0026amp;A Summary 1. Data-Centric AI vs. Model-Centric AI 2. Label Errors and Confident Learning 3. Advanced Confident Learning, LLM and GenAI applications 4. Class Imbalance, Outliers, and Distribution Shift 5. Dataset Creation and Curation 6. Data-centric Evaluation of ML Models 7. Data Curation for LLMs 8. Growing or Compressing Datasets 9. Interpretability in Data-Centric ML 10. Encoding Human Priors: Data Augmentation and Prompt Engineering 11. Data Privacy and Security Q1: How does Data-Centric AI differ from Model-Centric AI? # Model-Centric AI improves models assuming fixed data. Data-Centric AI improves data quality, coverage, and structure, recognizing data as the bottleneck for model success. Q2: Why are Label Errors and Confident Learning crucial? # Label errors silently degrade model performance. Confident Learning identifies mislabeled data points using model predictions and corrects them systematically. Q3: What advances exist in Confident Learning and LLM/GenAI applications? # Advanced Confident Learning enhances robustness to noisy outputs. It\u0026rsquo;s applied to improve foundation models like LLMs by cleaning training and synthetic data. Q4: How are Class Imbalance, Outliers, and Distribution Shift handled? # Class imbalance is managed via over/under-sampling, SMOTE, and weighting. Outliers are detected using isolation techniques or autoencoders. Distribution shifts are diagnosed and corrected with careful monitoring and adaptation. Q5: What is essential about Dataset Creation and Curation? # Good datasets start with thoughtful design, balanced sampling, and robust label validation. Crowdsourcing must be augmented with consensus models like Dawid-Skene or CROWDLAB. Q6: How does Data-Centric Evaluation of ML Models change standard practices? # Not just global metrics: need slice-based evaluation, error analysis, and influence functions. Subpopulations and rare cases must be properly assessed. Q7: How is Data Curation for LLMs unique? # LLMs memorize training data deeply. Curating high-quality fine-tuning datasets, synthetic data filtering, and evaluation by uncertainty quantification is critical. Q8: What role does Growing or Compressing Datasets play? # Active learning grows datasets smartly by labeling only informative samples. Core-set selection compresses datasets while preserving model performance, making training efficient. Q9: How does Interpretability relate to Data-Centric AI? # Models are only as interpretable as their features. Human-in-the-loop feature engineering ensures features are understandable, relevant, and actionable. Q10: How do we Encode Human Priors into Models? # Via Data Augmentation: enriching datasets to encode invariances (e.g., rotation, Mixup). Via Prompt Engineering: guiding LLMs at inference time with careful input manipulation. Q11: How do we secure Data Privacy and Security? # ML models risk leaking sensitive information. Defenses include membership inference mitigation, differential privacy, model regularization, and careful threat modeling. Q12: How does the full picture of Data-Centric AI flow? # Frame the problem with a Data-Centric mindset. Curate a well-constructed, balanced, and interpretable dataset. Detect and fix label errors, outliers, and bias early. Train models, but always re-evaluate data quality after errors. Focus evaluations on slices and high-loss examples. Grow datasets when needed (active learning) or compress intelligently (core-sets). Secure models against privacy attacks. Continuously refine, because data evolves in deployment. Q13: Final Takeaway # In Data-Centric AI, data is the model. Every improvement ‚Äî in accuracy, fairness, robustness, trust, and security ‚Äî roots back to the data. "},{"id":72,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/diagnostic_metrics_and_curves/","title":"Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations","section":"C3 ML Healthcare","content":" Diagnostic Metrics, Anchoring Perspectives, and Curve Interpretations # This guide summarizes the core diagnostic metrics based on anchoring logic (condition vs. prediction), and how these metrics relate to ROC and PR curves ‚Äî especially under balanced vs. imbalanced class distributions.\nüîπ Test-Centric Metrics (Anchored on Actual Condition) # Evaluates test performance, independent of disease prevalence. Anchor: Ground truth label (actual condition). Positive-Focused (Sensitivity) # Fix actual Positive label. Incorrect prediction: False Negative (FN). Pair: (TP, FN) Sensitivity = TP / (TP + FN) Negative-Focused (Specificity) # Fix actual Negative label. Incorrect prediction: False Positive (FP). Pair: (TN, FP) Specificity = TN / (TN + FP) üî∏ Outcome-Centric Metrics (Anchored on Prediction) # Evaluates usefulness of test result, dependent on both test performance and prevalence. Anchor: Test result (prediction output). Positive-Focused (PPV / Precision) # Fix Positive prediction. Incorrect prediction: False Positive (FP). Pair: (TP, FP) Positive Predictive Value (PPV) = TP / (TP + FP) Negative-Focused (NPV) # Fix Negative prediction. Incorrect prediction: False Negative (FN). Pair: (TN, FN) Negative Predictive Value (NPV) = TN / (TN + FN) üìä Extension to ROC and PR Curves # üéØ ROC Curve (Receiver Operating Characteristic) # What it does: # Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) across thresholds. TPR = Sensitivity = TP / (TP + FN) FPR = 1 ‚àí Specificity = FP / (FP + TN) These metrics are calculated by conditioning on the actual class labels, not predictions. Anchoring View: # ‚úÖ Test-Centric / Condition-Anchored Starts from actual condition and evaluates how well the test distinguishes between classes. Independent of class imbalance in its calculation. Use Case: # Suitable when both positive and negative classes are equally important. Can be misleading in highly imbalanced datasets (e.g., rare disease). üìà Precision-Recall (PR) Curve # What it does: # Plots Precision (PPV) vs. Recall (Sensitivity) across thresholds. Precision = TP / (TP + FP) Recall = TP / (TP + FN) Anchoring View: # ‚úÖ Outcome-Centric / Prediction-Anchored Focuses on the model‚Äôs positive predictions and how often they are correct. Particularly useful for evaluating performance on the positive class in imbalanced datasets. Use Case: # Ideal for problems with class imbalance, where the positive class is rare but important (e.g., cancer detection, fraud, anomaly detection). Answers: ‚ÄúWhen the model says positive, can I trust it?‚Äù üß† Summary of Metric Anchors and Curve Use # Curve Type Metrics Used Anchored On Evaluation Focus Best For ROC TPR (Sensitivity), FPR Actual condition Discrimination ability Balanced class settings PR Precision (PPV), Recall Prediction output Precision of predictions Imbalanced settings üí° Takeaway: # ROC Curve is a test-centric (condition-anchored) tool: great for balanced data, focuses on test performance across thresholds. PR Curve is an outcome-centric (prediction-anchored) tool: best for imbalanced data, reflects how reliable positive predictions are. "},{"id":73,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/ethics_in_ai_healthcare_qna/","title":"Ethics in AI for Healthcare","section":"C2 Clinical Data","content":" Ethics in AI for Healthcare: A Guided Q\u0026amp;A Framework # This document presents a structured chain-of-thought (CoT) using guiding questions and answers to understand ethical considerations in the development and deployment of AI in healthcare, based on Module 7 from the Stanford \u0026ldquo;Introduction to Clinical Data\u0026rdquo; course.\n1. Why is ethics important in the context of AI in healthcare? # Answer:\nAI tools impact patients directly or indirectly, whether through their development (research) or their deployment (clinical practice). Each of these domains carries different ethical responsibilities that must be considered and governed carefully.\n‚û°Ô∏è Leads to: Understanding the foundations of research ethics.\n2. How has the field of research ethics developed over time? # Answer:\nThrough responses to unethical practices (e.g., Tuskegee Study, Nazi experiments), a series of ethical frameworks and regulations emerged, including the Nuremberg Code, the Declaration of Helsinki, and most notably, the Belmont Report.\n‚û°Ô∏è Leads to: A deeper look into the Belmont Report and its enduring impact.\n3. What does the Belmont Report contribute to research ethics? # Answer:\nIt introduces three core principles:\nRespect for Persons: Informed consent and autonomy Beneficence: Minimize harm, maximize benefit Justice: Fair distribution of research benefits and burdens ‚û°Ô∏è Leads to: Applying these principles to modern AI data sources.\n4. Where does AI get its data, and what ethical concerns arise? # Answer:\nAI uses data from research repositories, clinical records, and even consumer devices. Ethical concerns include consent validity, privacy, data security, and the risk of underrepresenting vulnerable populations.\n‚û°Ô∏è Leads to: Addressing secondary uses of data and consent workarounds.\n5. How can researchers ethically use data collected for other purposes? # Answer:\nVia:\nQA exemptions Use of de-identified data IRB-approved waiver of consent\nThese methods are sometimes necessary but ethically controversial due to risks of eroding public trust. ‚û°Ô∏è Leads to: The ethical dilemma of returning individual results.\n6. Should researchers return results to participants? # Answer:\nIt depends. Options range from never returning results (to avoid harm/confusion) to always returning them (to respect autonomy). Most agree on a middle ground: only return results that are valid and actionable.\n‚û°Ô∏è Leads to: Examining systems where research and practice are merged‚Äîlike a Learning Health System.\n7. What is a Learning Health System (LHS), and how does it relate to AI? # Answer:\nAn LHS continuously learns from clinical care data to improve outcomes. AI is central to this feedback loop, but it blurs the line between research and care, making traditional ethical boundaries harder to apply.\n‚û°Ô∏è Leads to: Rethinking ethical frameworks for hybrid systems like LHS.\n8. Is there an ethical model better suited for a Learning Health System? # Answer:\nYes. A proposed model includes duties to:\nRespect patients (via transparency, not just consent) Improve care (beneficence) Reduce inequality (justice) Engage both clinicians and patients in the learning process\nHowever, it lacks strict rules for handling trade-offs between these duties. Summary:\nEach principle in the Belmont Report supports the others. Respect enables informed choice, beneficence ensures that choice isn\u0026rsquo;t harmful, and justice guarantees fairness across all participants. As AI transforms healthcare, our ethical thinking must evolve accordingly.\n"},{"id":74,"href":"/ai-workflows/eval/eval_infra/","title":"Eval Infra: Verifiable vs Non-Verifiable vs Hybrid","section":"Eval","content":" Eval Infra: Verifiable (STEM) vs Non-Verifiable vs Hybrid Why These 5 Layers? Separation of Concerns # Each layer serves a distinct purpose in the AI evaluation lifecycle, from research to production deployment.\nLayer Purpose Key Question Stakeholder Output L1: Benchmark Design Define what \u0026ldquo;good\u0026rdquo; means What are we measuring? Research Scientists Test suite + evaluation protocol L2: Evaluation Execution Actually measure performance How do we score it? ML Engineers Raw scores/labels per example L3: Scalability Handle volume \u0026amp; iteration speed Can we do this 1000x? MLOps/Infrastructure Evaluation pipeline infrastructure L4: Metrics \u0026amp; Reliability Trust the measurements Is this signal real? Data Scientists, Leadership Aggregate metrics + confidence intervals L5: Production Monitoring Maintain quality in the wild Is it still working? SREs, Product Managers Live dashboards + alerting systems How Layers Map to Natural Workflow # RESEARCH PHASE (Offline Development) ‚îÇ ‚îú‚îÄ L1: Design Benchmarks ‚îÇ ‚îî‚îÄ \"What constitutes correct/good performance?\" ‚îÇ ‚îú‚îÄ L2: Run Evaluations ‚îÇ ‚îî‚îÄ \"Generate responses and score them\" ‚îÇ ‚îú‚îÄ L3: Scale Infrastructure ‚îÇ ‚îî‚îÄ \"Need to iterate fast ‚Üí evaluate 10K examples/day\" ‚îÇ ‚îî‚îÄ L4: Analyze Results ‚îî‚îÄ \"Aggregate metrics, validate reliability\" DEPLOYMENT PHASE (Online Production) ‚îÇ ‚îî‚îÄ L5: Monitor Production ‚îî‚îÄ \"Continuous validation, catch regressions\" ‚îî‚îÄ Feed failures back to L1 (closed loop) Different Stakeholders Own Each Layern # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ L1: Research Scientists ‚îÇ ‚îÇ ‚Üí Design evaluation protocols ‚îÇ ‚îÇ ‚Üí Define what \"good\" means for the domain ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ L2: ML Engineers ‚îÇ ‚îÇ ‚Üí Implement evaluation scripts ‚îÇ ‚îÇ ‚Üí Run model inference + scoring ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ L3: MLOps / Infrastructure Engineers ‚îÇ ‚îÇ ‚Üí Build scalable eval pipelines ‚îÇ ‚îÇ ‚Üí Manage compute resources ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ L4: Data Scientists / Research Leadership ‚îÇ ‚îÇ ‚Üí Statistical analysis of eval results ‚îÇ ‚îÇ ‚Üí Validate metric reliability ‚îÇ ‚îÇ ‚Üí Make deployment decisions ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ L5: SREs / Product Managers ‚îÇ ‚îÇ ‚Üí Monitor production performance ‚îÇ ‚îÇ ‚Üí Alert on regressions ‚îÇ ‚îÇ ‚Üí Coordinate incident response ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò The Spectrum of Verifiability # Fully Verifiable ‚Üê‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Üí Non-Verifiable ‚îÇ ‚îÇ ‚îÇ Code/Math Technical Writing Creative/Social (external (hybrid: facts + (judgment only) oracle) style/clarity) Examples by Category # Fully Verifiable Hybrid (Partially Verifiable) Non-Verifiable ‚Ä¢ Code execution ‚Ä¢ Technical writing (facts ‚úì, clarity ‚úó) ‚Ä¢ Creative writing ‚Ä¢ Math computation ‚Ä¢ Translation (accuracy ‚úì, style ‚úó) ‚Ä¢ Persuasive marketing ‚Ä¢ Logic proofs ‚Ä¢ Legal analysis (precedent ‚úì, judgment ‚úó) ‚Ä¢ Empathetic therapy ‚Ä¢ Data extraction ‚Ä¢ Medical diagnosis (tests ‚úì, manner ‚úó) ‚Ä¢ Humor generation ‚Ä¢ Fact checking ‚Ä¢ Recipe generation (chemistry ‚úì, taste ‚úó) ‚Ä¢ Art critique ‚Ä¢ Physics simulation ‚Ä¢ Code review (bugs ‚úì, readability ‚úó) ‚Ä¢ Storytelling LAYER 1: BENCHMARK DESIGN # Verifiable (Code/Math/Logic) Hybrid (Technical/Professional) Non-Verifiable (Creative/Social) Structure: Problem + Test Suite ‚Üí Automated Oracle Structure: Task + Mixed Criteria ‚Üí Automated + Human Structure: Prompt + Rubric ‚Üí Human/AI Judgment Key Benchmarks:\n‚Ä¢ HumanEval (164 code problems)\n‚Ä¢ MATH (12K competition problems)\n‚Ä¢ GPQA (448 PhD questions) Key Benchmarks:\n‚Ä¢ Technical Writing Quality\n‚Ä¢ Translation (BLEU + human eval)\n‚Ä¢ Medical Q\u0026amp;A (facts + empathy) Key Benchmarks:\n‚Ä¢ MT-Bench (80 conversations)\n‚Ä¢ AlpacaEval (805 prompts)\n‚Ä¢ Chatbot Arena (live voting) Properties:\n‚úÖ Tests are deterministic\n‚úÖ Can generate infinite variants\n‚úÖ No human disagreement Properties:\n‚ö†Ô∏è Some aspects objective\n‚ö†Ô∏è Some aspects subjective\n‚ö†Ô∏è Requires dual evaluation Properties:\n‚ùå Ratings are subjective\n‚ùå Context-dependent quality\n‚ùå Humans disagree frequently Creation Time: 1-2 weeks Creation Time: 2-3 weeks Creation Time: 3-4 weeks Example:\nassert reverse([1,2,3]) == [3,2,1] Example:\nAccuracy: Does translation preserve meaning? (‚úì)\nFluency: Does it sound natural? (human) Example:\n\u0026ldquo;Is this story engaging?\u0026quot;\n‚Üí 5-point Likert scale (human) LAYER 2: EVALUATION EXECUTION # Verifiable Hybrid Non-Verifiable Pipeline:\n1. Generate solutions (10 min)\n2. Execute \u0026amp; verify (5 min)\n3. Compute metrics (\u0026lt;1 min) Pipeline:\n1. Generate outputs (10 min)\n2. Automated checks (5 min)\n3. Human evaluation (4-20 hours)\n4. Combine scores (30 min) Pipeline:\n1. Generate responses (5 min)\n2. Human/AI rating (8-40 hours)\n3. Aggregate \u0026amp; validate (1 hour) Verification:\npython result = execute_code(solution)\u0026lt;br\u0026gt;label = PASS if tests_pass else FAIL\u0026lt;br\u0026gt; Verification:\npython # Automated component\u0026lt;br\u0026gt;facts_correct = verify_facts(output)\u0026lt;br\u0026gt;# Human component\u0026lt;br\u0026gt;clarity = human_rate_clarity(output)\u0026lt;br\u0026gt;score = 0.5*facts + 0.5*clarity\u0026lt;br\u0026gt; Verification:\npython ratings = get_human_ratings(n=3)\u0026lt;br\u0026gt;score = mean(ratings)\u0026lt;br\u0026gt;# Then validate inter-rater agreement\u0026lt;br\u0026gt; Throughput: 10K-100K evals/hour Throughput: 500-5K evals/hour Throughput: 100-1K evals/hour Cost per eval: $0.001-0.01 Cost per eval: $0.05-1.00 Cost per eval: $0.10-5.00 Human time: 0 hours Human time: 4-20 hours (partial) Human time: 24-120 hours (full) LAYER 3: SCALABILITY # Verifiable Hybrid Non-Verifiable Bottleneck: Compute (GPU time) Bottleneck: Human time for subjective parts Bottleneck: Human bandwidth Scaling Examples:\n‚Ä¢ 164 problems ‚Üí $2, 15 min\n‚Ä¢ 10K problems ‚Üí $122, 2 hours\n‚Ä¢ 100K problems ‚Üí $1,220, 8 hours Scaling Examples:\n‚Ä¢ 100 docs ‚Üí $50, 4 hours\n‚Ä¢ 1K docs ‚Üí $500, 20 hours\n‚Ä¢ 10K docs ‚Üí $5K, 200 hours\n(Mix of automated + human) Scaling Examples:\n‚Ä¢ 80 prompts ‚Üí $400, 8-40 hours\n‚Ä¢ 10K prompts ‚Üí $37,500, 2,500 hours\n‚Ä¢ (Or $8K hybrid with AI judges) Human scaling: 0 hours regardless of scale Human scaling: Sub-linear (automate what\u0026rsquo;s possible) Human scaling: Linear or super-linear Constraint: Money (buy more GPUs) Constraint: Time + money (human for quality checks) Constraint: Time (recruit, train raters) Automation: 99.9% Automation: 40-70% (depends on domain) Automation: 5-30% (AI judges need validation) Scalability Strategy:\n‚Ä¢ Parallelize across GPUs\n‚Ä¢ Generate synthetic test cases\n‚Ä¢ Cost scales linearly Scalability Strategy:\n‚Ä¢ Automate objective criteria\n‚Ä¢ Sample human evaluation (10-20%)\n‚Ä¢ Use AI judges for subjective parts (with validation) Scalability Strategy:\n‚Ä¢ Train reward models on human labels\n‚Ä¢ Use LLM-as-judge (must validate)\n‚Ä¢ Spot-check 5-10% with humans LAYER 4: METRICS \u0026amp; RELIABILITY # Verifiable Hybrid Non-Verifiable Metrics:\n‚Ä¢ pass@k (% solved in k tries)\n‚Ä¢ Compile rate\n‚Ä¢ Exact match accuracy\n‚Ä¢ Error tolerance Metrics:\n‚Ä¢ Factual accuracy (automated)\n‚Ä¢ Readability score (formula)\n‚Ä¢ Clarity rating (human)\n‚Ä¢ Combined weighted score Metrics:\n‚Ä¢ Likert scale (1-5 ratings)\n‚Ä¢ Win rate vs baseline\n‚Ä¢ Elo ratings (head-to-head)\n‚Ä¢ Thumbs up/down ratio Properties:\n‚úÖ Objective \u0026amp; reproducible\n‚úÖ Labs can compare directly\n‚úÖ No gaming (oracle is external)\n‚úÖ Leaderboards meaningful Properties:\n‚ö†Ô∏è Partially objective\n‚ö†Ô∏è Requires careful weighting\n‚ö†Ô∏è Some gaming risk on subjective parts\n‚ö†Ô∏è Need to report both auto + human metrics Properties:\n‚ùå Subjective \u0026amp; noisy\n‚ùå Different protocols ‚Üí incomparable\n‚ùå Gaming risk (optimize for judge)\n‚ùå Leaderboards have selection bias Inter-evaluator agreement: 100% Inter-evaluator agreement:\nFacts: 95-100%\nQuality: 70-85% Inter-evaluator agreement: 60-80% Example Metrics:\n‚Ä¢ HumanEval pass@1: 67.8%\n‚Ä¢ MATH accuracy: 82.3%\n‚Ä¢ Error rate: 5.2% Example Metrics:\n‚Ä¢ Translation BLEU: 45.2 (auto)\n‚Ä¢ Fluency: 4.1/5 (human)\n‚Ä¢ Medical accuracy: 94% (auto), Empathy: 3.8/5 (human) Example Metrics:\n‚Ä¢ MT-Bench: 7.9/10 (GPT-4 judge)\n‚Ä¢ Human preference: 78% win rate\n‚Ä¢ Elo rating: 1,245 LAYER 5: PRODUCTION MONITORING # Verifiable Hybrid Non-Verifiable Real-time signals:\n‚Ä¢ Does code compile? ‚úì/‚úó\n‚Ä¢ Tests pass? ‚úì/‚úó\n‚Ä¢ User accepted? ‚úì/‚úó\n‚Ä¢ Execution time OK? ‚úì/‚úó Real-time signals:\n‚Ä¢ Facts verified? ‚úì/‚úó\n‚Ä¢ Format correct? ‚úì/‚úó\n‚Ä¢ User satisfaction proxy (usage time)\n‚Ä¢ Error rate Proxy signals:\n‚Ä¢ Thumbs up/down ratio\n‚Ä¢ Session length\n‚Ä¢ Regeneration rate\n‚Ä¢ Response length Monitoring:\n‚Ä¢ Every request ‚Üí Automated check\n‚Ä¢ Dashboard updates: Real-time\n‚Ä¢ Regression alerts: Instant Monitoring:\n‚Ä¢ Automated checks: Real-time\n‚Ä¢ Human spot-checks: Weekly (10% sample)\n‚Ä¢ Combined quality score trending Monitoring:\n‚Ä¢ Sample 100 conversations/week\n‚Ä¢ 3 humans rate each\n‚Ä¢ Compare to last month Action:\n‚Ä¢ Compile rate \u0026lt;90% ‚Üí Auto rollback\n‚Ä¢ Pass@1 drops \u0026gt;5% ‚Üí Alert engineer Action:\n‚Ä¢ Fact accuracy \u0026lt;95% ‚Üí Auto rollback\n‚Ä¢ Quality score drops \u0026gt;0.3 ‚Üí Investigate\n‚Ä¢ Run deeper human eval if needed Action:\n‚Ä¢ Quality drops \u0026gt;0.3 ‚Üí Investigate\n‚Ä¢ A/B test for 7 days\n‚Ä¢ Need human eval to decide Human role: Only when alerts fire Human role: Weekly spot-checks (10%) Human role: Continuous (weekly audits) Dashboard Example:\nCompile Rate: 94.2% üü¢\nPass@1: 67.8% üü¢\nLatency: 1.2s üü° Dashboard Example:\nFact Check: 96.1% üü¢\nUser Rating: 4.2/5 üü¢\nClarity (sampled): 3.9/5 üü° Dashboard Example:\nHuman Rating: 4.2/5 üî¥\nThumbs Up: 78% üü¢\nSession Time: 8.2min üü¢ EACH LAYER HAS DIFFERENT BOTTLENECK # Layer Verifiable Bottleneck Hybrid Bottleneck Non-Verifiable Bottleneck L1: Design Writing test suite Defining which parts are verifiable Getting human agreement on rubric L2: Execute GPU inference time Human time for quality checks Human annotation time L3: Scale Compute budget Hiring raters for quality Hiring/training many raters L4: Metrics Statistical analysis Balancing auto vs human metrics Inter-rater reliability L5: Monitor Infrastructure cost Continuous spot-checking Continuous human auditing REAL-WORLD EXAMPLES BY CATEGORY # Verifiable:\n‚úÖ Code Generation (GitHub Copilot, Cursor) ‚Üí Unit tests verify correctness ‚Üí Compile rate is objective ‚úÖ Math Problem Solving (Khan Academy AI) ‚Üí Symbolic solver verifies answers ‚Üí Can generate infinite practice problems ‚úÖ Data Extraction (GPT-4 with function calling) ‚Üí Schema validation is deterministic ‚Üí JSON parsing either works or fails ``` Hybrid:\n‚ö†Ô∏è Medical Diagnosis Assistant ‚Üí Facts: Test results, drug interactions (verifiable) ‚Üí Quality: Bedside manner, explanation clarity (human eval) ‚ö†Ô∏è Legal Document Analysis ‚Üí Facts: Case precedents, statutes (verifiable) ‚Üí Quality: Argument strength, writing quality (human eval) ‚ö†Ô∏è Translation Systems ‚Üí Accuracy: BLEU score, term consistency (automated) ‚Üí Fluency: Natural phrasing, cultural adaptation (human eval) ``` Non-Verifiable:\n‚ùå Creative Writing (Claude, ChatGPT creative mode) ‚Üí \u0026#34;Is this story engaging?\u0026#34; ‚Üí Subjective ‚Üí No automated test possible ‚ùå Therapy Chatbots (Woebot, Replika) ‚Üí \u0026#34;Is this empathetic?\u0026#34; ‚Üí Cultural/personal ‚Üí Requires human evaluation ‚ùå Marketing Copy Generation ‚Üí \u0026#34;Is this persuasive?\u0026#34; ‚Üí Audience-dependent ‚Üí A/B testing required (slow, expensive) ``` The Three-Way Split: # Verifiable = Evaluation bottlenecked by compute budget\nFast iteration, predictable costs, objective metrics Future: Unlimited synthetic data generation Hybrid = Evaluation bottlenecked by smart automation + targeted human input\nMedium iteration speed, mixed costs, dual metrics Future: Better AI judges for subjective aspects Non-Verifiable = Evaluation bottlenecked by human labor availability\nSlow iteration, uncertain costs, noisy metrics Future: Constitutional AI, better preference learning Why This Matters: # This three-way framework explains why:\n‚úÖ Coding assistants improve faster than creative writing tools ‚úÖ Math tutors are more reliable than therapy chatbots ‚úÖ Technical Q\u0026amp;A is easier to align than open-ended conversation ‚ö†Ô∏è Medical AI needs dual evaluation (facts + empathy) ‚ö†Ô∏è Translation quality requires both automated + human metrics The future of AI alignment depends on:\nFor verifiable domains: More efficient compute For hybrid domains: Better decomposition of verifiable vs subjective aspects For non-verifiable domains: Creating reliable \u0026ldquo;oracles\u0026rdquo; (AI judges as good as compilers) "},{"id":75,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/foundatoin_model/","title":"Foundation Models for Healthcare","section":"C3 ML Healthcare","content":" Foundation Models for Healthcare # What are Foundation Models? # Foundation models are trained on massive amounts of unlabeled data using self-supervised or unsupervised learning. They are \u0026ldquo;foundational\u0026rdquo; because they can be adapted to multiple downstream tasks with high efficiency and minimal data. They demonstrate sample efficiency and can handle multiple modalities like text, images, genomics, etc. Few-Shot vs. Zero-Shot Learning # Few-Shot Learning: Learns from just a few labeled examples per class and generalizes to new examples. Zero-Shot Learning: Learns to perform tasks it hasn‚Äôt seen in training, relying on general knowledge from pretraining. ‚û°Ô∏è These abilities allow foundation models to generalize efficiently across healthcare tasks, even with limited supervision.\nWhat kind of data powers these models? # Foundation models are trained on multi-modal health data:\nText: Clinical notes, EHRs, literature Images: X-rays, MRIs, CTs Sequences: Genomics, proteomics Graphs: Molecular structures Time Series: ECGs, continuous monitoring Video: Ultrasound Who provides this data? # Hospitals, pharma, insurance payers, academic researchers, patients (via wearables), and public forums. Downstream Use Cases # For providers: Diagnosis, treatment planning, trial recruitment, drug discovery. For patients: QA, health education, personalized guidance, assistive care. Foundation models serve as AI interfaces to improve decision-making and patient engagement.\nWhy foundation models are timely now # Human systems evolve linearly, but technology is exponential. Data is growing rapidly, e.g.: 1950s: Data doubled every 50 years. 2020s: Every 73 days. Healthcare data exploded from 150 EB (2013) ‚Üí over 2,000 EB (2020). The Chessboard Paradox # A grain of rice doubled per square = \u0026gt;9B grains by square 64. Shows how exponential growth is counterintuitive to humans. Compute acceleration # Moore‚Äôs Law + AI accelerators (e.g., GPUs) made it feasible to train large foundation models. ‚û°Ô∏è These forces combine to make now the critical window to apply foundation models in healthcare.\nNarrow vs. General AI # Narrow AI: Performs one task; static. General AI: Learns multiple tasks and evolves over time. Foundation models aim for General AI characteristics. Emergent Behaviors # Large-scale models exhibit behaviors not explicitly programmed. Example: Google‚Äôs PaLM: 8B: Basic QA \u0026amp; language understanding 62B: Summarization, code completion 540B: Common-sense reasoning, joke explanation, logic chaining Risk: Hallucination # Model may generate confident but false outputs (e.g., imaging results that don‚Äôt exist). Needs human oversight for reliability. Transformer Components # Self-attention: Captures relationships between all tokens. Encoder: Converts input tokens into vector embeddings. Decoder: Generates output from internal representation. RLHF: Reinforcement Learning with Human Feedback # Step 1: Train a supervised model on human examples. Step 2: Collect human preferences to train a reward model. Step 3: Fine-tune the language model using reinforcement learning (PPO). This aligns model outputs with human intent and preferences.\nWhat is Prompt Engineering? # Crafting inputs to steer output behavior of foundation models. Prompt Types # Simple instructions Role-based prompts Few-shot examples Chain-of-Thought (CoT) Zero-shot-CoT (\u0026ldquo;Let\u0026rsquo;s think step by step.\u0026rdquo;) Self-consistency (multiple CoTs, pick majority) Generative knowledge prep (generate before answering) Text-Based Applications in Healthcare # Appointment scheduling Inbox management Chart summarization Trial eligibility Decision support Medical QA and patient communication ‚û°Ô∏è These tools reduce burnout and support both provider productivity and patient engagement.\nModalities Beyond Text # Imaging (X-rays, CTs) Genomics/proteomics Signal data (ECG) VATT-like models process multiple data types in a unified transformer architecture. Do Foundation Models ‚ÄúUnderstand‚Äù Imaging? # They can generate plausible results, but: Miss clinical context Can\u0026rsquo;t compare time-series or integrate history like radiologists Imaging as a Biomarker Source # CT @ L3 can yield: Fat/muscle measurements Aortic calcification Organomegaly Predictive health markers Foundation models unlock quantitative phenotyping from visual data.\nClinical Readiness Gaps # 83% clinicians want AI in training 70% feel overwhelmed by new tech Academia vs. Industry # Industry dominates in compute + data Collaboration is essential for clinical relevance \u0026amp; ethical development Model Drift Risks # Data Drift: Input data distribution changes Model Drift: Degrading performance over time Deployment Best Practices # Monitor performance regularly Update models with new data Ensure data quality Audit for fairness and bias Collaborate across sectors ‚û°Ô∏è Treat foundation models like medical devices ‚Äî with continuous monitoring, recalibration, and governance.\nüìö Additional Readings # Attention Is All You Need The Illustrated Transformer The Annotated Transformer Opportunities and Risks of Foundation Models (CRFM) Shifting ML for Healthcare ‚Äì Nature Biomedical Engineering "},{"id":76,"href":"/ai-workflows/reasoning/graph-reasoning/","title":"Graph Reasoning","section":"Reasoning","content":" Graph Reasoning: Knowledge Graph (KG) vs. GraphRAG Knowledge Graph (Structure) ‚ûï LLM (Reasoning) ‚û°Ô∏è GraphRAG (Hybrid QA System) üîÑ Relationship Between KG and GraphRAG # A Knowledge Graph can serve as the data backbone for GraphRAG. KG provides the structured facts, GraphRAG adds flexible retrieval and LLM reasoning. GraphRAG extracts subgraphs, embeds them, and uses LLMs to generate natural language answers. LLMs can also help enrich the KG, creating a feedback loop. GraphRAG is essentially a modern hybrid system, combining symbolic structure with neural flexibility. Side-by-Side Comparison # Feature KG GraphRAG üß© Core Idea Graph-based representation of entities and relationships Retrieval-Augmented Generation with a graph-structured retrieval backend üèóÔ∏è Structure Nodes (entities/concepts) + edges (relations) Combines a graph + retriever + LLM (generator) üîç Primary Use Case Semantic search, reasoning, data integration, and explainable AI Answering complex queries with structured reasoning + natural language generation üß† Reasoning Type Symbolic / rule-based / graph traversal Hybrid: retrieval + neural reasoning over graph paths üßÆ Tech Stack RDF, OWL, Neo4j, Blazegraph, SPARQL LangChain, LlamaIndex, HuggingFace Transformers, Neo4j/NetworkX for graph, vector DBs üß™ Inference Deterministic (SPARQL, rules, logic) or probabilistic (PGMs) LLM-based generation informed by graph-aware retrieval üîó Integration with LLMs Optional; LLMs can query or summarize KG Essential; LLMs decode retrieved graph info into answers üìò Example in Healthcare \u0026ldquo;What drugs interact with Warfarin?\u0026rdquo; ‚Üí Uses drug-drug interaction KG \u0026ldquo;What treatments are best for elderly diabetic patients with hypertension?\u0026rdquo; ‚Üí Uses patient-condition-treatment graph to guide LLM üìà Scalability Can grow large but needs curation and consistency Scalable via modular retrievers; dynamic context injection üì£ Explainability High: paths are interpretable Medium: explainable only if LLM is instructed to reason with trace üìö Data Format Triples: (subject, predicate, object) Graph-augmented documents, vector embeddings, node-context pairs üéØ Strengths Precision, transparency, semantic integrity Flexibility, context-aware QA, natural language synthesis üß± Weaknesses Hard to build/maintain at scale, brittle for unstructured text Less structured, can hallucinate, graph reasoning quality depends on retriever design ü©∫ Healthcare Use Cases # Use Case KG GraphRAG 1. Drug Interaction Checks KG connects drugs via known interaction relationships from structured databases (e.g., RxNorm, DrugBank).\nüîπ \u0026ldquo;Does Warfarin interact with NSAIDs?\u0026rdquo; ‚Üí Traverse KG paths. GraphRAG retrieves documents discussing drug interactions, side effects, or contraindications and summarizes them.\nüîπ \u0026ldquo;What should patients taking Warfarin avoid?\u0026rdquo; 2. Clinical Decision Support Encodes clinical guidelines as rules and semantic paths (e.g., \u0026ldquo;If diabetic AND hypertensive THEN consider ACE inhibitors\u0026rdquo;). Retrieves relevant chunks of guidelines and case studies, then LLM synthesizes a tailored answer.\nüîπ \u0026ldquo;Best treatment plans for elderly diabetic patients with kidney disease?\u0026rdquo; 3. Patient Phenotyping Uses ontologies (e.g., SNOMED CT, HPO) to infer phenotypes based on coded EHR data.\nüîπ Identify \u0026ldquo;Type 2 Diabetes\u0026rdquo; from a network of symptoms and lab values. Retrieves semantically similar patient trajectories or phenotypes, helping answer: üîπ \u0026ldquo;How was this phenotype managed in similar patients?\u0026rdquo; 4. Rare Disease Diagnosis Graph-based inference across symptoms, genes, and conditions to suggest candidate diseases. Combines graph paths with medical literature to support LLM-based diagnostics and explanations.\nüîπ \u0026ldquo;What rare diseases match these symptoms?\u0026rdquo; 5. Biomedical Research Discovery Connects genes, diseases, pathways, and drugs to suggest new hypotheses.\nüîπ \u0026ldquo;Which genes are linked to both Parkinson‚Äôs and depression?\u0026rdquo; Retrieves multi-hop literature paths and generates natural language hypotheses.\nüîπ \u0026ldquo;What is the link between gut microbiome and Alzheimer‚Äôs?\u0026rdquo; 6. Clinical Trial Matching Links patient features to trial eligibility criteria through structured relationships. Matches unstructured patient notes with trials via hybrid graph + text retrieval.\nüîπ \u0026ldquo;Which clinical trials is this patient eligible for?\u0026rdquo; 7. Medical Education / Q\u0026amp;A Students query a structured KG to explore medical knowledge interactively. Natural language Q\u0026amp;A system over combined textbook + graph data.\nüîπ \u0026ldquo;Explain why beta-blockers are contraindicated in asthma patients.\u0026rdquo; "},{"id":77,"href":"/ai-workflows/reasoning/graph-reasoning/graphrag/","title":"GraphRAG","section":"Graph Reasoning","content":" GraphRAG "},{"id":78,"href":"/healthcare/domain_knowledge/hands-on-healthcare-data/","title":"Hands-On Healthcare Data","section":"Domain","content":" Hands-On Healthcare Data: Taming the Complexity of Real-World Data Book: https://www.oreilly.com/library/view/hands-on-healthcare-data/9781098112912/ Code: https://gitlab.com/hands-on-healthcare-data Chapter Summary 1. Introduction to Healthcare Data Overview of data types (EHR, claims, registries, trials) and enterprise challenges. 2. Technical Introduction Covers Docker, database systems, and data architecture. 3. Standardized Vocabularies in Healthcare Discusses vocabularies like SNOMED, ICD, UMLS and their usage. 4. Deep Dive: Electronic Health Records Data Explores MIMIC, Synthea, and normalization issues in EHRs. Summary 5. Deep Dive: Claims Data Analysis of claims datasets like SynPUF and integration strategies. 6. Machine Learning and Analytics Feature engineering, graph-based ML, embeddings, SQL vs. graph ops. Summary 7. Trends in Healthcare Analytics Federated learning, NLP in healthcare, and harmonization trends. 8. Harmonization, and Final Thoughts RWD diversity, business-technical gap, graph limitations. "},{"id":79,"href":"/healthcare/data/healthcare_data/","title":"Healthcare Data Layers","section":"Data","content":" Healthcare Data Layers 1Ô∏è‚É£ Data Sources (Raw Data \u0026amp; Collection Level) # These are the foundational data sources used in healthcare analysis, originating from clinical trials, hospitals, insurance claims, and patient records.\nClinical Data (RCTs, EHR, OMOP, CDM) ‚Äì Structured, controlled, and often randomized data used for regulatory and research applications. Real-World Data (RWD: EHR, Claims, Registries) ‚Äì Observational and confounded, requiring advanced causal inference methods to extract meaningful insights. Relationship: Clinical Data is typically highly structured and standardized, whereas RWD is heterogeneous, requiring bias correction. 2Ô∏è‚É£ Data Management \u0026amp; Standardization (Processing \u0026amp; Infrastructure Level) # This layer ensures that raw clinical \u0026amp; real-world data are cleaned, structured, and made interoperable for analysis.\nHealthcare Informatics ‚Äì The framework for data integration, ETL processes, standardization (OMOP, FHIR, CDMs), interoperability, and terminology mapping (SNOMED, LOINC, ICD). Healthcare Informatics acts as a bridge between data collection (clinical \u0026amp; RWD) and analytics. Without informatics, AI models and statistical analyses would lack clean, structured, and standardized data. 3Ô∏è‚É£ Data Analytics \u0026amp; Decision Intelligence (AI \u0026amp; Statistical Analysis Level) # This layer applies statistical, machine learning (ML), and deep learning (DL) models to structured and unstructured healthcare data for actionable insights.\nTraditional Data Science \u0026amp; Statistical Analysis (Used for both Clinical \u0026amp; RWD)\nBiostatistics, Bayesian Methods, Survival Analysis, Causal Inference (PSM, DAGs, DiD) Used to control bias, estimate treatment effects, and generate regulatory-grade evidence (RWE). AI in Healthcare (Machine Learning \u0026amp; Deep Learning Applications)\nSupervised Learning (Logistic Regression, Decision Trees, Random Forests) Deep Learning (CNNs, Transformers, NLP, Reinforcement Learning) Model Interpretability (SHAP, LIME) and AI Fairness (Bias Mitigation) Relationship:\nAI \u0026amp; ML rely on structured, clean data (from Healthcare Informatics) and leverage Clinical Data \u0026amp; RWD to generate predictions and automate decision-making. Statistical analysis methods (causal inference, survival analysis) are critical for ensuring valid results before AI is applied. "},{"id":80,"href":"/healthcare/data/healthcare_sources/","title":"Healthcare Data Sources","section":"Data","content":" Healthcare Data Sources Phenotype KnowledgeBase (PheKB) # Description:\nA collaborative portal for sharing and validating electronic phenotype definitions used in observational health research.\nTags: phenotyping, EHR, cohort definitions\nUse Cases:\nStandardized phenotype definitions for conditions like diabetes, asthma, etc. Sharing phenotype algorithms across institutions MIMIC-IV (Medical Information Mart for Intensive Care) # Description:\nA large, publicly available critical care database containing de-identified health data from ICU patients at the Beth Israel Deaconess Medical Center.\nTags: ICU, de-identified data, clinical research\nUse Cases:\nPredictive modeling in critical care Benchmarking clinical algorithms Training deep learning models Access Requirements:\nRequires credentialed training and data use agreement via PhysioNet\nOHDSI / OMOP Common Data Model # Description:\nAn open community initiative and standard model for organizing observational health data across institutions and studies.\nTags: standardization, EHR, interoperability, CDM\nUse Cases:\nConverting disparate data sources into a consistent format Enabling federated analysis across healthcare systems Supporting tools like ATLAS for cohort building National COVID Cohort Collaborative (N3C) # Description:\nA centralized, secure platform for analyzing harmonized COVID-19 clinical data from dozens of healthcare providers across the US.\nTags: COVID-19, federated research, clinical data\nUse Cases:\nStudying disease trajectories and treatment effects Multisite analytics using harmonized EHR data Evaluating outcomes for long COVID Access Requirements:\nApplication and institutional affiliation required\nBioPortal # Description:\nA comprehensive repository of biomedical ontologies from the National Center for Biomedical Ontology.\nTags: ontologies, terminology, semantic web, linked data\nUse Cases:\nAccessing ontologies like SNOMED CT, ICD, LOINC, RxNorm Mapping data to standard vocabularies Enabling semantic interoperability Unified Medical Language System (UMLS) # Description:\nIntegrates over 200 biomedical vocabularies to support natural language processing, terminology mapping, and EHR data harmonization.\nTags: NLP, standard vocabularies, concept mapping\nUse Cases:\nLinking clinical terms to standard codes Enhancing search and retrieval in clinical systems Supporting NLP tools like MetaMap and cTAKES Access Requirements:\nFree license from NLM, requires annual agreement\nAphrodite # Description:\nAn R package developed by OHDSI that supports semi-supervised phenotype algorithm development using feature engineering and machine learning methods on OMOP Common Data Model (CDM) datasets.\nTags: phenotyping, machine learning, semi-supervised, OMOP, OHDSI\nUse Cases:\nRapid development of phenotype classifiers using imperfectly labeled data. Applying machine learning models to predict phenotypes based on structured EHR data. Feature extraction from OMOP CDM to support supervised or semi-supervised learning tasks. "},{"id":81,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/healthcare_use_cases_for_non_textual_unstructured_data/","title":"Healthcare Use Cases for Non-textual Unstructured Data","section":"C3 ML Healthcare","content":" Healthcare Use Cases for Non-textual Unstructured Data # 1. Are foundation models limited to text? # No. Although many early applications focus on text, foundation models are inherently multimodal. They can process and learn from images, audio, video, and other forms of unstructured data without modifying the core model.\n2. What kind of unstructured data exists in healthcare beyond text? # Medical images: X-rays, CT, MRI Audio data: Patient speech, clinical voice notes Video data: Endoscopy, movement assessments Digital pathology and genomic data: High-resolution slides and sequence strings 3. How do image-text foundation models work in healthcare? # They learn from paired image and text data (e.g., medical images and associated reports) to understand content holistically. This allows for better diagnostic performance and contextual understanding than single-modality models.\n4. What is the potential impact of image-text models on radiology? # Radiology is AI-friendly due to digital formats and standards. Current AI tools support quantification, detection, classification, etc. Foundation models go beyond‚Äîintegrating prior exams, clinical context, and treatment recommendations. 5. Why is the current model development approach inefficient? # Today‚Äôs approach builds narrow, task-specific models requiring large labeled datasets. Foundation models reduce this burden via few-shot learning, general knowledge transfer, and multimodal reasoning.\n6. What advantages do foundation models bring to medical imaging? # Combine image + text for deeper insight Reduce false positives/negatives via context Extract insights beyond human interpretation Generate full radiology reports or treatment suggestions Accelerate model development with less labeled data 7. How are foundation models used in image-based diagnosis? # They can:\nUnderstand body composition via imaging biomarkers Detect osteoporosis, aortic calcium, visceral fat, etc. Predict adverse events from routine imaging 8. Can foundation models incorporate genomic or pathology data? # Yes. They can process complex biomedical data:\nDigital pathology slides Genomic sequences in FASTA format Gene expression and mutation patterns This enables discovery of clinically meaningful patterns across modalities.\n9. What is the value of multimodal foundation models? # Integrate text, imaging, genomics, clinical history Offer personalized care and richer diagnostics Support communication with patients in any language or literacy level 10. How do foundation models help in image model development? # They support:\nPreprocessing Data augmentation Synthetic data generation This accelerates model iteration and reduces time to deployment.\n11. Can voice-text data be used in healthcare applications? # Yes. Foundation models can:\nTranscribe speech Analyze speech patterns for diagnosis (e.g., Parkinson‚Äôs) Enable voice prosthetics for patients who‚Äôve lost speech Support virtual medical assistants and mental health chatbots 12. What are some patient-facing applications of voice-text models? # Mental health bots using speech/text input Virtual assistants for disabled individuals Real-time transcription and communication aids 13. What cognitive shift do foundation models support? # They help users move from:\nComprehension-based reasoning ‚Üí using known knowledge Fluid reasoning ‚Üí solving unfamiliar problems with abstracted understanding Foundation models act as co-pilots, enhancing human decision-making.\n14. Do foundation models replace human decision-making? # No. They augment, not replace. Final decisions still rest with trained professionals who interpret the model‚Äôs output with judgment and context.\n15. What‚Äôs the catch with all this power? # Even with their vast capabilities, foundation models are subject to the no free lunch theorem. They have trade-offs, biases, and limitations‚Äîtopics discussed in follow-up modules.\n"},{"id":82,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/healthcare_use_cases_for_text_data/","title":"Healthcare Use Cases for Text Data","section":"C3 ML Healthcare","content":" Healthcare Use Cases for Text Data # 1. Can large language models like ChatGPT perform at a physician level? # Yes. ChatGPT has demonstrated performance comparable to expert physicians on tasks like the USMLE medical board exam. This raises important questions about the evolving role of human expertise in healthcare as LLMs continue to advance.\n2. Should LLMs be integrated into medical training or exams? # Possibly. LLMs could enhance the medical licensing exam process by reflecting real-world clinical scenarios. However, it\u0026rsquo;s essential for healthcare professionals to understand their benefits and limitations before full integration.\n3. What are the risks of integrating LLMs into clinical systems? # While useful for tasks like recipe generation or patient education, LLMs can fail unexpectedly, hallucinate references, or output incorrect information. Human oversight and validation remain critical.\n4. What are practical healthcare use cases for LLMs today? # Clerical task automation: Scheduling, patient communication, and triaging. Inbox management: Reducing message overload and provider burnout. Collaborative assistant: Recommending actions based on patient history. Low-code innovation: Empowering clinicians to build apps/tools. 5. How do LLMs support medical data processing? # LLMs streamline key NLP tasks:\nTokenization: Segmenting clinical notes into analyzable units. Named Entity Recognition (NER): Identifying drugs, diseases, etc. Negation Detection: Understanding sentiment/context (e.g., \u0026ldquo;no cancer\u0026rdquo;). Relation Extraction: Mapping relationships between entities. De-identification: Masking PHI for privacy compliance. 6. Why are foundation models better than traditional NLP for medical text? # They handle variation across institutions, formats, and languages with few-shot/zero-shot learning, reducing the need for custom engineering and enabling broader generalization.\n7. Can LLMs handle complex clinical queries without structured data? # Yes. Prompts like:\n‚ÄúFind all named entities related to diabetes management‚Äù ‚ÄúDe-identify this record per HIPAA‚Äù ‚ÄúHow has cancer progressed after Keytruda treatment?‚Äù show how LLMs can perform analytics directly from unstructured text. 8. Can LLMs be further trained on clinical data? # Yes. Training LLMs on patient records, trials, and guidelines can increase domain-specific accuracy. Applications include:\nClinical Decision Support Drug Interaction Warnings Guideline Recommendations 9. How can LLMs help with clinical trial recruitment? # LLMs can evaluate eligibility based on:\nPatient history Medications Lab results They can also explain trials directly to patients, improving enrollment.\n10. What role can LLMs play in patient communication? # They can:\nAnswer health-related questions Translate jargon into plain language Provide reminders and follow-ups Offer multilingual, conversational support 11. How can LLMs assist with billing and coding? # With medical terminology knowledge, LLMs can:\nAssign billing codes Improve record-keeping Reduce administrative burden 12. Can LLMs support public health efforts? # Yes. They can monitor:\nOutbreak detection using EHRs, social media Pattern recognition across data sources This enables faster responses to public health threats. 13. Can LLMs process and learn from genomic data? # Yes. Genomic data (e.g., FASTA format) is text-based. LLMs can:\nIdentify mutations linked to diseases Predict disease risk Integrate with clinical and lifestyle data 14. What is the benefit of multimodal analysis in genomics? # LLMs can combine:\nGenomic sequences EHRs Environmental/lifestyle data This integration enables personalized care and discovery of complex health patterns.\n15. Can LLMs support pharmacogenomics? # Yes. They can identify:\nDrug responses Adverse reactions Genetic factors impacting efficacy This paves the way for precision medicine.\n16. How do LLMs improve drug discovery? # Applications include:\nVirtual screening: Identify promising molecules Lead optimization: Improve safety and effectiveness Toxicity prediction: Flag unsafe compounds early Mechanism of action prediction: Understand how a drug works 17. What is the long-term outlook for LLMs in healthcare? # The future is expansive:\nFrom analytics and operations to clinical care and research Support for providers, patients, researchers Accelerating breakthroughs in drug development and personalized medicine LLMs are set to revolutionize the healthcare landscape, and we are only scratching the surface.\n"},{"id":83,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/how_foundation_models_work_in_healthcare_applications/","title":"How Foundation Models Work","section":"C3 ML Healthcare","content":" How Foundation Models Work (in Healthcare Applications) # 1. What are foundation models and why are large language models (LLMs) a key example? # Foundation models are large-scale machine learning models trained on vast and diverse datasets, enabling them to perform well across multiple tasks. Large Language Models (LLMs) like GPT are a major class of these, powering tools like ChatGPT and being early, widely successful examples.\n2. What architecture do foundation models, especially LLMs, rely on? # They rely heavily on the Transformer architecture, introduced in the paper ‚ÄúAttention Is All You Need‚Äù. This architecture uses self-attention mechanisms to allow models to learn relationships across sequences of data, making them highly effective for tasks involving language and other sequential data.\n3. How does the attention mechanism work in transformers? # The attention mechanism enables the model to focus on different parts of the input when making predictions. Self-attention lets each element of the input sequence attend to all others, allowing the model to integrate contextual information holistically.\n4. How are transformers structured in large language models? # Transformers in LLMs are deep neural networks built from stacks of layers combining attention with traditional neural layers. They process input as sequences‚Äîideal for language data‚Äîand use tokenization to convert text into manageable units (words, subwords, etc.).\n5. Can tokenization be applied to other data types? # Yes. Tokenization can be generalized to image patches, audio frames, or structured data. This generalization enables foundation models to learn from and operate across multiple modalities.\n6. What are transformer encoders and how are they used in models like BERT? # A transformer encoder transforms input sequences into dense vector representations. BERT, for example, is pretrained using self-supervised learning by masking words and predicting them, allowing it to learn language representations without labeled data.\n7. What about transformer decoders and models like GPT? # Transformer decoders, unlike encoders, generate sequences from input vectors or initial prompts. GPT (Generative Pre-trained Transformer) models are based on decoder-only architectures and are trained to predict the next token in a sequence‚Äîa task aligned with language generation.\n8. How do models like GPT-3 perform impressive tasks with little supervision? # Thanks to transfer learning, GPT models demonstrate zero-shot and few-shot learning, performing tasks they weren\u0026rsquo;t explicitly trained for by using prompts and examples within the input.\n9. How does GPT handle complex tasks like medical Q\u0026amp;A or clinical reasoning? # LLMs can respond to medical questions, explain reasoning for clinical answers, and even alter case scenarios to change correct answers. This shows their potential for medical education and diagnostic simulation.\n10. What are the limitations of LLMs trained like GPT-3? # Since they‚Äôre trained on general web and literature data, LLMs may generate plausible but incorrect outputs, due to misalignment with human values or domain knowledge. Their training objective (next-word prediction) does not inherently align with truth or utility.\n11. How does ChatGPT improve upon this with human feedback? # ChatGPT uses Reinforcement Learning from Human Feedback (RLHF):\nStep 1: Supervised Fine-Tuning (SFT) with human-written responses. Step 2: Reward Model trained from human-ranked outputs. Step 3: Reinforcement learning (e.g., PPO) optimizes the model to favor preferred responses. 12. What is a ‚Äúprompt‚Äù and why does it matter? # A prompt is the input that guides the model\u0026rsquo;s output. Its structure and content influence results dramatically, making prompt engineering a critical skill for getting reliable, targeted responses from foundation models.\n13. What are some common prompt engineering styles? # Instruction prompt: Direct task requests (e.g., ‚ÄúHow to manage diabetes?‚Äù) Role-based prompt: Assigning the model a persona (e.g., ‚ÄúYou are a nurse‚Ä¶‚Äù) Few-shot prompt: Providing examples before the actual question Chain-of-thought (CoT): Encouraging the model to reason step-by-step Zero-shot CoT: Adding phrases like ‚ÄúLet‚Äôs think step-by-step‚Äù to guide reasoning Self-consistency prompting: Generating multiple answers and choosing the majority Generative Knowledge prompting: First generating facts, then reasoning over them 14. Why is chain-of-thought prompting effective, and what are its limits? # CoT prompting helps elicit reasoned answers, particularly in clinical contexts. However, it‚Äôs most effective in larger models (100B+ parameters) and less so in smaller ones.\n15. Do foundation models apply beyond language? # Yes. The transformer architecture extends to:\nImages: via patch tokenization (e.g., Vision Transformers) Audio/Speech: like OpenAI‚Äôs Whisper for transcription Multimodal data: like DALL¬∑E 2 for text-to-image generation 16. How does a multimodal model like DALL¬∑E 2 work? # DALL¬∑E 2:\nEncodes a text prompt into a vector. Maps it to visual feature space. Uses a diffusion model decoder to generate images from that representation. 17. What are diffusion models and how do they help? # Diffusion models learn to denoise images progressively, allowing them to generate high-quality, realistic outputs. They‚Äôre widely used in modern generative vision models like DALL¬∑E 2.\n18. What\u0026rsquo;s the future outlook for foundation models in healthcare? # Foundation models, particularly LLMs and multimodal transformers, are rapidly evolving. With human feedback, prompt engineering, and domain-specific fine-tuning, they offer immense potential for clinical decision support, medical education, and personalized care.\n"},{"id":84,"href":"/ai-workflows/genai/bert_cls_classification_summary/","title":"How to Use BERT's CLS Token for Classification","section":"GenAI","content":" How to Use BERT's CLS Token for Classification ‚ùì Question # How can we use the [CLS] token (i.e., h_cls) from the last layer of BERT for classification tasks? Given that the BERT output has shape [batch_size, sequence_length, hidden_size], how is it valid to pass only [batch_size, hidden_size] to a nn.Linear(hidden_size, num_classes) without flattening the sequence? And why don\u0026rsquo;t we flatten the whole sequence ‚Äî wouldn\u0026rsquo;t that destroy order?\n‚úÖ Answer # üîπ BERT Output and the [CLS] Token # BERT outputs a tensor of shape:\n[batch_size, sequence_length, hidden_size] But for classification tasks, we typically use only the [CLS] token, which is located at position 0 in the sequence:\nh_cls = outputs.last_hidden_state[:, 0, :] # Shape: [batch_size, hidden_size] This token is designed to act as a summary representation of the entire sequence, and this output shape matches exactly what a nn.Linear(hidden_size, num_classes) expects ‚Äî no flattening needed.\nüîπ Why Not Flatten? # Flattening the whole sequence (e.g., [batch_size, sequence_length * hidden_size]) loses:\nToken order Positional embeddings Sequence structure In NLP, this breaks the semantic and syntactic structure of the input. Instead, use:\nüî∏ Recommended Pooling Strategies # Strategy Description [CLS] Token Use outputs[:, 0, :]; trained as a sequence summary Mean Pooling outputs.mean(dim=1); averages token embeddings Max Pooling outputs.max(dim=1).values; takes strongest signal Attention Pooling Learns weights to summarize tokens adaptively üìö Sources and Justification # BERT Paper: Devlin et al. (2018) ‚Äî [CLS] token for classification Sentence-BERT: Reimers \u0026amp; Gurevych (2019) ‚Äî Mean pooling often better for embeddings Hugging Face Transformers: Practical implementation patterns NLP Community Practices: Kaggle, blogs, and tutorials üß™ Summary # Use [CLS] or pooling (not flattening) for sequence-level tasks. Flattening destroys sequence information and is rarely appropriate in NLP. The linear layer works on [batch_size, hidden_size] ‚Äî no need to flatten across tokens. "},{"id":85,"href":"/ipark/","title":"Inhee Park, PhD - Resume","section":"","content":" Inhee Park - Resume "},{"id":86,"href":"/ai-workflows/genai/multimodel_llms/data_prep/","title":"Inputs and Data Preparation for Multimodal LLMs","section":"Multimodal LLMs","content":" Inputs and Data Preparation for Multimodal LLMs Multimodal LLMs are language models that can process and reason over multiple data types, especially:\nText Images (Optionally: audio, video, or other modalities) They are designed to understand both visual and linguistic context, enabling tasks like visual question answering, image captioning, grounding, and perception-based reasoning.\nüñºÔ∏è + üí¨ Input Format # Inputs typically include:\nImage(s): RGB images, optionally annotated (e.g., bounding boxes, circles) Text Prompt: Task instruction or question (e.g., \u0026ldquo;Which object is closer?\u0026rdquo;) Answer Choices (optional): For classification-style tasks like BLINK inputs = { \u0026#34;images\u0026#34;: [...], # preprocessed (resized, normalized) tensors or raw image paths \u0026#34;text\u0026#34;: \u0026#34;Which point is closer to the camera? (A) A (B) B\u0026#34; } Some APIs accept JSON-style mixed prompts with interleaved text and image tokens.\nData Preparation Pipeline # Image Collection\nUse open datasets (COCO, LVIS, IIW, WikiArt) or your own; resize consistently (e.g., 224x224 or 1024px).\nVisual Prompt Annotation\nAdd circles (keypoints), boxes (objects), or masks (regions) using tools like OpenCV, CVAT, or FiftyOne.\nText Prompt Design\nWrite clear, natural or templated questions.\ne.g., \u0026ldquo;Which image completes the jigsaw?\u0026rdquo; e.g., \u0026ldquo;Is the laptop to the left of the bear?\u0026rdquo; Label Encoding\nClassification: (A), (B), (C), (D) Generation: Free-text string Evaluation: Ground-truth match or similarity Example Entry (BLINK-style) # { \u0026#34;image_1\u0026#34;: \u0026#34;img001.jpg\u0026#34;, \u0026#34;image_2\u0026#34;: \u0026#34;img002.jpg\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Which point corresponds to the reference point (REF)? (A) A (B) B (C) C (D) D\u0026#34;, \u0026#34;visual_prompts\u0026#34;: { \u0026#34;ref_point\u0026#34;: [x1, y1], \u0026#34;candidates\u0026#34;: [[x2, y2], [x3, y3], [x4, y4], [x5, y5]] }, \u0026#34;answer\u0026#34;: \u0026#34;C\u0026#34; } Use Cases # Visual Question Answering (VQA) Visual Grounding \u0026amp; Alignment Perception-based Evaluation (e.g., BLINK) Medical Image Reasoning Image Captioning / Retrieval "},{"id":87,"href":"/ai-workflows/reasoning/graph-reasoning/knowledge-graphs/","title":"Knowledge Graphs","section":"Graph Reasoning","content":" Knowledge Graphs Neo4j GDS (Graph Data Science) vs. Core Neo4j (Cypher) "},{"id":88,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/missing_values/","title":"Missing Data Scenarios in Healthcare Modeling","section":"C2 Clinical Data","content":" Missing Data Scenarios in Healthcare Modeling # 1. Should Be Measured But Wasn‚Äôt # Description: The value is expected but is missing due to random or procedural issues (e.g., lab error, missed test). Technical Term: MCAR: Missing Completely At Random MAR: Missing At Random Example: A routine blood test wasn\u0026rsquo;t recorded because the sample was lost. Strategy: Impute (mean, median, or model-based). Add a missingness indicator variable (e.g., var_missing = 1). Rationale: The missingness is unrelated to the value itself, so estimation is relatively safe. 2. Mostly Zero Due to Rare Occurrence # Description: Not truly missing ‚Äî the value is zero or absent for most patients because the condition/event is rare. Technical Term: Not Missing (No abbreviation needed) Example: HIV diagnosis column is 0 for most patients. Strategy: Do not impute ‚Äî the 0s are meaningful and reflect true absence. Rationale: These are real values, and zeros carry clinical meaning. 3. Deliberately Not Recorded # Description: Clinician or system chooses not to record a value based on context (e.g., patient clearly stable or too ill). Technical Term: MNAR: Missing Not At Random Example: Sodium level not tested because the patient was clearly stable. Strategy: Avoid imputation if possible ‚Äî it may introduce bias. Use models that handle missingness natively (e.g., decision trees, XGBoost, LightGBM). Consider adding a missingness indicator. Rationale: The missingness depends on the unobserved value and may carry predictive signal. Summary Table # Case Description Abbreviation Impute? Extra Notes 1 Should be measured but wasn‚Äôt MCAR / MAR ‚úÖ Yes Add indicator if signal is likely 2 Mostly zero (rare condition) Not Missing üö´ No Keep as is ‚Äî zeros are informative 3 Deliberately not recorded MNAR ‚ö†Ô∏è Caution Use native handling + possible indicator "},{"id":89,"href":"/ai-workflows/genai/multimodel_llms/","title":"Multimodal LLMs","section":"GenAI","content":" Multimodal LLMs Core Components and Fusion Strategies in Multimodal LLMs Inputs and Data Preparation for Multimodal LLMs "},{"id":90,"href":"/ai-workflows/reasoning/graph-reasoning/knowledge-graphs/gds_vs_cypher/","title":"Neo4j GDS (Graph Data Science) vs. Core Neo4j (Cypher)","section":"Knowledge Graphs","content":" Neo4j GDS (Graph Data Science) vs. Core Neo4j (Cypher) 1. Purpose Comparison # Aspect Neo4j (Cypher) GDS (Graph Data Science) Primary Purpose Transactional queries, CRUD operations Graph analytics, algorithms, machine learning Execution Works directly on disk database Projects optimized graph into memory Speed Good for pattern match and retrieval Fast for graph-wide computations Scale Suited for operational systems Handles millions-billions of nodes/relationships Isolation Operates on live data Safe, read-only in-memory graphs Flexibility Good for flexible queries Pre-built scalable algorithms (PageRank, Louvain, etc.) Optimization Query optimization on indexes Memory-efficient subgraph projection Persistence Directly modifies database (unless read-only) Results can stay in memory or optionally write back 2. Summary Flow of GDS Workflow # Step Cypher Call Purpose ‚ë† Project Graph gds.graph.project Create an in-memory optimized graph ‚ë° List Graphs gds.graph.list Manage in-memory graph catalog ‚ë¢ Run Algorithm (Mutate) gds.pageRank.mutate, gds.degree.mutate, etc. Compute and store properties in memory ‚ë£ Stream Results gds.graph.nodeProperties.stream Retrieve computed properties ‚ë§ (Optional) Write to DB gds.pageRank.write, etc. Persist computed results to database ‚ë• Drop Graph gds.graph.drop Free memory by deleting in-memory graphs // Neo4j GDS Flow Diagram +----------------+ +-----------------------+ +----------------------+ | Neo4j Database |==\u0026gt;| GDS Graph Projection |==\u0026gt;| Graph Catalog | | (Stored Nodes, | | (In-Memory Subgraph) | | (Manage In-Memory | | Relationships)| | | | Graphs: List, Drop) | +----------------+ +-----------------------+ +----------------------+ || || \\/ +---------------------+ | GDS Algorithms | | (PageRank, | | Community Detect., | | Similarity, ML) | +---------------------+ || || \\/ +---------------------+ | Results | | (Mutate, Write back,| | Stream to client) | +---------------------+ 3. Example: GDS Workflow Code Snippet (Impossible by Cypher Alone) # // Project graph into memory CALL gds.graph.project( \u0026#39;friends-graph\u0026#39;, \u0026#39;Person\u0026#39;, \u0026#39;FRIEND\u0026#39; ); // Run PageRank algorithm and store scores in memory CALL gds.pageRank.mutate( \u0026#39;friends-graph\u0026#39;, { mutateProperty: \u0026#39;pageRankScore\u0026#39; } ); // Stream top PageRank results CALL gds.graph.nodeProperties.stream( \u0026#39;friends-graph\u0026#39;, [\u0026#39;pageRankScore\u0026#39;] ) YIELD nodeId, propertyValue RETURN gds.util.asNode(nodeId).name AS personName, propertyValue AS pageRankScore ORDER BY pageRankScore DESC LIMIT 10; // Clean up memory CALL gds.graph.drop(\u0026#39;friends-graph\u0026#39;); üöÄ This full in-memory graph analysis flow cannot be achieved using Cypher alone.\n4. Key Points # GDS Graphs ‚â† Neo4j Database: They are temporary, in-memory copies optimized for analytics. Projection: Only include nodes, relationships, properties you need. Graph Catalog: Manage multiple in-memory graphs independently. Mutate Mode: Save computed values without touching database. Write Mode: Explicitly write analytics results back to database if needed. Drop: Always free memory after analytics is complete. üöÄ Final Mindset # Use Cypher for database operations.\nUse GDS for fast, scalable, and isolated graph analytics and machine learning.\nüìö Reference # Neo4j GraphAcademy - GDS Product Introduction "},{"id":91,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/omop_vs_rlhf_comparison/","title":"OMOP vs. RLHF","section":"C2 Clinical Data","content":" OMOP vs. RLHF: A Side-by-Side Comparison # This document compares OMOP (Observational Medical Outcomes Partnership) in healthcare with RLHF (Reinforcement Learning from Human Feedback) in generative AI, focusing on their structures, purposes, and alignment with Learning Health System (LHS) principles.\nüîç Summary Table # Aspect OMOP (Healthcare) RLHF (GenAI) Domain Clinical/healthcare data Natural language modeling Purpose Standardize and structure real-world patient data for learning, analytics, and AI Align AI model behavior with human preferences and values Core Process ETL (Extract-Transform-Load) clinical data into a common format for analysis Fine-tune a pretrained LLM using human-labeled preferences or rewards Data Source EHRs, claims, labs, devices Human judgments on AI-generated outputs Feedback Type Structured medical events (diagnoses, drugs, labs, etc.) Human preference signals on outputs (better/worse answers) Learning Method Enables observational \u0026amp; causal learning from patient data Reinforcement learning from ranked or scored examples Governance Layer Ethics via IRB, consent, privacy laws Ethics via safety research, alignment goals, red-teaming Use in Feedback Loops LHS uses OMOP to ‚Äúlearn from care to improve care‚Äù RLHF uses feedback to ‚Äúteach the model to behave better‚Äù üîÅ Conceptual Analogy # OMOP + Learning Health System (LHS) is to the health system\nas\nRLHF is to a generative AI model.\nIn both cases:\nData flows through a system Human-derived feedback loops guide improvement The system continuously adapts and aligns with user or patient needs üß† Key Takeaways # Both OMOP and RLHF are feedback-driven learning architectures grounded in human data. OMOP is part of an ecosystem (LHS) that feeds learning back into medical care. RLHF aligns generative models with human preferences through iterative fine-tuning. Each reflects a shift toward real-time, adaptive, ethically grounded learning. Would you like to extend this comparison with diagrams, code examples, or regulatory implications?\n"},{"id":92,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/oap_framework_healthcare/","title":"Output-Action Pairing (OAP) Framework in Healthcare","section":"C3 ML Healthcare","content":" üß† Output-Action Pairing (OAP) Framework in Healthcare # This guide provides real-world examples of the Output-Action Pairing (OAP) framework: aligning machine learning model outputs with concrete clinical actions to improve care.\nüìã OAP Template # Output (Prediction) Action Taken Who Acts Why It Helps What the model predicts The clinical step or decision triggered The role/team responsible How it improves outcomes or safety ‚úÖ Real-World Examples # 1. Sepsis Prediction # Output: High risk of sepsis in next 6 hours Action: Alert care team, initiate fluids/labs/antibiotics Who acts: Rapid response team (nurses + physicians) Why it helps: Early treatment improves survival 2. Readmission Risk Score # Output: 30% chance of readmission within 30 days Action: Extra discharge planning, follow-up calls, medication check Who acts: Care coordinator + pharmacist Why it helps: Reduces avoidable readmissions 3. Pneumothorax Detection on Chest X-ray # Output: Pneumothorax detected Action: Immediate flag to radiologist and ER for review Who acts: Radiologist + ER team Why it helps: Enables life-saving chest tube intervention 4. COVID-19 Triage # Output: High risk of severe COVID progression Action: ICU evaluation, enhanced monitoring, begin treatment Who acts: Hospitalist or ICU triage physician Why it helps: Allocates ICU resources effectively 5. Fall Risk in Hospital # Output: High fall risk during admission Action: Enable fall precautions (alarms, sitter, etc.) Who acts: Nursing team Why it helps: Prevents injury and hospital complications 6. Stroke Detection via CT # Output: Acute stroke suspected on scan Action: Notify neurologist, activate stroke protocol (tPA window) Who acts: Radiologist + Stroke Response Team Why it helps: Reduces time to brain-saving treatment üîÑ Summary # The OAP framework ensures that ML predictions translate to action, improving clinical relevance and patient safety. Every model in healthcare should answer:\nWhat is the output? What is the action? Who will act on it? How does it help the patient? "},{"id":93,"href":"/ai-workflows/rlhf/rlhf2006/ppo_comparison/","title":"PPO in LLMs vs PPO in Walker2D","section":"RLHF 2006","content":" ü§ñü¶ø Understanding PPO: From Language Generation to Robot Control ‚Äî Code, Concepts, and Comparisons Proximal Policy Optimization (PPO) in both large language models (LLMs, e.g., GPT-style) and classical control environments (e.g., Walker2D), focusing on the structure of the PPO update and how actions are selected during inference.\n1. üßæ PPO step() Call ‚Äî Argument-by-Argument Breakdown # ppo_trainer.step( queries=[input_ids[0]], # Prompt (tokenized) ‚Äî represents the current state responses=[response_ids[0]], # Generated tokens ‚Äî represents the action taken rewards=[reward] # Scalar from reward model ‚Äî score for that action ) Mapping to Classic RL (Walker2D) # PPO Argument ü§ñ LLM (RLHF) ü¶ø Walker2D (Classic RL) queries = [input_ids[0]] Prompt as input (discrete tokenized state) Robot\u0026rsquo;s continuous state (joint angles, velocities) responses = [response_ids[0]] Generated tokens (sequence of actions) Applied joint torques (vector of real numbers) rewards = [reward] Reward model output (alignment score) Environment reward (e.g., distance walked) 2. üéØ Action Selection in PPO # How does the agent choose its next action, given a state/prompt?\nü§ñ LLMs (Text Generation) # # Given a prompt (state) input_ids = tokenizer(\u0026#34;What causes rain?\u0026#34;, return_tensors=\u0026#34;pt\u0026#34;).input_ids # Model outputs token logits for next action outputs = model(input_ids=input_ids) logits = outputs.logits[:, -1, :] probs = torch.softmax(logits, dim=-1) # Sample the next token (action) from distribution next_token = torch.multinomial(probs, num_samples=1) # Repeat to generate full response ü¶ø Walker2D (Physical Control) # # Get current robot state state = get_env_state() # vector like [Œ∏1, Œ∏2, v1, v2...] # Policy network outputs action distribution parameters mean, std = policy_net(state) # Sample a continuous action (e.g., torque values) action = torch.normal(mean, std) # Apply action to environment next_state, reward, done, info = env.step(action) üîÅ Comparison of Action Logic # Component ü§ñ LLM (RLHF) ü¶ø Walker2D (Classic RL) State Prompt text Robot‚Äôs physical state Action Next token (discrete) Joint torques (continuous) Policy Output Token logits (softmaxed) Mean \u0026amp; std dev of Gaussian per action dim Sampling Method Multinomial over vocab Sample from Gaussian Result Extend response with chosen token Step to new physical state üîÅ PPO Mapping in LLMs (RLHF) vs Classical RL # Category PPO in LLMs (RLHF) ü¶ø PPO in Walker2D (Classic RL) Agent Language Model (e.g., GPT-2, o1) Control Policy Network Environment Static or semi-static prompt context Physics-based simulator (e.g., MuJoCo) State Prompt (or full token context so far) Robot‚Äôs current physical state (joint angles, velocity, etc.) Action Next token in the sequence (discrete, vocabulary-sized) Torque values for each joint (continuous, multi-dimensional) Trajectory Sequence of tokens (prompt + response) Sequence of joint states and actions over time Reward Signal Given after full response (from reward model trained via human preferences) Immediate reward at each time step (distance walked, balance maintained, etc.) Reward Nature Sparse, episodic, scalar (usually one reward per episode) Dense, frequent, multi-dimensional (continuous feedback per step) Goal Generate text aligned with human values/preferences Learn movement to walk forward efficiently without falling Policy Network Transformer LM (large, ~billions of params) Feedforward or RNN-based controller (small, e.g., MLP) Reference Model Frozen copy of base LM (used for KL-penalty regularization) Usually none (KL not common in Walker2D PPO) Training Stability Needs KL penalty to prevent mode collapse / nonsense generations PPO alone is usually enough due to continuous feedback Evaluation Human evals, reward model scores (e.g., helpfulness, safety) Distance walked, steps survived, control energy used üó£Ô∏è ‚ÄúSay the right thing, the way a human likes‚Äù ü¶ø ‚ÄúMove the right way, so you don‚Äôt fall‚Äù Actions are words, optimizing a sequence to match human preference Actions are forces, optimizing to physically walk and stay balanced Reference https://rlhfbook.com/ https://gymnasium.farama.org/ "},{"id":94,"href":"/ai-workflows/rlhf/rlhf2006/ppo_vs_dpo_comparison/","title":"PPO vs DPO in RLHF","section":"RLHF 2006","content":" PPO vs DPO in RLHF PPO (Traditional RLHF) # Core Concept # \u0026ldquo;Let me generate new text, score it with the reward model, and update my policy based on those scores.\u0026rdquo;\nPipeline # Step 1: Train Reward Model (Ch7) Preference Data ‚Üí Reward Model Step 2: RL Optimization (Ch11) Policy generates text ‚Üí RM scores it ‚Üí PPO updates policy (Repeat with new generations) Chapter Flow # Ch6 (Preference Data) ‚Üì Ch7 (Reward Model Training) ‚Üì Ch11 (PPO RL Optimization) ‚Üì Final Model Example Code # # PPO Training Loop for batch in prompts: # Generate NEW responses responses = policy.generate(batch) # Score with reward model rewards = reward_model(responses) # Compute advantages advantages = compute_advantages( rewards, values ) # Update with PPO policy.update_with_ppo(advantages) The Math # J(Œ∏) = E[min(ratio * A, clip(ratio, 1-Œµ, 1+Œµ) * A)] - Œ≤ * D_KL(œÄ || œÄ_ref) Where: - ratio = œÄ_Œ∏(a|s) / œÄ_old(a|s) - A = advantage - clip prevents large updates - D_KL = distance from reference Pros # ‚úÖ Online learning: Discover new responses ‚úÖ Best performance: Highest quality ‚úÖ Adaptive: Uses current policy ‚úÖ Proven: ChatGPT, Claude, etc. Cons # ‚ùå Complex: RM + value + PPO ‚ùå Expensive: Multiple models ‚ùå High memory: RM + policy + ref ‚ùå Hard to tune: Many hyperparameters Best For # üéØ Maximum performance üéØ Large compute budgets üéØ Frontier models üéØ Continuous improvement Examples # ChatGPT (OpenAI) Claude (Anthropic) InstructGPT (OpenAI) DeepSeek R1 DPO (Direct Alignment) # Core Concept # \u0026ldquo;Let me directly learn from the preference data without generating anything new.\u0026rdquo;\nPipeline # Single Step: Direct Optimization Preference Data ‚Üí DPO Loss ‚Üí Update Policy (No RM, no generation, just gradient updates) Chapter Flow # Ch6 (Preference Data) ‚Üì Ch12 (DPO Direct Optimization) ‚Üì Final Model (Skip Ch7 and Ch11!) Example Code # # DPO Training Loop for (prompt, chosen, rejected) \\ in preference_data: # No generation! Just compute loss loss = -log(œÉ( Œ≤ * log(P(chosen)/P_ref(chosen)) - Œ≤ * log(P(rejected)/P_ref(rejected)) )) # Simple gradient descent policy.update(loss) The Math # L = -E[log œÉ( Œ≤ * log(œÄ(y_c)/œÄ_ref(y_c)) - Œ≤ * log(œÄ(y_r)/œÄ_ref(y_r)) )] Where: - œÉ = sigmoid function - Œ≤ = controls KL penalty - œÄ_ref = reference (frozen) Secret: Implicit reward\nr(x,y) = Œ≤ * log(œÄ(y|x) / œÄ_ref(y|x)) Pros # ‚úÖ Simple: One loss function ‚úÖ Memory efficient: No separate RM ‚úÖ Cheaper: No RM inference ‚úÖ Easy to implement: Standard training ‚úÖ Accessible: For academic labs Cons # ‚ùå Offline only: Can\u0026rsquo;t generate new data ‚ùå Data dependent: Limited by dataset ‚ùå Lower ceiling: May not match PPO ‚ùå Preference displacement: Both ‚Üì Best For # üéØ Research \u0026amp; experimentation üéØ Limited compute üéØ Academic settings üéØ Quick iterations üéØ Good fixed dataset Examples # Zephyr-7B (HuggingFace) Llama 3 Instruct (Meta) T√ºlu 2 \u0026amp; 3 (AI2) Many open-source models # Aspect PPO (Proximal Policy Optimization) DPO (Direct Preference Optimization) Approach Two-stage: Train RM ‚Üí Use RL One-stage: Direct optimization from preferences Reward Model ‚úÖ Needs separate RM (Ch7) ‚ùå No separate RM needed (implicit) Training Data Generates new responses during training Uses fixed preference dataset Algorithm Type Online RL (policy-gradient) Offline optimization (gradient ascent) Complexity More complex: RM + value function + PPO Simpler: Just one loss function Memory High: Need RM + policy + reference model Lower: Just policy + reference model Data Usage On-policy: Generates fresh data each step Off-policy: Uses pre-collected data Cost More expensive: Forward passes through RM Cheaper: No RM inference needed When Popular 2017-2023 (ChatGPT, InstructGPT) 2023-present (Llama 3, Zephyr, T√ºlu) "},{"id":95,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c2_clinical_data/diabetes_phenotype_pipeline/","title":"Rule-Based Electronic Phenotyping Example: Type 2 Diabetes","section":"C2 Clinical Data","content":" Rule-Based Electronic Phenotyping Example: Type 2 Diabetes # This notebook walks through the process of defining an electronic phenotype using a rule-based approach, with a focus on Type 2 Diabetes. The pipeline includes concept mapping, multi-patient evaluation, and phenotype logic visualization.\nüîπ Step 1: Simulated Vocabulary Lookup (UMLS / OMOP) # We define the clinical concept (Type 2 Diabetes) using relevant ICD-10 and RxNorm codes.\n# Simulated UMLS/OMOP vocab mapping UMLS_LOOKUP = { \u0026#34;type2_diabetes\u0026#34;: { \u0026#34;icd10\u0026#34;: {\u0026#34;E11.9\u0026#34;, \u0026#34;E11.65\u0026#34;, \u0026#34;E11.00\u0026#34;}, \u0026#34;rxnorm\u0026#34;: {\u0026#34;metformin\u0026#34;, \u0026#34;insulin\u0026#34;}, } } üîπ Step 2: Multi-Patient Phenotyping Logic # Each patient is checked for:\nPresence of ‚â•2 relevant ICD-10 codes Any matching diabetes-related medication (RxNorm) Start date of phenotype based on first matching code from typing import List, Dict, Set from datetime import datetime def has_required_codes(patient, valid_codes: Set[str], source_field: str, min_count=1) -\u0026gt; bool: return len([code for code in patient[source_field] if code in valid_codes]) \u0026gt;= min_count def find_start_date(patient, valid_codes: Set[str], code_dates: Dict[str, List[str]]) -\u0026gt; str: dates = [] for code in valid_codes: if code in code_dates: dates.extend(code_dates[code]) return min(dates) if dates else None def apply_phenotype_definition(patient, concept_map) -\u0026gt; Dict: icd_match = has_required_codes(patient, concept_map[\u0026#34;icd10\u0026#34;], \u0026#34;icd10_codes\u0026#34;, min_count=2) med_match = has_required_codes(patient, concept_map[\u0026#34;rxnorm\u0026#34;], \u0026#34;medications\u0026#34;) start_date = find_start_date(patient, concept_map[\u0026#34;icd10\u0026#34;] | concept_map[\u0026#34;rxnorm\u0026#34;], patient[\u0026#34;code_dates\u0026#34;]) return { \u0026#34;phenotype_positive\u0026#34;: icd_match and med_match, \u0026#34;has_diabetes_codes\u0026#34;: icd_match, \u0026#34;has_diabetes_med\u0026#34;: med_match, \u0026#34;start_date\u0026#34;: start_date } # Sample patient data (multiple patients) patients = [ { \u0026#34;id\u0026#34;: \u0026#34;P001\u0026#34;, \u0026#34;icd10_codes\u0026#34;: [\u0026#34;E11.9\u0026#34;, \u0026#34;E11.9\u0026#34;], \u0026#34;medications\u0026#34;: [\u0026#34;metformin\u0026#34;], \u0026#34;code_dates\u0026#34;: { \u0026#34;E11.9\u0026#34;: [\u0026#34;2023-01-01\u0026#34;, \u0026#34;2023-03-01\u0026#34;], \u0026#34;metformin\u0026#34;: [\u0026#34;2023-01-05\u0026#34;] } }, { \u0026#34;id\u0026#34;: \u0026#34;P002\u0026#34;, \u0026#34;icd10_codes\u0026#34;: [\u0026#34;I10\u0026#34;], # Hypertension only \u0026#34;medications\u0026#34;: [\u0026#34;lisinopril\u0026#34;], \u0026#34;code_dates\u0026#34;: { \u0026#34;I10\u0026#34;: [\u0026#34;2023-04-01\u0026#34;], \u0026#34;lisinopril\u0026#34;: [\u0026#34;2023-04-05\u0026#34;] } } ] # Apply phenotype to all results = {} for patient in patients: result = apply_phenotype_definition(patient, UMLS_LOOKUP[\u0026#34;type2_diabetes\u0026#34;]) results[patient[\u0026#34;id\u0026#34;]] = result # Show results for pid, res in results.items(): print(f\u0026#34;{pid}: {res}\u0026#34;) üîπ Step 3: Phenotype Logic Flowchart # Below is a visual flowchart that shows the phenotype logic step-by-step.\nAlt text ‚úÖ Summary # This markdown covers:\nRule-based phenotyping using ICD and RxNorm codes Handling multiple patients Simulated code-date structure Logical combination of conditions (AND logic) A visual diagram of the rule logic This framework can be expanded to:\nInclude real UMLS/OMOP lookups via API Support more complex logic (time gaps, lab thresholds) Incorporate chart-reviewed gold standards "},{"id":96,"href":"/ai-workflows/genai/self_attention_summary/","title":"Self-Attention in Transformers: A Visual Breakdown","section":"GenAI","content":" Self-Attention in Transformers: A Visual Breakdown This document summarizes key questions about self-attention, embedding vectors, positions, and the input matrix in Transformers ‚Äî using the image you provided as the foundation.\nüß† What Is Happening in the Diagram? # The figure shows how self-attention computes the output for a specific position (\u0026ldquo;detection\u0026rdquo;) by:\nGenerating attention weights between that position and all other positions. Using those weights to compute a weighted sum of the input feature vectors. üß© Key Concepts Explained # Term Meaning Element A token or word in the input sequence. Each row in the matrix is one. Position The index (0-based) of each element. Used to maintain order. Sequence The full ordered list of elements (e.g. a sentence). Word The natural-language item each element may represent. Feature Values Vector representation of the element (its embedding). While element and position are tightly linked (1:1), they are conceptually distinct: Position = slot/index Element = content in that slot üßÆ How Attention Scores Are Computed # Self-attention uses scaled dot-product attention:\nInput matrix X (from the figure) holds all embeddings. It is projected into Q, K, V using learned weights. Attention scores = dot(Q[i], K[j]) / sqrt(d_k) Softmax turns scores into attention weights. Output vector = weighted sum over all V[j], using those weights. The purple bar on the left in the figure shows these attention weights (e.g., [0.3, 0.2, 0.1, 0.3, 0, ...]).\n‚úÖ What the Image Represents # Part of Image Concept in Transformer Right-side matrix (rows) Input feature matrix X Each row One input element (word/token) Left-side purple weights Attention scores for one position Final row at bottom Output vector (weighted sum of inputs) Prepared with explanations from ChatGPT based on your questions.\n"},{"id":97,"href":"/ai-workflows/rlhf/rlhf2006/instruct_gpt_architecture/","title":"Single GPUT (RTX4090) RLHF Training Pipeline w/ TRL","section":"RLHF 2006","content":" Single GPUT (RTX4090) RLHF Training Pipeline w/ TRL ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Anthropic/hh-rlhf Dataset ‚îÇ ‚îÇ 160k examples with \"chosen\" and \"rejected\" responses ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ \"chosen\" only ‚îÇ preference pairs ‚îÇ (20k subset) ‚îÇ (50k subset) ‚Üì ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ meta-llama/Llama-2-7b-hf ‚îÇ ‚îÇ ‚îÇ (4-bit quantized base) ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚Üì ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ STEP 1: SFT Training ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Input: \"chosen\" responses (20k) ‚îÇ ‚îÇ ‚îÇ ‚îÇ Loss: Cross-entropy on completions ‚îÇ ‚îÇ ‚îÇ ‚îÇ Metric: Perplexity ‚Üí 3.3-4.5 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Memory: 18-20 GB ‚îÇ ‚îÇ ‚îÇ ‚îÇ Time: 2-4 hours ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ SFT Model ‚îÇ (~50 MB LoRA adapters) ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚Üì ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Policy (PPO) ‚îÇ ‚îÇ Reference (PPO) ‚îÇ ‚îÇ RM Base ‚îÇ ‚îÇ +value head ‚îÇ ‚îÇ frozen copy ‚îÇ ‚îÇ +reward head ‚îÇ ‚îÇ trainable ‚îÇ ‚îÇ for KL penalty ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ + pairs (50k) ‚îÇ ‚îÇ ‚Üì ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ STEP 2: Reward Model ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Input: Pairs (50k) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Loss: Ranking loss ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Metric: Accuracy \u003e70% ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Memory: 20-22 GB ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Time: 3-6 hours ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚Üì ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Reward Model ‚îÇ (frozen scorer) ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ + prompts (20k) ‚îÇ ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ STEP 3: PPO RLHF Optimization ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇ Input: Prompts (20k) ‚îÇ ‚îÇ ‚îÇ ‚îÇ Policy + Reference + RM ‚îÇ ‚îÇ ‚îÇ ‚îÇ Loss: PPO clipped + KL penalty ‚îÇ ‚îÇ ‚îÇ ‚îÇ Metric: Mean reward ‚Üë, KL 0.1-0.3 ‚îÇ ‚îÇ ‚îÇ ‚îÇ Memory: 22-24 GB ‚îÇ ‚îÇ ‚îÇ ‚îÇ Time: 6-12 hours ‚îÇ ‚îÇ ‚îÇ ‚îÇ Loop: 1000 PPO steps ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Final RLHF Model ‚îÇ ‚îÇ (LoRA adapters) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Memory Usage Breakdown (RTX 4090 - 24 GB) # Component Memory Notes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Base Model (4-bit) ~3.5 GB Llama-2-7B in NF4 LoRA Adapters ~0.05 GB r=16, small footprint Optimizer States ~8-10 GB Paged AdamW 8-bit Activations ~6-8 GB With gradient checkpointing KV Cache (PPO generation) ~2-4 GB During generation phase ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Total 18-24 GB Fits on RTX 4090! Training Metrics Timeline # SFT RM PPO Time: 0h‚îÄ‚îÄ‚îÄ‚îÄ4h 4h‚îÄ‚îÄ‚îÄ‚îÄ10h 10h‚îÄ‚îÄ‚îÄ‚îÄ22h ‚îÇ ‚îÇ ‚îÇ Loss: 2.5 ‚Üí 1.2 0.69 ‚Üí 0.35 - Perplexity: 12 ‚Üí 3.3 - - Accuracy: - 50% ‚Üí 72% - Reward: - - +2 ‚Üí +8 KL: - - 0.05 ‚Üí 0.25 ‚îÇ ‚îÇ ‚îÇ Output: SFT Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Reward Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí RLHF Model Key Relationships # SFT ‚Üí RM: Warm start (better initialization than random) SFT ‚Üí Policy: Direct inheritance (then optimized) SFT ‚Üí Reference: Frozen copy (anchor for KL penalty) RM ‚Üí PPO: Provides reward signal (quality score) Loss Functions # SFT Loss:\nL_SFT = -log P(y_completion | x_prompt) RM Loss:\nL_RM = -log œÉ(r(x, y_chosen) - r(x, y_rejected)) PPO Loss:\nL_PPO = min(r_t(Œ∏)¬∑A_t, clip(r_t(Œ∏), 1¬±Œµ)¬∑A_t) + Œ≤¬∑D_KL(œÄ||œÄ_ref) where r_t(Œ∏) = œÄ_Œ∏(a|s) / œÄ_Œ∏_old(a|s) (probability ratio) "},{"id":98,"href":"/ai-workflows/rlhf/rlhf2006/instruct_gpt/","title":"The Complete InstructGPT Recipe (Ch 4.2.1)","section":"RLHF 2006","content":" The Complete Instruct_GPT Recipe (Ch 4.2.1) Sequential and Two Separate Datasets for RLHF # Attribute Dataset 1: comes 1st Dataset 2: comes next Size ~10K examples (InstructGPT), ~1M modern ~100K preference pairs (InstructGPT) Format (prompt, good_response) - SINGLE examples (prompt, chosen, rejected) - PAIRWISE comparisons Source Human-written OR synthetic from strong models Human labelers comparing SFT model outputs Purpose Teach the model HOW TO RESPOND in chat format Teach what GOOD vs BAD responses look like When Collected BEFORE preference data collection AFTER SFT model exists (use SFT to generate responses) Used to Train SFT Model (Ch9) Reward Model (Ch7) Example { \u0026quot;prompt\u0026quot;: \u0026quot;What is machine learning?\u0026quot;, \u0026quot;response\u0026quot;: \u0026quot;Machine learning is a branch of AI that...\u0026quot; } {\u0026quot;prompt\u0026quot;: \u0026quot;What is machine learning?\u0026quot;, \u0026quot;chosen\u0026quot;: \u0026quot;Machine learning is a branch of AI that enables...\u0026quot;, \u0026quot;rejected\u0026quot;: \u0026quot;ML is when computers learn stuff.\u0026quot; } when to prepare 1st and 2nd dataset # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Ch9 SFT ‚îÇ‚Üê FIRST: Prepare SFT data ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚Üì [Train SFT Model] ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê Generate responses ‚îÇ Use in ‚îÇ for humans to compare ‚îÇ Ch7 ‚îÇ ‚îÇ ‚îÇ RM ‚îÇ ‚Üì ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇCh6 Pref Data ‚îÇ‚Üê SECOND: Collect preferences ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚Üì ‚Üì Ch7 RM Use RM in Ch11 ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì Ch11 RL(PPO) The Complete InstructGPT Recipe (Ch 4.2.1) # ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Pretrained Base Model ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ STEP 1: Ch9 SFT ‚îÇ ‚îÇ ‚Ä¢ Prepare single good examples ‚îÇ ‚îÇ ‚Ä¢ Train model on (prompt, response) pairs ‚îÇ ‚îÇ ‚Ä¢ Output: SFT Model (can generate responses) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ STEP 2a: Ch6 Data Collection‚îÇ ‚îÇ STEP 2b: Ch7 RM ‚îÇ ‚îÇ ‚Ä¢ Use SFT to generate ‚îÇ ‚îÇ ‚Ä¢ Start from SFT ‚îÇ ‚îÇ ‚Ä¢ Humans compare outputs ‚îÇ‚îÄ‚Üí‚îÇ ‚Ä¢ Train on Ch6 data ‚îÇ ‚îÇ ‚Ä¢ Create preference pairs ‚îÇ ‚îÇ ‚Ä¢ Output: RM ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ STEP 3: Ch11 RL Optimization ‚îÇ ‚îÇ ‚Ä¢ Policy: SFT Model (from Step 1) ‚îÇ ‚îÇ ‚Ä¢ Scorer: Reward Model (from Step 2) ‚îÇ ‚îÇ ‚Ä¢ Optimize policy using RM feedback ‚îÇ ‚îÇ ‚Ä¢ Output: Final RLHF-trained Model ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò "},{"id":99,"href":"/healthcare/domain_knowledge/ai-in-healthcare/c3_ml_healthcare/precision_vs_recall_in_healthcare/","title":"Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare","section":"C3 ML Healthcare","content":" Tradeoffs in Machine Learning: Precision vs. Recall in Healthcare # This guide summarizes two key scenarios in healthcare where we might prefer:\nHigh Precision but Lower Recall High Recall but Lower Precision (1) High Precision, Lower Recall # ‚úÖ When to Use: # When false positives are costly or harmful When resources are limited In early screening/filtering stages üìå Justification: # You want to be very confident before taking action. Missing some real cases is acceptable if wrongly flagging someone leads to emotional, financial, or clinical harm. üí° Examples: # Genetic Testing for Rare Diseases: Only flag patients when you\u0026rsquo;re very sure. A false positive could cause unnecessary panic or life changes. ICU Bed Allocation: If you only have 5 beds, you‚Äôd want to use them for patients who are most certainly critical. Drug Discovery Pre-Screening: Select molecules that are most likely to work, even if some potential candidates are missed. (2) High Recall, Lower Precision # ‚úÖ When to Use: # When missing a real case is dangerous When early detection can improve outcomes When follow-up tests or actions are safe and cheap üìå Justification: # It\u0026rsquo;s better to catch every possible case, even if you have some false alarms. Especially important in serious or rapidly progressing conditions. üí° Examples: # Cancer Screening: Better to flag more patients for follow-up than miss someone with early-stage cancer. Sepsis Prediction in ER: Alerting the care team early‚Äîeven with some false alarms‚Äîcan save lives. COVID-19 Testing in High-Risk Areas: Broad detection to prevent spread, even if some healthy people test positive. üß† Summary Table # Scenario Priority Justification Example High Precision, Lower Recall Precision üü¢ Avoid harm/cost from false positives Genetic testing, ICU triage High Recall, Lower Precision Recall üü¢ Avoid missing critical or contagious conditions Cancer screening, sepsis alert "},{"id":100,"href":"/ai-workflows/eval/traditional_vs_ai_data_science/","title":"Traditional Data Science vs AI Data Science (Model-Centric)","section":"Eval","content":" Traditional Data Science vs AI Data Science (Model-Centric) Mental Reset \u0026amp; Workflow Shift # Traditional data science\nAnalyzes a stable world Collect ‚Üí Clean ‚Üí Model ‚Üí Validate ‚Üí Deploy AI data science\nMeasures and shapes a moving, self-modifying system. Hypothesis ‚Üí Design probe ‚Üí Stress / compare ‚Üí Analyze failure distribution ‚Üí Translate to training signal ‚Üí Repeat Side-by-Side Comparison # Dimension Traditional Data Science AI Data Science (Model-Centric) Data distribution Mostly stationary Strongly non-stationary Object of study External systems (users, markets) The model itself Errors Mostly independent (noise) Highly correlated (structure) Metrics Scalar, aggregate Diagnostic, process-level Ground truth Well-defined labels Often ambiguous / constructed Feedback loop Slow, indirect Fast, tight, training-coupled Rare events Often ignorable Often highest-signal Evaluation goal Optimize performance Shape behavior \u0026amp; alignment Data generation Observational Experimental, adversarial Time horizon Retrospective Predictive, anticipatory Key Differences ‚Üí Implications # Aspect Traditional DS AI DS Practical Implication Distribution shift Exception Default Averages don‚Äôt predict the future Data source World ‚Üí data Model ‚Üí data Evaluation is an intervention Error structure Random noise Clustered failures One failure implies many Rarity Low priority High signal Diagnose, don‚Äôt ignore Correctness Given Schema-defined Label design is critical Metrics Descriptive Prescriptive Bad metrics ‚Üí bad models Outcome vs process Outcome-focused Process-focused Right answer ‚â† right reasoning Evaluation style Observational Experimental Probing is mandatory Final takeaway # Traditional data science measures the world; AI data science measures and shapes a system that is itself evolving.\n"},{"id":101,"href":"/ai-workflows/genai/transformer_attention_concepts/","title":"Transformer Attention: Full Conceptual Breakdown","section":"GenAI","content":" Transformer Attention: Full Conceptual Breakdown This document summarizes an in-depth discussion on attention mechanisms in Transformers, with a special focus on vocabulary embeddings, Q/K/V matrices, and multi-head attention.\nüìå 1. Understanding the Self-Attention Image # The image shows a single-head self-attention computation. Each row is a token (element) at a position, with a feature vector (embedding). The attention weights (left column) are used to compute a weighted sum over these vectors. The final output vector is shown at the bottom ‚Äî this is the attention output for one token. üîç 2. Element vs. Position # Element: the actual word or token in the input sequence. Position: the index of the element in the sequence. Though tightly coupled (1:1), they are conceptually different. Transformers rely on positional encoding to retain order, since attention alone is orderless. ü§ñ 3. How Attention Scores Are Computed # Input embeddings X are projected into:\nQueries (Q) Keys (K) Values (V) Attention score between token i and j:\nscore = dot(Q[i], K[j]) / sqrt(d_k) Apply softmax to get weights.\nMultiply each Value by its weight and sum ‚Üí gives the final output vector.\nüß† 4. What Is X in the Diagram? # The large matrix on the right of the image is the input embedding matrix X. Shape: sequence_length √ó embedding_dim It is built by looking up each token‚Äôs vector from the vocabulary embedding matrix. üîÑ 5. What Is Multi-Head Attention? # Single-head attention is shown in the image. Multi-head attention: Splits X into smaller chunks (d_model / n_heads) Computes self-attention in parallel on each chunk (head) Concatenates results from all heads Applies a final linear projection üî° 6. Vocab Embedding Matrix vs. Q/K/V # Vocabulary embedding matrix: Initialized randomly Trained to map each token to a vector Q, K, V: Computed from X using learned matrices W_Q, W_K, W_V Not stored in the vocabulary matrix Are trainable and persistent ‚ôªÔ∏è 7. Lifetime of W_Q, W_K, W_V # These matrices are: Initialized once Trained over time Reused across batches They are not reset per input or per batch. Gradients update them through backpropagation. üì• 8. Is Vocabulary Matrix Also Trainable? # ‚úÖ Yes.\nIt is randomly initialized and trained alongside the rest of the model. Each token lookup retrieves a vector from this matrix. This matrix evolves to encode semantic relationships between words. üì¶ 9. Use Cases After Training # Goal Uses Vocab Matrix Uses W_Q/K/V Inference on new sentence ‚úÖ ‚úÖ Static embedding for a token ‚úÖ ‚ùå Contextual embedding in sentence ‚úÖ ‚úÖ üìê 10. Dimensions of X, Q, K, V, and Attention # Let:\nL = sequence length d_model = embedding dimension (e.g. 512) n_heads = number of attention heads d_k = d_model / n_heads Component Shape Input X (L, d_model) W_Q, W_K, W_V (d_model, d_model) Q, K, V (stacked) (n_heads, L, d_k) Attention output (head) (L, d_k) Concatenated heads (L, d_model) Final output (L, d_model) ‚ùì 11. Why Isn‚Äôt the Final Output a Distribution Over Vocabulary? # This is a great question that highlights a common confusion.\nThe output of multi-head attention (and the full Transformer stack) is:\n(L, d_model) But the vocabulary distribution comes after applying a final linear layer:\nW_vocab ‚àà ‚Ñù^(d_model √ó vocab_size) logits = output √ó W_vocab ‚Üí (L, vocab_size) Then softmax gives:\nprobability distribution over vocabulary for each token position Stage Output Shape Multi-head Attention (L, d_model) Final Linear Projection (L, vocab_size) Softmax (L, vocab_size) So the discrepancy is resolved when we remember that attention is only a component ‚Äî the final vocabulary distribution is computed later in the model pipeline.\nPrepared as a study summary by ChatGPT based on a thread of detailed conceptual questions.\n"},{"id":102,"href":"/healthcare/clinical_ai/why_clinical_ai/","title":"Why Clinical NLP \u0026 GenAI Are Growing in Healthcare","section":"AI Applications","content":" Why Clinical NLP \u0026 GenAI Are Growing in Healthcare Clinical NLP \u0026amp; GenAI are growing rapidly in healthcare because they unlock massive untapped value in unstructured data ‚Äî which has historically been hard to use, yet contains the richest clinical context.\n1. 80% of Clinical Data is Unstructured # EHRs are full of free-text clinical notes, discharge summaries, radiology reports, operative notes, etc. Traditional models work well with structured data (ICD, labs), but miss context like: ‚ÄúPatient denies chest pain‚Äù ‚ÄúFamily history of diabetes‚Äù ‚ÄúPatient expressed concern about medication side effects‚Äù NLP allows us to extract clinical meaning from this text and turn it into computable features.\n2. LLMs Unlocked Previously Impossible Use Cases # Older NLP methods (regex, rule-based, small transformers) had limited scope and brittle performance. LLMs like GPT-4, BioGPT, Med-PaLM, ClinicalBERT now: Understand clinical language Handle ambiguity and nuance (negation, temporality, coreference) Can answer questions, summarize, or extract entities with minimal supervision We now have zero-shot/few-shot models that can generalize better and faster.\n3. Tooling and Ecosystem Improvements # LLMOps tools (e.g., LangChain, LlamaIndex) make it easy to build: RAG pipelines from medical knowledge bases (e.g., UpToDate, PubMed) Clinical chatbots, document summarizers, question-answering tools Medical NLP toolkits are becoming better: scispaCy, medspaCy, MetaMap, cTAKES, MedCAT HuggingFace models like BioClinicalBERT, BlueBERT, PubMedBERT 4. Real Clinical Needs Driving Demand # Physicians are overwhelmed by documentation ‚Äî GenAI is helping with: Ambient scribes (auto-documenting patient visits) Auto-summarization of notes, referrals, discharge instructions Researchers want to extract phenotypes or chart review signals at scale Payers want to mine notes for HCC coding or prior authorization info NLP reduces chart review time from hours to seconds\n5. Regulatory and Business Shifts # FDA and CMS are recognizing NLP-derived features in trials and risk models Private sector is investing heavily (e.g., Nuance, Abridge, AWS HealthScribe, Epic‚Äôs NoteReader, Google Med-PaLM) NLP applications align with value-based care and documentation burden reduction, two big industry trends 6. Surge in Research and Commercial Applications # Explosion of clinical NLP papers, open datasets (MIMIC-III notes, i2b2, n2c2), and competitions Many startups and research labs focus entirely on GenAI for clinical use cases Oncology-Specific Use Case for Clinical NLP # Use Case: Automated Tumor Board Summarization # Problem: Oncologists review vast free-text data for tumor board meetings, including pathology, radiology, and progress notes. NLP Solution: Extracts key findings (e.g., tumor staging, mutations, response to therapy) from notes Summarizes patient‚Äôs oncologic timeline Suggests evidence-based treatment pathways using integrated knowledge bases Impact: Saves time preparing for multi-disciplinary meetings Ensures consistent and comprehensive reviews Enables decision support and documentation automation ‚úÖ Summary: Why Clinical NLP \u0026amp; GenAI Are Growing # Factor Description Untapped Data 80% of EHR is free text ‚Äî highly valuable, underused LLM Capabilities GPT, BioGPT, etc., can extract, summarize, reason Tooling Libraries and APIs make NLP workflows easier to deploy Clinical Demand Ambient documentation, summarization, triage tools Market Forces Reimbursement, policy, burnout, and value-based care Research Fuel Rich open datasets (MIMIC), benchmarks, HuggingFace "}]